<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Taylor Arnold and Lauren Tilton">

<title>1.2 Tutorial II: Moving Images â€“ Distant Viewing Scripts</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./image.html" rel="next">
<link href="./tutorial-digital-images.html" rel="prev">
<link href="./img/dlogo.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-eb3b6280cc05ac445df4c21feef64b49.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./tutorials.html">Tutorials</a></li><li class="breadcrumb-item"><a href="./tutorial-moving-images.html">1.2 Tutorial II: Moving Images</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Distant Viewing Scripts</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/distant-viewing/dv-demo/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome!</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./tutorials.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tutorials</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial-digital-images.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1.1 Tutorial I: Digital Images</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial-moving-images.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">1.2 Tutorial II: Moving Images</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./image.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Images</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./metrics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.1 Metrics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./color.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.2 Color</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.3 Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./object.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.4 Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./depth.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.5 Depth Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./segment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.6 Image Segmentation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./embed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.7 Embedding</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./video.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Video + Audio</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./shot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.1 Shot Boundary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./transcription.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.2 Transcription</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./diarization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.3 Diarization</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./text.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Text</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sentiment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4.1 Sentiment Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4.2 Review Prediction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./comment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4.3 Comment Prediction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mask.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4.4 Text Mask</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./multimodal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Multimodal</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./zeroshot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.1 Zero-Shot Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./caption.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.2 Image Caption</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./citation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Citation + Funding</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#setup" id="toc-setup" class="nav-link active" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#network-era-sitcom-dataset" id="toc-network-era-sitcom-dataset" class="nav-link" data-scroll-target="#network-era-sitcom-dataset">Network-Era Sitcom Dataset</a></li>
  <li><a href="#sample-movie-file" id="toc-sample-movie-file" class="nav-link" data-scroll-target="#sample-movie-file">Sample Movie File</a></li>
  <li><a href="#working-with-video-frames-as-images" id="toc-working-with-video-frames-as-images" class="nav-link" data-scroll-target="#working-with-video-frames-as-images">Working with Video Frames as Images</a></li>
  <li><a href="#shot-boundary-detection" id="toc-shot-boundary-detection" class="nav-link" data-scroll-target="#shot-boundary-detection">Shot Boundary Detection</a></li>
  <li><a href="#face-detection-and-facial-recognition" id="toc-face-detection-and-facial-recognition" class="nav-link" data-scroll-target="#face-detection-and-facial-recognition">Face Detection and Facial Recognition</a></li>
  <li><a href="#building-annotations-from-a-video-file" id="toc-building-annotations-from-a-video-file" class="nav-link" data-scroll-target="#building-annotations-from-a-video-file">Building Annotations from a Video File</a></li>
  <li><a href="#full-corpus-analysis" id="toc-full-corpus-analysis" class="nav-link" data-scroll-target="#full-corpus-analysis">Full Corpus Analysis</a></li>
  <li><a href="#conclusions-and-next-steps" id="toc-conclusions-and-next-steps" class="nav-link" data-scroll-target="#conclusions-and-next-steps">Conclusions and Next Steps</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./tutorials.html">Tutorials</a></li><li class="breadcrumb-item"><a href="./tutorial-moving-images.html">1.2 Tutorial II: Moving Images</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">1.2 Tutorial II: Moving Images</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This notebook explores the theory and methods introduced in the book <em>Distant Viewing</em> (MIT Press, 2023) to study visual style in two network era sitcoms. Specifically, we will look at every televised episode of the series <em>Bewitched</em> (1964-1972) and <em>I Dream of Jeannie</em> (1965-1970). In the notebook we will first walk through the methodology using a short 45 second sample video and going slowly through all of the steps. Then, due to time, file size, and copyright constraints, we will load a precomputed set of image annotations mirroring those from the 45 second sample and then use this larger set for the purpose of analysis. Here are the specific learning outcomes for the tutorial:</p>
<ol type="1">
<li>Explain how digital videos can be understood as a sequence of images.</li>
<li>Apply functions in the distant viewing toolkit to a short video file.</li>
<li>Calculate shot breaks using a state-of-the-art computer vision algorithm and connect shot breaks to research questions in media studies.</li>
<li>Classify the estimated identity of characters using computer visional algorithms and face embeddings.</li>
<li>Illustrate how to address humanities research questions using computer vision algorithms applied to a corpus of television shows.</li>
</ol>
<p>This notebook does not require any previous knowledge of Python or computer vision. However, it moves fairly quickly through the preliminary steps of working with digital images and only explains the most important aspects of the Python code in each step. For a more in-depth introduction to how computers view images images and python, we recommend first following the Distant Viewing Tutorial: Movie Posters and Color Analysis notebook using the movie posters corpus, which can be accessed <a href="https://colab.research.google.com/drive/1qQKQw8qHsTG7mK7Rz-z8nBfl98QBMWGf?usp=sharing">here</a>. For more about the Python package that we built, Distant Viewing Toolkit (<strong>dvt</strong>), please visit <a href="https://github.com/distant-viewing/dvt">our GitHub repository</a>.</p>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<div id="7d41fd73" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> dvt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="network-era-sitcom-dataset" class="level2">
<h2 class="anchored" data-anchor-id="network-era-sitcom-dataset">Network-Era Sitcom Dataset</h2>
<p>Before we start looking at the computational steps needed to work with moving image data, itâ€™s helpful to understand the humanities research questions that motivate this work. Here, we are concerned with two popular U.S. sitcoms from the 1960s and early 1970s: <em>Bewitched</em> (1964-1972) and <em>I Dream of Jeannie</em> (1965-1970). Here is metadata about each of the episodes from the entire runs of the two shows:</p>
<div id="0af90b03" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>meta <span class="op">=</span> pd.read_csv(<span class="st">"data/ttl_sitcom_metadata.csv"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>meta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">series</th>
<th data-quarto-table-cell-role="th">season</th>
<th data-quarto-table-cell-role="th">number</th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">director</th>
<th data-quarto-table-cell-role="th">writer</th>
<th data-quarto-table-cell-role="th">air_date</th>
<th data-quarto-table-cell-role="th">description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Bewitched</td>
<td>1</td>
<td>1</td>
<td>I, Darrin, Take This Witch, Samantha</td>
<td>William Asher</td>
<td>Sol Saks</td>
<td>1964-09-17</td>
<td>In the pilot episode, strangers Samantha (Eliz...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Bewitched</td>
<td>1</td>
<td>2</td>
<td>Be It Ever So Mortgaged</td>
<td>William Asher</td>
<td>Barbara Avedon</td>
<td>1964-09-24</td>
<td>Endora is still surprised that Sam is willing ...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Bewitched</td>
<td>1</td>
<td>3</td>
<td>It Shouldn't Happen to a Dog</td>
<td>William Asher</td>
<td>Jerry Davis</td>
<td>4-10-01)[3</td>
<td>Samantha is preparing dinner for Darrin's pote...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Bewitched</td>
<td>1</td>
<td>4</td>
<td>Mother Meets What's-His-Name</td>
<td>William Asher</td>
<td>Danny Arnold</td>
<td>1964-10-08</td>
<td>Samantha is visited by Gladys Kravitz, June Fo...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Bewitched</td>
<td>1</td>
<td>5</td>
<td>Help, Help, Don't Save Me</td>
<td>William Asher</td>
<td>Danny Arnold</td>
<td>1964-10-15</td>
<td>Darrin has been spending all hours working on ...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">388</td>
<td>I Dream of Jeannie</td>
<td>5</td>
<td>22</td>
<td>Eternally Yours, Jeannie</td>
<td>Joseph Goodson</td>
<td>James Henerson</td>
<td>1970-03-17</td>
<td>Tony gets a letter from his old high school sw...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">389</td>
<td>I Dream of Jeannie</td>
<td>5</td>
<td>23</td>
<td>An Astronaut in Sheep's Clothing</td>
<td>Bruce Kessler</td>
<td>James Henerson</td>
<td>1970-03-24</td>
<td>After Jeannie blinks Tony a drum major's unifo...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">390</td>
<td>I Dream of Jeannie</td>
<td>5</td>
<td>24</td>
<td>Hurricane Jeannie</td>
<td>Claudio Guzman</td>
<td>James Henerson</td>
<td>1970-04-28</td>
<td>A hurricane traps Tony, Jeannie, Roger and Dr....</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">391</td>
<td>I Dream of Jeannie</td>
<td>5</td>
<td>25</td>
<td>One Jeannie Beats Four of a Kind</td>
<td>Michael Ansara</td>
<td>Perry Grant, Richard Bensfield</td>
<td>1970-05-19</td>
<td>General Schaeffer tells Major Healey of a card...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">392</td>
<td>I Dream of Jeannie</td>
<td>5</td>
<td>26</td>
<td>My Master, the Chili King</td>
<td>Claudio Guzman</td>
<td>James Henerson</td>
<td>1970-05-26</td>
<td>Tony's cousin Arvel (Gabriel Dell) persuades J...</td>
</tr>
</tbody>
</table>

<p>393 rows Ã— 8 columns</p>
</div>
</div>
</div>
<p>These two series are often compared and constrasted to one another, with <em>I Dream of Jeannie</em> being seen as an attempt by NBC to copy the success of ABCâ€™s <em>Bewitched</em>, which started one season prior. While a lot has been written about the cultural significance of these two shows, prior research had not seriously considered the visual style of the two shows. We are interested in how the way that the shows are shot and edited contribute to our understanding of questions such as who is/are the main characters, to what extent does the visual style contribute to relationships between the characters, and how do these aspects change over time. The latter is particularly interesting because both shows started their broadcasts in black-and-white before transitioning to color for the majority of the later seasons.</p>
</section>
<section id="sample-movie-file" class="level2">
<h2 class="anchored" data-anchor-id="sample-movie-file">Sample Movie File</h2>
<p>Now that we have some ideas of the big questions we are interested in, letâ€™s look at a short sample from one of the series to understand the computational steps needed to work with moving images. When doing computational analyses with visual and audiovisual materials, it is important to frequently go back and look at specific examples to ensure that we understand how our large-scale analysis connects back to the actual human experience of viewing. In this notebook we are going to start by working slowly with the process of creating annotations that summarize a short 45 second clip from the first episode of the third season of the show <em>Bewitched</em>. Running the code below shows an embedded version of the clip that can be watched within this page:</p>
<video controls="" width="600">
<source src="data/bewitched_sample.mp4" type="video/mp4">
</video>
<p>We suggest watching the clip a couple of times. Try to pay attention (and maybe even write down) the number of shots in the clip and how they are framed. In the next few sections, we will use computer vision algorithms to try to capture these features as structured annotations.</p>
</section>
<section id="working-with-video-frames-as-images" class="level2">
<h2 class="anchored" data-anchor-id="working-with-video-frames-as-images">Working with Video Frames as Images</h2>
<p>Video file formats such as <strong>mp4</strong>, <strong>avi</strong>, and <strong>ogg</strong> typically store moving images in a complex format that is highly optimized to reduce file sizes and decompression time. When working with moving images computationally in programs such as Python, however, we typically decompress these files and turn them into a sequence of individual images stored as arrays of pixels, with one image for each frame in the input video. The distant viewing toolkit (<strong>dvt</strong>) contains several functions to efficently work with video files in Python. In this section, we will demonstrate how these work.</p>
<p>First, we can use the <code>video_info</code> function to access metadata about a video file. Here, we will load the metadata for our sample video file, which displays the number of frames, the height and width of each frame in pixels, and rate of playback given as frames per second (<code>fps</code>).</p>
<div id="1afbc06e" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>info <span class="op">=</span> dvt.video_info(<span class="st">'data/bewitched_sample.mp4'</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>info</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>{'frame_count': 1319.0,
 'height': 480.0,
 'width': 710.0,
 'fps': 29.97002997002997}</code></pre>
</div>
</div>
<p>As we see in the metadata, even this short clip has 1319 individual frames. When working with an entire television episode, let along a feature-length movie file, it quickly becomes difficult to load all of frames from a video into Python at once. The image sizes, particularly once we start looking at materials in high-definition, are just too large to hold these all in even a computerâ€™s memory at the same time.</p>
<p>The distant viewing toolkit (<strong>dvt</strong>) offers an alternative approach using the <code>yield_video</code> function. It allows us to write a loop object which loads in each frame of the video file one by one. Specifically, each time the yield function is called, it returns the next frame as an array of pixels, an integer describing the frame number from the start of the video file, and a time code giving the number of seconds since the start of the video.</p>
<p>The code below show an example of how the <code>yield_video</code> function works. We cycle through each of the frames. For each frame we store the frame number, the timestamp, and the average pixel values (a measurement of the frameâ€™s brightness). Then, we turn this into a tabular data frame with one row for each frame in the video.</p>
<div id="5e27cef1" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> {<span class="st">'frame'</span>: [], <span class="st">'time'</span>: [], <span class="st">'brightness'</span>: []}</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> img, frame, msec <span class="kw">in</span> dvt.yield_video(<span class="st">"data/bewitched_sample.mp4"</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  output[<span class="st">'frame'</span>].append(frame)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  output[<span class="st">'time'</span>].append(msec)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  output[<span class="st">'brightness'</span>].append(np.mean(img))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> pd.DataFrame(output)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">frame</th>
<th data-quarto-table-cell-role="th">time</th>
<th data-quarto-table-cell-role="th">brightness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0</td>
<td>0.000000</td>
<td>87.132526</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1</td>
<td>0.033367</td>
<td>87.221070</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>2</td>
<td>0.066733</td>
<td>87.496499</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>3</td>
<td>0.100100</td>
<td>87.620548</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>4</td>
<td>0.133467</td>
<td>87.445358</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1314</td>
<td>1314</td>
<td>43.843800</td>
<td>68.011383</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1315</td>
<td>1315</td>
<td>43.877167</td>
<td>68.054915</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1316</td>
<td>1316</td>
<td>43.910533</td>
<td>68.043498</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1317</td>
<td>1317</td>
<td>43.943900</td>
<td>68.028847</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1318</td>
<td>1318</td>
<td>43.977267</td>
<td>67.969112</td>
</tr>
</tbody>
</table>

<p>1319 rows Ã— 3 columns</p>
</div>
</div>
</div>
<p>Brightness is one of the simpliest annotations that we can use to summarize an image. But, even this simple measurement can already provide a rough way of representing our video clip. To see this, letâ€™s plot the brightness of the frame over time.</p>
<div id="2aef9fb2" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>plt.plot(output[<span class="st">'time'</span>], output[<span class="st">'brightness'</span>])</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial-moving-images_files/figure-html/cell-6-output-1.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Looking at the brightness, particularly the large jumps in the values, can you identify the shot breaks that you found when watching the video? There is a steady increase in the brightness from about 15 seconds to 20 seconds. What does this correspond to in the video file?</p>
</section>
<section id="shot-boundary-detection" class="level2">
<h2 class="anchored" data-anchor-id="shot-boundary-detection">Shot Boundary Detection</h2>
<p>One of the first tasks that we often want to perform on a video file is to identify the breaks between shots. This process is called <em>shot boundary detection</em>. As we saw in the brightness plot above, some simple heuristics can be used to approximately identify many kinds of shot breaks. If we want more accurate predictions, which do not falsely identify quick motion as a shot boundary or fail to find subtle breaks such as cross-fades, we need to make use of a more complex algorithm. The distant viewing toolkit (<strong>dvt</strong>) includes a neural-network based shot boundary detection algorithm that we have found works well across many different genres and input types.</p>
<p>To run the built-in shot boundary detection algorithm, we need to first create a new annotator with the <code>AnnoShotBreaks</code> function. Then, we run the annotator over the video file. This is the only annotator in the toolkit that works directly with a video file rather than individual images or frames. The Python code will print out its progress through the file as it runs, returning a dictionary object with the predicted boundaries.</p>
<div id="08caa7c1" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>anno_breaks <span class="op">=</span> dvt.AnnoShotBreaks()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>out_breaks <span class="op">=</span> anno_breaks.run(<span class="st">"data/bewitched_sample.mp4"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/admin/gh/dvscripts/env/lib/python3.12/site-packages/dvt/shots.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model.load_state_dict(torch.load(model_path))</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Processing video frames 50/1319Processing video frames 100/1319Processing video frames 150/1319Processing video frames 200/1319Processing video frames 250/1319Processing video frames 300/1319Processing video frames 350/1319Processing video frames 400/1319Processing video frames 450/1319Processing video frames 500/1319Processing video frames 550/1319Processing video frames 600/1319Processing video frames 650/1319Processing video frames 700/1319Processing video frames 750/1319Processing video frames 800/1319Processing video frames 850/1319Processing video frames 900/1319Processing video frames 950/1319Processing video frames 1000/1319Processing video frames 1050/1319Processing video frames 1100/1319Processing video frames 1150/1319Processing video frames 1200/1319Processing video frames 1250/1319Processing video frames 1300/1319Processing video frames 1319/1319</code></pre>
</div>
</div>
<p>We will convert the output of the shot boundary detection into a tabular dataset as well as add the start time and end time information using the metadata that we grabbed from the <code>video_info</code> function above. Here is what the algorithm found for this video clip:</p>
<div id="9081f2c6" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>shot <span class="op">=</span> pd.DataFrame(out_breaks[<span class="st">'scenes'</span>])</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>shot[<span class="st">'mid'</span>] <span class="op">=</span> (shot[<span class="st">'start'</span>] <span class="op">+</span> shot[<span class="st">'end'</span>]) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>shot[<span class="st">'start_t'</span>] <span class="op">=</span> shot[<span class="st">'start'</span>] <span class="op">/</span> info[<span class="st">'fps'</span>]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>shot[<span class="st">'end_t'</span>] <span class="op">=</span> shot[<span class="st">'end'</span>] <span class="op">/</span> info[<span class="st">'fps'</span>]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>shot</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">start</th>
<th data-quarto-table-cell-role="th">end</th>
<th data-quarto-table-cell-role="th">mid</th>
<th data-quarto-table-cell-role="th">start_t</th>
<th data-quarto-table-cell-role="th">end_t</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0</td>
<td>98</td>
<td>49</td>
<td>0.000000</td>
<td>3.269933</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>99</td>
<td>184</td>
<td>141</td>
<td>3.303300</td>
<td>6.139467</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>185</td>
<td>427</td>
<td>306</td>
<td>6.172833</td>
<td>14.247567</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>428</td>
<td>1006</td>
<td>717</td>
<td>14.280933</td>
<td>33.566867</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1007</td>
<td>1213</td>
<td>1110</td>
<td>33.600233</td>
<td>40.473767</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>1214</td>
<td>1318</td>
<td>1266</td>
<td>40.507133</td>
<td>43.977267</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Take a few minutes to confirm that these shot boundaries correspond to the ones that you found when watching the video. You can even rewatch (and pause at each break) the video and see that it corresponds with the breaks that were automatically identified in the shot boundary detection.</p>
<p>In the next section we will look at face detection and identification. This is a common task when working with movie image data but ultimately works using individual images. To make this a bit easier to work with at first, we will use the following code to build a dataset with one frame from each of the detected shots in the video clip file. Specifically, we grab the middle frame from each cut, as specified in the <code>shot</code> data frame above.</p>
<div id="078ef660" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>img_list <span class="op">=</span> []</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> img, frame, msec <span class="kw">in</span> dvt.yield_video(<span class="st">"data/bewitched_sample.mp4"</span>):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> frame <span class="kw">in</span> shot[<span class="st">'mid'</span>].values:</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    img_list.append(img)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To get a sense of what these look like, we can combine the images and convert them into thumbnails to get a view of what the six shots from our sample video file look like.</p>
<div id="2cb5c862" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>img_comb <span class="op">=</span> np.hstack(img_list)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>img_comb <span class="op">=</span> cv2.resize(img_comb, (img_comb.shape[<span class="dv">1</span>] <span class="op">//</span> <span class="dv">5</span>, img_comb.shape[<span class="dv">0</span>] <span class="op">//</span> <span class="dv">5</span>))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(img_comb)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial-moving-images_files/figure-html/cell-10-output-1.png" width="566" height="105" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We will use the full versions of these six images in the next section as we identify the location, size, and identity of the characters in each shot.</p>
</section>
<section id="face-detection-and-facial-recognition" class="level2">
<h2 class="anchored" data-anchor-id="face-detection-and-facial-recognition">Face Detection and Facial Recognition</h2>
<p>Locating and identifying the faces present in a particular frame of a video file is a common way of understanding the narrative structure and visual style of the source material. In this section, we will see how to work with faces using the functions within the distant viewing toolkit (<strong>dvt</strong>) applied to the six images from the sample video file that we extracted in the previous section. After seeing how to do this with a static set of images, in the following section we will see how to put this together in a way that would scale to larger video files.</p>
<p>To get started, we will load the face detection annotator <code>AnnoFaces</code> that is included in the distant viewing toolkit. The first time this is run, the function will automatically download the relevant model files and save them locally in our Colab workspace.</p>
<div id="c90da08b" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>anno_face <span class="op">=</span> dvt.AnnoFaces()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/admin/gh/dvscripts/env/lib/python3.12/site-packages/dvt/face.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.mtcnn.load_state_dict(torch.load(model_path_mtcnn))
/Users/admin/gh/dvscripts/env/lib/python3.12/site-packages/dvt/face.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.vggface.load_state_dict(torch.load(model_path_vggface))</code></pre>
</div>
</div>
<p>To apply the annotation, we pass an image file to the <code>run</code> method of the annotator. Optionally, we can set the flag <code>visualize</code> to <code>True</code> in order to return a visualization of the detected face(s). Letâ€™s run the annotation over the third image (remembering that Python starts counting at zero, so the third image is selected by selecting the image in position 2).</p>
<div id="23aa066e" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>out_face <span class="op">=</span> anno_face.run(img_list[<span class="dv">2</span>], visualize<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Because we set the visualization flag to <code>True</code>, the returned results include an object called <code>img</code> that has a copy of the input frame along with a box around any detected faces. Here we see that the algorithm has detected one face, which is likely the number of faces a human annotator would also have found in the frame.</p>
<div id="7ae3d02c" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>plt.imshow(out_face[<span class="st">'img'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial-moving-images_files/figure-html/cell-13-output-1.png" width="580" height="398" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The visualization of the detected face is great for a one-off understanding of the algorithm. In order to store this information in a way that we can use for computational analyses, we need to have the location of the detected face represented in a strutured format. This is found in the <code>boxes</code> element of the output, which is designed to be convertable into a pandas data frame. Here is all of the structured information about the face detected in the frame above:</p>
<div id="3fe5b4cf" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(out_face[<span class="st">'boxes'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">face_id</th>
<th data-quarto-table-cell-role="th">x</th>
<th data-quarto-table-cell-role="th">xend</th>
<th data-quarto-table-cell-role="th">y</th>
<th data-quarto-table-cell-role="th">yend</th>
<th data-quarto-table-cell-role="th">prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0</td>
<td>279</td>
<td>413</td>
<td>119</td>
<td>299</td>
<td>0.999931</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>If you followed along with the introductory notebook using movie posters, this is the same information that we used to detect the number of faces present on each poster. One element that makes working with moving images different is that we usually have the same people showing up over and over again across frames and shots. Labeling the identity of the people present in each shot is an essential step in understanding the narrative structure of the material. The distant viewing toolkit (<strong>dvt</strong>) also includes information for doing facial recognition. This requires a bit more care as we need to start by identifying the characters that we want to identify in the first place.</p>
<p>By default, when we run the face detection algorithm, each detected faces is also assocaited with a sequence of 512 numbers called an <em>embedding</em>. The individual numbers do not have a direct meaning, but are instead defined in a relative way such that if two images of the same person are each associated with an embedding, we would expect the embeddings to be more similar to one another than they are to the embeddings of other faces. As an example, we can see the shape of the embedding object from our example frame above.</p>
<div id="816d6cb9" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>out_face[<span class="st">'embed'</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>(1, 512)</code></pre>
</div>
</div>
<p>One way to use these embeddings is to first identify portraits of the actors that we are interested in from the video file, and then associate each of these with a character name. Then, we can compute the embeddings of these faces and store them. Finally, each time we detect a face in a frame, we can check how similar the embedding of the detected face is compared to the faces in our reference set. If one of the reference characters is sufficently close, we will assume that we have found the associated character.</p>
<p>One of the objects that we downloaded in the setup section of this notebook was a folder with portraits of the four main actors from <em>Bewitched</em> named with the name of their character in the show. In the code below we cycle through these images and record the name and embedding of each of the faces. Also, for reference, we store a thumbnail of the actorâ€™s portrait.</p>
<div id="1a370339" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>face_embed <span class="op">=</span> []</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>face_name <span class="op">=</span> []</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>face_img <span class="op">=</span> []</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> path <span class="kw">in</span> <span class="bu">sorted</span>(os.listdir(<span class="st">"data/faces"</span>)):</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>  img <span class="op">=</span> dvt.load_image(<span class="st">"data/faces/"</span> <span class="op">+</span> path)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>  out_face <span class="op">=</span> anno_face.run(img)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>  face_embed <span class="op">+=</span> [out_face[<span class="st">'embed'</span>][<span class="dv">0</span>,:]]</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>  face_name <span class="op">+=</span> [path[:<span class="op">-</span><span class="dv">4</span>]]</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>  face_img <span class="op">+=</span> [cv2.resize(img, (<span class="dv">200</span>, <span class="dv">250</span>))]</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>face_embed <span class="op">=</span> np.vstack(face_embed)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>face_name <span class="op">=</span> np.array(face_name)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>face_img <span class="op">=</span> np.hstack(face_img)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here are the portraits of the characters. They are ordered in alphabetical order: Darrin, Endora, Larry, and Sam.</p>
<div id="5e948802" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>plt.imshow(face_img)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="tutorial-moving-images_files/figure-html/cell-17-output-1.png" width="575" height="209" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In order to measure the similarity of two embeddings, we use a mathematical technique called a <em>dot product</em>. This number will be <code>1</code> if two embeddings are exactly the same and <code>-1</code> if they are exactly opposite one another. Typically, two faces that are of different people will have a similar score somewhere close to zero. Letâ€™s start by seeing the similarity scores between the set of four portraits.</p>
<div id="ec43714f" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>np.dot(face_embed, face_embed.T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>array([[ 1.        ,  0.06135434, -0.19991453,  0.1696034 ],
       [ 0.06135434,  1.0000001 , -0.09233201, -0.03006548],
       [-0.19991453, -0.09233201,  0.9999998 , -0.03063131],
       [ 0.1696034 , -0.03006548, -0.03063131,  0.9999999 ]],
      dtype=float32)</code></pre>
</div>
</div>
<p>The first column is Darrin, and then compares Darrin to each of the four images. Since he is the first image, we see <code>.99999</code> because the photo is being compared to itself. Then, image of Darin is compared to Endora (<code>.06135</code>), Larry (<code>-.19993</code>), and Samantha (<code>.16960</code>).</p>
<p>The values along the diagonal are all 1 (or very close to one) because we are comparing one embedding to itself. All of the other similarity scores are somewhere between <code>-0.2</code> and <code>+.17</code>, which is as expected since each of the portraits shows a unique actor.</p>
<p>Now, letâ€™s compare these reference faces to the embedding of the individual frame that we started this section with: the middle frame from the third shot showing Samantha in a pink bedroom (<code>img_list[2]</code>).</p>
<div id="5f58226d" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>out_face <span class="op">=</span> anno_face.run(img_list[<span class="dv">2</span>])</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>dist <span class="op">=</span> np.dot(face_embed, out_face[<span class="st">'embed'</span>].T)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>dist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>array([[ 0.15695155],
       [ 0.08723371],
       [-0.04768787],
       [ 0.61289704]], dtype=float32)</code></pre>
</div>
</div>
<p>Here we see that there is a much large similarity score (<code>0.59</code>) between this face and the last character in our set. And if we look at the image, we see in fact that this is the same character: Samantha. We can associate the face with the character name algorithmically using the following code.</p>
<div id="6b9e3749" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>face_name[np.argmax(dist, axis <span class="op">=</span> <span class="dv">0</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>array(['sam'], dtype='&lt;U6')</code></pre>
</div>
</div>
<p>The code above associates the face with the character that has the highest similarity. It would be a good idea to also store the similarity score and then only trust the relationship if this score is sufficently large, which we can do with the following code.</p>
<div id="9fa8a65e" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">max</span>(dist, axis <span class="op">=</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>array([0.61289704], dtype=float32)</code></pre>
</div>
</div>
<p>Now that we understand how this process works for a single image, letâ€™s apply this technique to each of the shots in the sample file.</p>
</section>
<section id="building-annotations-from-a-video-file" class="level2">
<h2 class="anchored" data-anchor-id="building-annotations-from-a-video-file">Building Annotations from a Video File</h2>
<p>We can combine the techniques above with the <code>yield_video</code> function from <strong>dvt</strong> to identify faces in each of the shots. Itâ€™s possible to run the face detection algorithm over every frame, and this has some advantages. For the interest of time and simplicity, we will only use our face detection algorithm on the middle frame of each detected shot.</p>
<div id="d84ef3a9" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> []</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> img, frame, msec <span class="kw">in</span> dvt.yield_video(<span class="st">"data/bewitched_sample.mp4"</span>):</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> frame <span class="kw">in</span> shot[<span class="st">'mid'</span>].values:</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> anno_face.run(img)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> out[<span class="st">'boxes'</span>]:</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>      out_df <span class="op">=</span> pd.DataFrame(out[<span class="st">'boxes'</span>])</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>      out_df[<span class="st">'frame'</span>] <span class="op">=</span> frame</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>      out_df[<span class="st">'time'</span>] <span class="op">=</span> msec</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>      dist <span class="op">=</span> np.dot(face_embed, out[<span class="st">'embed'</span>].T)</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>      out_df[<span class="st">'character'</span>] <span class="op">=</span> face_name[np.argmax(dist, axis <span class="op">=</span> <span class="dv">0</span>)]</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>      out_df[<span class="st">'confidence'</span>] <span class="op">=</span> np.<span class="bu">max</span>(dist, axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>      output.append(out_df)</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> pd.concat(output)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> output[output[<span class="st">'prob'</span>] <span class="op">&gt;</span> <span class="fl">0.9</span>]</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">face_id</th>
<th data-quarto-table-cell-role="th">x</th>
<th data-quarto-table-cell-role="th">xend</th>
<th data-quarto-table-cell-role="th">y</th>
<th data-quarto-table-cell-role="th">yend</th>
<th data-quarto-table-cell-role="th">prob</th>
<th data-quarto-table-cell-role="th">frame</th>
<th data-quarto-table-cell-role="th">time</th>
<th data-quarto-table-cell-role="th">character</th>
<th data-quarto-table-cell-role="th">confidence</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0</td>
<td>303</td>
<td>420</td>
<td>87</td>
<td>256</td>
<td>0.998564</td>
<td>49</td>
<td>1.634967</td>
<td>sam</td>
<td>0.730811</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>0</td>
<td>241</td>
<td>455</td>
<td>181</td>
<td>451</td>
<td>0.999004</td>
<td>141</td>
<td>4.704700</td>
<td>endora</td>
<td>0.845351</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0</td>
<td>279</td>
<td>413</td>
<td>119</td>
<td>299</td>
<td>0.999931</td>
<td>306</td>
<td>10.210200</td>
<td>sam</td>
<td>0.612897</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>0</td>
<td>110</td>
<td>175</td>
<td>165</td>
<td>240</td>
<td>0.998715</td>
<td>717</td>
<td>23.923900</td>
<td>darrin</td>
<td>0.377601</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1</td>
<td>1</td>
<td>407</td>
<td>465</td>
<td>116</td>
<td>193</td>
<td>0.996172</td>
<td>717</td>
<td>23.923900</td>
<td>larry</td>
<td>0.509248</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>0</td>
<td>230</td>
<td>444</td>
<td>82</td>
<td>374</td>
<td>0.999645</td>
<td>1110</td>
<td>37.037000</td>
<td>darrin</td>
<td>0.685214</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0</td>
<td>225</td>
<td>428</td>
<td>151</td>
<td>410</td>
<td>0.999964</td>
<td>1266</td>
<td>42.242200</td>
<td>larry</td>
<td>0.803259</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Take a few minutes to go back to the thumbnail images from each of these detected faces and see how they line up with the characters that are actually present (if you are not familiar with <em>Bewitched</em>, use the portraits to learn each of the character names). You should see that they line up well, though the fourth shot has a fairly low confidence for Darrin due to the fact that his face is fairly small in the middle frame. We could do better if we added the first frame of the shot, which is more tightly focused on Darrin. We always recommend taking a look and reviewing the results as you go. Then, we can adjust our approach as needed based on the audiovisual data that we are working with and the areas of inquiry animating our analysis.</p>
</section>
<section id="full-corpus-analysis" class="level2">
<h2 class="anchored" data-anchor-id="full-corpus-analysis">Full Corpus Analysis</h2>
<p>We now have all of the methods and code needed to identify shots and faces in a moving image file. If we had an entire episode of <em>Bewitched</em>, we could run the exact same sequence of steps above on the full episode without changing anything. The only difference would be that the results would take much longer to finish running. If we had a folder with every episode of the series, we would just need to add one extra layer to our loop to cycle over each video file and make sure to include information about the filename on each row of the output. Finally, if we wanted to extend this to another series, we would just need to add the portraits of any characters that we wish to identify to our folder of reference images.</p>
<p>We return to the two shows â€” <em>Bewitched</em> and <em>I Dream of Jeanine</em> â€” that are central to Chapter 5 of <em>Distant Viewing</em>. The chapter centers around a comparison of the two magical sit-coms, which we can do by putting together shot boundary and then face detection and recognition.</p>
<p>The full set of video files for the two shows are quite large and the files are under copyright. It also takes a long time to process all of these files, particularly within a free Colab session. Since it is not possible to run the annotations directly on the full set here, we will instead work with the pre-computed annotations that were downloaded during the notebook setup. The structure of the dataset includes one row for each shot, with data about the timing of the shot and the number of faces.</p>
<div id="c5f97136" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>shots <span class="op">=</span> pd.read_csv(<span class="st">"data/ttl_sitcom_shots.csv"</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>shots</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">series</th>
<th data-quarto-table-cell-role="th">video</th>
<th data-quarto-table-cell-role="th">sid</th>
<th data-quarto-table-cell-role="th">frame_start</th>
<th data-quarto-table-cell-role="th">frame_stop</th>
<th data-quarto-table-cell-role="th">time</th>
<th data-quarto-table-cell-role="th">num_face</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Bewitched</td>
<td>bw_s01_e01</td>
<td>0</td>
<td>0</td>
<td>331</td>
<td>13.84</td>
<td>2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Bewitched</td>
<td>bw_s01_e01</td>
<td>1</td>
<td>332</td>
<td>508</td>
<td>7.38</td>
<td>2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Bewitched</td>
<td>bw_s01_e01</td>
<td>2</td>
<td>509</td>
<td>586</td>
<td>3.25</td>
<td>2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Bewitched</td>
<td>bw_s01_e01</td>
<td>3</td>
<td>587</td>
<td>657</td>
<td>2.96</td>
<td>2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Bewitched</td>
<td>bw_s01_e01</td>
<td>4</td>
<td>658</td>
<td>785</td>
<td>5.34</td>
<td>2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">102328</td>
<td>I Dream of Jeannie</td>
<td>idoj_s05_e26</td>
<td>161</td>
<td>32340</td>
<td>32474</td>
<td>5.63</td>
<td>2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">102329</td>
<td>I Dream of Jeannie</td>
<td>idoj_s05_e26</td>
<td>162</td>
<td>32475</td>
<td>32575</td>
<td>4.21</td>
<td>7</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">102330</td>
<td>I Dream of Jeannie</td>
<td>idoj_s05_e26</td>
<td>163</td>
<td>32576</td>
<td>33620</td>
<td>43.58</td>
<td>4</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">102331</td>
<td>I Dream of Jeannie</td>
<td>idoj_s05_e26</td>
<td>164</td>
<td>33621</td>
<td>33639</td>
<td>0.79</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">102332</td>
<td>I Dream of Jeannie</td>
<td>idoj_s05_e26</td>
<td>165</td>
<td>33640</td>
<td>33711</td>
<td>3.00</td>
<td>1</td>
</tr>
</tbody>
</table>

<p>102333 rows Ã— 7 columns</p>
</div>
</div>
</div>
<p>One quick metric that we can compute is the average shot length for each of the two series. This is a commonly used measurement to understand the pacing of a movie or television show. Here we see that Bewitched is slightly faster, with an average shot length of 5.3 seconds compared to the 6.2 seconds of <em>I Dream of Jeannie</em>.</p>
<div id="daf0e96c" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>shots.groupby([<span class="st">'series'</span>])[<span class="st">'time'</span>].mean().sort_values()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>series
Bewitched             5.323480
I Dream of Jeannie    6.249331
Name: time, dtype: float64</code></pre>
</div>
</div>
<p>A related measurement is the median shot length, which here shows that the median shot length is slightly longer in <em>Bewitched</em> compared to <em>Jeannie</em>. So, while a shot in <em>Bewitched</em> is slightly longer, there may be a set of particularly long shows in <em>Jeannie</em> that causes the average of that show to be longer.</p>
<div id="c183b800" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>shots.groupby([<span class="st">'series'</span>])[<span class="st">'time'</span>].median().sort_values()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>series
I Dream of Jeannie    3.0
Bewitched             3.3
Name: time, dtype: float64</code></pre>
</div>
</div>
<p>One of the interesting patterns that we noticed when writting the chapter based on these two series is that the average shot length is closely related to the number of faces present on the screen. We decided to truncate the number of faces to three (so that anything greater than 3 faces becomes 3) given our knowledge of the two shows. Notice how the average shot length increases with the number of faces on screen in both series.</p>
<div id="1778597f" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>shots.loc[shots[<span class="st">'num_face'</span>] <span class="op">&gt;</span> <span class="dv">3</span>, <span class="st">'num_face'</span>] <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>shots.groupby([<span class="st">'series'</span>, <span class="st">'num_face'</span>])[<span class="st">'time'</span>].mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>series              num_face
Bewitched           0            2.862049
                    1            3.787825
                    2            7.794772
                    3           11.485585
I Dream of Jeannie  0            3.288346
                    1            3.571409
                    2            9.444328
                    3           14.627574
Name: time, dtype: float64</code></pre>
</div>
</div>
<p>We saw an example of this pattern in our sample video file, where the one shot with two characters was much longer than the shot with a single character. This helps validate that the shot length is related to visual style. At least for these two series, it is related to how often we see a close up on an individual character talking (or acting) versus seeing multiple characters interacting together in a longer shot.</p>
<p>We also have a dataset that indicates the specific characters detected in shots from the two series. This uses a face detection algorithm similar to the one that we saw used in our sample video file. Here, we have one row for each detected character in a shot. There are four main characters in each of the series.</p>
<div id="9a604c3a" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>characters <span class="op">=</span> pd.read_csv(<span class="st">"data/ttl_sitcom_characters.csv"</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>characters</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">series</th>
<th data-quarto-table-cell-role="th">video</th>
<th data-quarto-table-cell-role="th">sid</th>
<th data-quarto-table-cell-role="th">time</th>
<th data-quarto-table-cell-role="th">character</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Bewitched</td>
<td>bw_s01_e01</td>
<td>1</td>
<td>7.38</td>
<td>Darrin</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Bewitched</td>
<td>bw_s01_e01</td>
<td>11</td>
<td>3.33</td>
<td>Darrin</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Bewitched</td>
<td>bw_s01_e01</td>
<td>49</td>
<td>2.58</td>
<td>Darrin</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Bewitched</td>
<td>bw_s01_e01</td>
<td>52</td>
<td>1.04</td>
<td>Darrin</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Bewitched</td>
<td>bw_s01_e01</td>
<td>53</td>
<td>4.33</td>
<td>Darrin</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">59798</td>
<td>I Dream of Jeannie</td>
<td>idoj_s05_e26</td>
<td>143</td>
<td>17.22</td>
<td>Roger</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">59799</td>
<td>I Dream of Jeannie</td>
<td>idoj_s05_e26</td>
<td>146</td>
<td>7.13</td>
<td>Roger</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">59800</td>
<td>I Dream of Jeannie</td>
<td>idoj_s05_e26</td>
<td>151</td>
<td>4.84</td>
<td>Roger</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">59801</td>
<td>I Dream of Jeannie</td>
<td>idoj_s05_e26</td>
<td>161</td>
<td>5.63</td>
<td>Roger</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">59802</td>
<td>I Dream of Jeannie</td>
<td>idoj_s05_e26</td>
<td>163</td>
<td>43.58</td>
<td>Roger</td>
</tr>
</tbody>
</table>

<p>59803 rows Ã— 5 columns</p>
</div>
</div>
</div>
<p>We can use the average amount of time that a character is on screen in a given episode as a rough measurement of the characterâ€™s visual importance in the show. Here, we compute this metric for each of the characters and sort in descending order within the series.</p>
<div id="74e3a39a" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>temp <span class="op">=</span> characters.groupby([<span class="st">'series'</span>, <span class="st">'video'</span>, <span class="st">'character'</span>])[<span class="st">'time'</span>].agg(<span class="st">'sum'</span>).reset_index()</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>temp <span class="op">=</span> temp.groupby([<span class="st">'series'</span>, <span class="st">'character'</span>])[<span class="st">'time'</span>].agg(<span class="st">'mean'</span>).reset_index()</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>temp[<span class="st">'time'</span>] <span class="op">=</span> temp[<span class="st">'time'</span>] <span class="op">/</span> <span class="dv">60</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>temp.sort_values([<span class="st">'series'</span>, <span class="st">'time'</span>], ascending<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">series</th>
<th data-quarto-table-cell-role="th">character</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">7</td>
<td>I Dream of Jeannie</td>
<td>Tony</td>
<td>10.972683</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">6</td>
<td>I Dream of Jeannie</td>
<td>Roger</td>
<td>5.930222</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>I Dream of Jeannie</td>
<td>Alfred</td>
<td>4.698559</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>I Dream of Jeannie</td>
<td>Jeannie</td>
<td>3.920746</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3</td>
<td>Bewitched</td>
<td>Samantha</td>
<td>6.986240</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>Bewitched</td>
<td>Darrin</td>
<td>6.859456</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Bewitched</td>
<td>Larry</td>
<td>4.086578</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Bewitched</td>
<td>Endora</td>
<td>2.751711</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>And, already we see one of the main conclusions of the analysis of these series: while they are thought of as very similar, their narrative structures are quite different. <em>Jeannie</em> is actually centered on the male lead Tony; the titual character Jeannie is often little more than a plot device who sets the action in motion. In constrast, <em>Bewitched</em> is most focused on the relationship between Samantha and Darrin, which each of them having nearly the same amount of overall screen time.</p>
<p>Our annotations - shot detection, face detection, and face recognition - can now be analyzed from many different angles to explore different aspects of the shows, which we delve further into in Chapter 5. Key here is that just three annotations open up analytical possibilities, and can address humanities questions about audiovisual data.</p>
</section>
<section id="conclusions-and-next-steps" class="level2">
<h2 class="anchored" data-anchor-id="conclusions-and-next-steps">Conclusions and Next Steps</h2>
<p>This notebook has given an introduction to use the distant viewing toolkit (<strong>dvt</strong>) to annotate movie image data from a digital video file. Weâ€™ve seen how to get basic metadata about a video file from within Python, how to cycle through the individual frames of a video file, and how to detect shot breaks using a custom algorithm. Although not specific to moving images, we also covered the process of using computer vision algorithms to do facial recognition, which is a common task in many applications but particularly useful when working with film and television corpora. Finally, we loaded a larger dataset showing all of the detected shots and characters across the entire runs of two U.S. Network-Era sitcoms and performed several example analyses on them.</p>
<p>For readers interested in more details about the specific case study of these two sitcoms, we suggest reading the fifth chapter of the <a href="https://www.distantviewing.org/book/"><em>Distant Viewing</em></a> book, available under an open access license. The first two chapters of the book may also be of interest as they offer a more general theoretical and methodological approach to the computational analysis of digital images.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./tutorial-digital-images.html" class="pagination-link" aria-label="1.1 Tutorial I: Digital Images">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">1.1 Tutorial I: Digital Images</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./image.html" class="pagination-link" aria-label="Images">
        <span class="nav-page-text">Images</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Distant Viewing Scripts</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>