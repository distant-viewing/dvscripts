[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Distant Viewing Scripts",
    "section": "",
    "text": "Welcome!\n\n\n\nWelcome! Distant Viewing Scripts is a free, open-source website that provides tutorials and scripts for doing computational analysis with visual and multimodal data. All of the code is written using the Python programming language, with open-source models and freely accessible data. Please use the table of contents navigation n the left to access the pages of the site.\nDistant Viewing Scripts is designed to complement the Distant Viewing Explorer, which provides web-based tools for applying and visualizing AI models directly in the browser. For those new to programming and/or the application of computer vision models, we recommend starting with the Distant Viewing Explorer to get a quick hands-on sense of the methods and approaches before continuing with the material here.\nThe first section includes several longer tutorials. The first tutorial gives a gentle introduction to image analysis. It includes slides and discussion questions, making it an excellent starting point for both classroom use as well as self-study. The second tutorial expands the techniques from the first to include working with moving images. The final tutorial explores how to create and organize your own visual collections.\nThe remainder of the scripts are grouped by data type, with one page dedicated to a different type of model. Many of these mirror the interactive visualizations from the Distant Viewing Explorer website. The code should be easy to adapt to your own datasets and to expand with other models of a similar kind that are available online.\n\nTaylor Arnold  Lauren Tilton  Directors, Distant Viewing Lab, University of Richmond\n\n\n\n\n\n\nAchiam, Josh, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, et al. 2023. “GPT-4 Technical Report.” arXiv Preprint arXiv:2303.08774.\n\n\nAdnan, Myasar Mundher, Mohd Shafry Mohd Rahim, Amjad Rehman, Zahid Mehmood, Tanzila Saba, and Rizwan Ali Naqvi. 2021. “Automatic Image Annotation Based on Deep Learning Models: A Systematic Review and Future Challenges.” IEEE Access 9: 50253–64.\n\n\nAfzal, Shehzad, Sohaib Ghani, Mohamad Mazen Hittawe, Sheikh Faisal Rashid, Omar M Knio, Markus Hadwiger, and Ibrahim Hoteit. 2023. “Visualization and Visual Analytics Approaches for Image and Video Datasets: A Survey.” ACM Transactions on Interactive Intelligent Systems 13 (1): 1–41.\n\n\nAnitha Kumari, K, C Mouneeshwari, RB Udhaya, and R Jasmitha. 2020. “Automated Image Captioning for flickr8k Dataset.” In Proceedings of International Conference on Artificial Intelligence, Smart Grid and Smart City Applications: AISGSC 2019, 679–87. Springer.\n\n\nArchives, United States National. n.d. “DOCUMERICA: The Environmental Protection Agency’s Program to Photographically Document Subjects of Environmental Concern, 1972–1977.” https://catalog.archives.gov/id/542493.\n\n\nArnold, Taylor, and Lauren Tilton. 2019. “Distant Viewing: Analyzing Large Visual Corpora.” Digital Scholarship in the Humanities 34 (Supplement_1): i3–16.\n\n\n———. 2020. “Distant Viewing Toolkit: A Python Package for the Analysis of Visual Culture.” Journal of Open Source Software 5 (45): 1800.\n\n\n———. 2023. Distant Viewing: Computational Exploration of Digital Images. MIT Press.\n\n\nChen, Hailin, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut, Ruochen Zhao, Caiming Xiong, and Shafiq Joty. 2023. “ChatGPT’s One-Year Anniversary: Are Open-Source Large Language Models Catching Up?” arXiv Preprint arXiv:2311.16989.\n\n\nColeman, Catherine Nicole. 2020. “Managing Bias When Library Collections Become Data.” International Journal of Librarianship 5 (1): 8–19.\n\n\nCuntz, Alexander, Paul J Heald, and Matthias Sahli. 2023. “Digitization and Availability of Artworks in Online Museum Collections.” World Intellectual Property Organization (WIPO) Economic Research Working Paper Series, no. 75.\n\n\nDeal, Laura. 2015. “Visualizing Digital Collections.” Technical Services Quarterly 32 (1): 14–34.\n\n\nDemiralp, Çagatay, Carlos E Scheidegger, Gordon L Kindlmann, David H Laidlaw, and Jeffrey Heer. 2014. “Visual Embedding: A Model for Visualization.” IEEE Computer Graphics and Applications 34 (1): 10–15.\n\n\nDi Lenardo, Isabella, Benoı̂t Laurent Auguste Seguin, and Frédéric Kaplan. 2016. “Visual Patterns Discovery in Large Databases of Paintings.” In Digital Humanities 2016.\n\n\nDı́az-Rodrı́guez, Natalia, and Galena Pisoni. 2020. “Accessible Cultural Heritage Through Explainable Artificial Intelligence.” In Adjunct Publication of the 28th ACM Conference on User Modeling, Adaptation and Personalization, 317–24.\n\n\nFlueckiger, Barbara, and Gaudenz Halter. 2020. “Methods and Advanced Tools for the Analysis of Film Colors in Digital Humanities.” DHQ: Digital Humanities Quarterly 14 (4).\n\n\nFraser, Kathleen C, Svetlana Kiritchenko, and Isar Nejadgholi. 2023. “A Friendly Face: Do Text-to-Image Systems Rely on Stereotypes When the Input Is Under-Specified?” arXiv Preprint arXiv:2302.07159.\n\n\nGefen, Alexandre, Léa Saint-Raymond, and Tommaso Venturini. 2021. “AI for Digital Humanities and Computational Social Sciences.” Reflections on Artificial Intelligence for Humanity, 191–202.\n\n\nHiippala, Tuomo, and John A Bateman. 2022. “Semiotically-Grounded Distant Viewing of Diagrams: Insights from Two Multimodal Corpora.” Digital Scholarship in the Humanities 37 (2): 405–25.\n\n\nKing, Ryan C, Vishnu Bharani, Kunal Shah, Yee Hui Yeo, and Jamil S Samaan. 2024. “GPT-4V Passes the BLS and ACLS Examinations: An Analysis of GPT-4V’s Image Recognition Capabilities.” Resuscitation 195.\n\n\nKlinkert, Ivo, Liam A McDonnell, Stefan L Luxembourg, AF Maarten Altelaar, Erika R Amstalden, Sander R Piersma, and Ron Heeren. 2007. “Tools and Strategies for Visualization of Large Image Data Sets in High-Resolution Imaging Mass Spectrometry.” Review of Scientific Instruments 78 (5).\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLee, Benjamin Charles Germain. 2023. “The ‘Collections as ML Data’ Checklist for Machine Learning and Cultural Heritage.” Journal of the Association for Information Science and Technology.\n\n\nLei, Yiming, Zilong Li, Yangyang Li, Junping Zhang, and Hongming Shan. 2024. “LICO: Explainable Models with Language-Image Consistency.” Advances in Neural Information Processing Systems 36.\n\n\nLiu, Fang, Mohan Zhang, Baoying Zheng, Shenglan Cui, Wentao Ma, and Zhixiong Liu. 2023. “Feature Fusion via Multi-Target Learning for Ancient Artwork Captioning.” Information Fusion 97: 101811.\n\n\nMcInnes, Leland, John Healy, and James Melville. 2018. “UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.” arXiv Preprint arXiv:1802.03426.\n\n\nMeinecke, Christofer, Chris Hall, and Stefan Jänicke. 2022. “Towards Enhancing Virtual Museums by Contextualizing Art Through Interactive Visualizations.” ACM Journal on Computing and Cultural Heritage 15 (4): 1–26.\n\n\nMoreux, Jean-Philippe. 2023. “Intelligence Artificielle Et Indexation Des Images.” In Journées Du Patrimoine écrit:“l’image Aura-t-Elle Le Dernier Mot? Regards Croisés Sur Les Collections Iconographiques En Bibliothèques”.\n\n\nMorse, Christopher, Blandine Landau, Carine Lallemand, Lars Wieneke, and Vincent Koenig. 2022. “From #Museumathome to #Athomeatthemuseum: Digital Museums and Dialogical Engagement Beyond the COVID-19 Pandemic.” ACM Journal on Computing and Cultural Heritage (JOCCH) 15 (2): 1–29.\n\n\nMurtagh, Fionn, and Pierre Legendre. 2014. “Ward’s Hierarchical Agglomerative Clustering Method: Which Algorithms Implement Ward’s Criterion?” Journal of Classification 31: 274–95.\n\n\nPaiss, Roni, Hila Chefer, and Lior Wolf. 2022. “No Token Left Behind: Explainability-Aided Image Classification and Generation.” In European Conference on Computer Vision, 334–50. Springer.\n\n\nPetukhova, Alina, Joao P Matos-Carvalho, and Nuno Fachada. 2024. “Text Clustering with LLM Embeddings.” arXiv Preprint arXiv:2403.15112.\n\n\nPuscasiu, Adela, Alexandra Fanca, Dan-Ioan Gota, and Honoriu Valean. 2020. “Automated Image Captioning.” In 2020 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR), 1–6. IEEE.\n\n\nQi, Zhongang, Saeed Khorram, and Li Fuxin. 2021. “Embedding Deep Networks into Visual Explanations.” Artificial Intelligence 292: 103435.\n\n\nQi, Zhongang, and Fuxin Li. 2017. “Learning Explainable Embeddings for Deep Networks.” In NIPS Workshop on Interpretable Machine Learning. Vol. 31.\n\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” In International Conference on Machine Learning, 8748–63. PMLR.\n\n\nRinaldi, Antonio M, Cristiano Russo, and Cristian Tommasino. 2023. “Automatic Image Captioning Combining Natural Language Processing and Deep Neural Networks.” Results in Engineering 18: 101107.\n\n\nSheng, Shurong, and Marie-Francine Moens. 2019. “Generating Captions for Images of Ancient Artworks.” In Proceedings of the 27th ACM International Conference on Multimedia, 2478–86.\n\n\nSiddiqui, Nabeel. 2024. “Cutting the Frame: An in-Depth Look at the Hitchcock Computer Vision Dataset.” Journal of Open Humanities Data 10 (1).\n\n\nSmits, Thomas, and Melvin Wevers. 2023. “A Multimodal Turn in Digital Humanities. Using Contrastive Machine Learning Models to Explore, Enrich, and Analyze Digital Visual Historical Collections.” Digital Scholarship in the Humanities 38 (3): 1267–80.\n\n\nStefanowitsch, Anatol. 2020. Corpus Linguistics: A Guide to the Methodology. Language Science Press.\n\n\nStraka, Milan, Jan Hajic, and Jana Straková. 2016. “UDPipe: Trainable Pipeline for Processing CoNLL-u Files Performing Tokenization, Morphological Analysis, Pos Tagging and Parsing.” In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), 4290–97.\n\n\nTan, Mingxing, and Quoc Le. 2019. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” In International Conference on Machine Learning, 6105–14. PMLR.\n\n\nVerma, Akash, Arun Kumar Yadav, Mohit Kumar, and Divakar Yadav. 2024. “Automatic Image Caption Generation Using Deep Learning.” Multimedia Tools and Applications 83 (2): 5309–25.\n\n\nWevers, Melvin, and Thomas Smits. 2020. “The Visual Digital Turn: Using Neural Networks to Study Historical Images.” Digital Scholarship in the Humanities 35 (1): 194–207.\n\n\nWhitelaw, Mitchell. 2015. “Generous Interfaces for Digital Cultural Collections.” Digital Humanities Quarterly 9 (1): 1–16.\n\n\nWindhager, Florian, Paolo Federico, Günther Schreder, Katrin Glinka, Marian Dörk, Silvia Miksch, and Eva Mayr. 2018. “Visualization of Cultural Heritage Collection Data: State of the Art and Future Challenges.” IEEE Transactions on Visualization and Computer Graphics 25 (6): 2311–30.\n\n\nWu, Wenhao, Huanjin Yao, Mengxi Zhang, Yuxin Song, Wanli Ouyang, and Jingdong Wang. 2023. “GPT4Vis: What Can GPT-4 Do for Zero-Shot Visual Recognition?” arXiv Preprint arXiv:2311.15732.\n\n\nYe, Yilin, Rong Huang, and Wei Zeng. 2022. “VISAtlas: An Image-Based Exploration and Query System for Large Visualization Collections via Neural Image Embedding.” IEEE Transactions on Visualization and Computer Graphics.\n\n\nYin, Shukang, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023. “A Survey on Multimodal Large Language Models.” arXiv Preprint arXiv:2306.13549.\n\n\nYou, Haoxuan, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. 2023. “Ferret: Refer and Ground Anything Anywhere at Any Granularity.” arXiv Preprint arXiv:2310.07704.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "This section contains three tutorials that show how to use Python to apply computational techniques to visual materials. The first tutorial provides a gentle introduction to the structure of digital images and how to work with them. The second tutorial expands this to video files. The third is focused on how to build your own corpus of visual data.",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorial-digital-images.html",
    "href": "tutorial-digital-images.html",
    "title": "1.1 Tutorial I: Digital Images",
    "section": "",
    "text": "Setup\nThis notebook provides an introduction to the methods presented in the book Distant Viewing: Computational Exploration of Digital Images (MIT Press, 2023). We replicate and extend portions of the analysis using a collection of 5000 movie posters presented in the the third chapter of the book. We do not assume any prior knowledge of Python or computer vision in these notes. While a complete introduction to Python is not in the scope of our introduction, we do our best to highlight the main features of the language as they apply to the application here. Here are the specific learning outcomes for the tutorial:\nFor further information about the distant viewing toolkit (dvt), the open-source Python package that we have developed, please see the project’s homepage. More information about the theory of distant viewing and the specific application to movie image posters can be the found in our book, which is available to download for free under an open access license on our website along with additional data and code to replicate the other studies shown in the text. A second notebook following up on the methods here using moving images can be found here.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport dvt\nimport cv2\nIn Google Colab, your working environment resets itself everytime you reopen a notebook. Therefore, all of the steps above need to be re-run each time that you start the notebook. If you were running this code on your own machine, the installation of the dvt package and downloading the data would only need to be done once. Loading the modules in the final code chunk, however, always needs to be run each time that Python is restarted.",
    "crumbs": [
      "Tutorials",
      "1.1 Tutorial I: Digital Images"
    ]
  },
  {
    "objectID": "tutorial-digital-images.html#movie-poster-dataset",
    "href": "tutorial-digital-images.html#movie-poster-dataset",
    "title": "1.1 Tutorial I: Digital Images",
    "section": "Movie Poster Dataset",
    "text": "Movie Poster Dataset\nBefore we jump into the analysis of the movie posters images, it is important to take a moment to look at the metadata that we have attached to each poster and to understand the structure of the dataset. In the code below, we use the read_csv function from the pandas module (which we have given the short name pd following standard Python conventions) to load the csv file that has one row for each movie in our dataset. We will save the output of the function as an object named posters. In the final line of the code, we write the object name all by itself, which causes the first file lines of the dataset to be printed inside of the notebook for us to look at. The data contains one row for each of the 100 top grossing films for each year from 1970 through 2019. For a few movies during the 1970s we were not able to find the movie posters; these are excluded from the dataset. For each movie, we have the year, the title, the file name of the associated image of the movie poster, and a description of the half decade that the movie comes from. The latter will be used in our analysis of change over time.\n\nposters = pd.read_csv(\"data/movies_50_years_meta.csv\")\nposters\n\n\n\n\n\n\n\n\nyear\ntitle\npath\nperiod\n\n\n\n\n0\n1970\nLove Story\nthm/1970_love_story.jpg\n1970-1974\n\n\n1\n1970\nAirport\nthm/1970_airport.jpg\n1970-1974\n\n\n2\n1970\nM.A.S.H.\nthm/1970_mash.jpg\n1970-1974\n\n\n3\n1970\nPatton\nthm/1970_patton.jpg\n1970-1974\n\n\n4\n1970\nLittle Big Man\nthm/1970_little_big_man.jpg\n1970-1974\n\n\n...\n...\n...\n...\n...\n\n\n4675\n2019\nThe Art of Self-Defense\nthm/2019_the_art_of_selfdefense.jpg\n2015-2019\n\n\n4676\n2019\nLuce\nthm/2019_luce.jpg\n2015-2019\n\n\n4677\n2019\nThe Other Side of Heaven 2: Fire of Faith\nthm/2019_the_other_side_of_heaven__fire_of_fai...\n2015-2019\n\n\n4678\n2019\nThe Aftermath\nthm/2019_the_aftermath.jpg\n2015-2019\n\n\n4679\n2019\nThe Kid\nthm/2019_the_kid.jpg\n2015-2019\n\n\n\n\n4680 rows × 4 columns\n\n\n\nWe have another set of metadata that associates each film with one or more genre categories. The dataset contains one row for each pair of film and genre tag. The year is included because there are several films that share the same title, but can be uniquely identified by knowing the title and year of the film.\n\ngenre = pd.read_csv(\"data/movies_50_years_tags.csv\")\ngenre\n\n\n\n\n\n\n\n\nyear\ntitle\ngenre\n\n\n\n\n0\n1970\nLove Story\nDrama\n\n\n1\n1970\nLove Story\nRomance\n\n\n2\n1970\nAirport\nAction\n\n\n3\n1970\nAirport\nDrama\n\n\n4\n1970\nAirport\nThriller\n\n\n...\n...\n...\n...\n\n\n11309\n2019\nThe Other Side of Heaven 2: Fire of Faith\nDrama\n\n\n11310\n2019\nThe Aftermath\nDrama\n\n\n11311\n2019\nThe Aftermath\nRomance\n\n\n11312\n2019\nThe Kid\nBiography\n\n\n11313\n2019\nThe Kid\nDrama\n\n\n\n\n11314 rows × 3 columns\n\n\n\nLooking at the first and last few rows, do the genre tags seem reasonable for the given films? Our analyses in the remainder of the notebook will focus on movie poster patterns across time periods and genres.\nNow that we have a sense of our data, what kinds of questions might we be interesed in exploring?",
    "crumbs": [
      "Tutorials",
      "1.1 Tutorial I: Digital Images"
    ]
  },
  {
    "objectID": "tutorial-digital-images.html#digital-images",
    "href": "tutorial-digital-images.html#digital-images",
    "title": "1.1 Tutorial I: Digital Images",
    "section": "Digital Images",
    "text": "Digital Images\nNow that we have seen the metadata for the movie posters, let’s look at how digital images are manipulated in Python by loading in the image from a single movie poster. All of the movie posters are stored as JPEG files (an abbreviation for the Joint Photographic Experts Group). This is a common image format that can be opened and understood by nearly any program or device that works with images. If you opened a JPEG file on your computer or phone, it would display the image without any special setup required. Many of the images that you see on public websites are stored as JPEG files and are processed and displayed by your browser.\nIn the code below, we use the function dvt.load_image to load an image into Python. We save the image as an object named img. The path to the poster image is taken directly from the metadata above. Here, we are using the formula of taking the name of the dataset (posters) followed by a period and the name of the column (path) followed by the row number in square brackets ([10]). We have selected the poster from the John Wayne film The Legend because it has a distinct orange tone that will be interesting to look at. After loading the image into Python, we print out the image object in the second line by including it on it’s own final line.\n\nimg = dvt.load_image(\"data/\" + posters.path[10])\nplt.imshow(img)\n\n\n\n\n\n\n\n\nWe see that it’s relatively easy to load a common image file format into Python and then to display it within the notebook. In order to best understand the computational analyses that follow, it will be helpful to investigate the the way that digital image is represented inside of Python. We can use the built-in Python function type to see the obect type of any Python object. Let’s do that below:\n\ntype(img)\n\nnumpy.ndarray\n\n\nYou may have reasonably assumed that the object would have a name related to the fact that it contains an image. But, we see that it is cryptically called a numpy.ndarray. What does this mean? This is a generic data type created by the numpy library (the same one that we loaded above in the setup section) to store rectangular blocks of numbers.\nTo understand how an array of numbers can represent an image, we will print out the object’s shape attribute (an attribute is a characteristic of a Python object that we can access with the object name followed by a . and the attribute name). This tells us how the numbers in array are arranged.\n\nimg.shape\n\n(229, 150, 3)\n\n\nWe see that the shape of the object has three components. The first number tells us how many rows of numbers there are and the second tells us how many columns there are. The third number indicates that there is a third dimension with a size of three. The easiest way to picture this is to think of having three sets of rectangular grids of numbers, each with 229 rows and 150 numbers. Think of an Excel file with three sheets, each having a grid of numbers of the same size.\nA specific row and column in this grid of numbers represents a pixel (picture + element), the smallest individal component of an image. We need three different numbers for each pixel to indicate the amount of red, green, and blue light that is needed to combine to create the color at each particular location. The Python library that we are using here represents the quantity of light on a scale from 0 (not turned on at all) through 255 (as bright as possible). Blending these three components together can recreate nearly any color observable by the human eye.\nTo make this more concrete, let’s see an example of the numbers that create the image above. There are far too many to look at all at once. Instead, we will use a bracket notation to select the first ten rows, first eight columns, and the first color component. Python uses a convention that is common in computer programing that starts counting at zero, so the 0 below grabs from the first array of numbers, which here correspond to the red color intensity.\n\nimg[:10, :8, 0]\n\narray([[202, 206, 205, 204, 208, 207, 204, 208],\n       [208, 206, 206, 206, 205, 206, 207, 206],\n       [204, 204, 210, 208, 197, 197, 206, 209],\n       [207, 204, 208, 207, 200, 202, 208, 208],\n       [209, 206, 205, 206, 207, 208, 209, 206],\n       [204, 206, 207, 206, 206, 205, 205, 207],\n       [206, 206, 206, 206, 208, 207, 207, 207],\n       [206, 204, 205, 206, 206, 207, 208, 206],\n       [206, 206, 206, 206, 206, 206, 207, 208],\n       [206, 206, 206, 206, 206, 207, 207, 208]], dtype=uint8)\n\n\nThe portion of the image that we grabbed above is the far upper left-hand corner. We see that to represent this corner we need to turn on a lot of red light (around 200ish out a possible 255). However, if we look at the image there appear to be no color red anywhere. The upper left-hand corner appears to be white. To understand how this is the case, let’s look at the second component, which is the amount of green light.\n\nimg[:10, :8, 1]\n\narray([[202, 206, 205, 204, 208, 207, 204, 208],\n       [208, 206, 206, 206, 205, 206, 207, 206],\n       [204, 204, 210, 208, 197, 197, 206, 209],\n       [207, 204, 208, 207, 200, 202, 208, 208],\n       [209, 206, 205, 206, 207, 208, 209, 206],\n       [204, 206, 207, 206, 206, 205, 205, 207],\n       [206, 206, 206, 206, 208, 207, 207, 207],\n       [206, 204, 205, 206, 206, 207, 208, 206],\n       [206, 206, 206, 206, 206, 206, 207, 208],\n       [206, 206, 206, 206, 206, 207, 207, 208]], dtype=uint8)\n\n\nAnd while we are at it, also the amount of blue light.\n\nimg[:10, :8, 2]\n\narray([[202, 206, 205, 204, 208, 207, 204, 208],\n       [208, 206, 206, 206, 205, 206, 207, 206],\n       [206, 206, 212, 210, 199, 199, 208, 211],\n       [209, 206, 210, 209, 202, 204, 210, 210],\n       [211, 208, 207, 208, 209, 210, 211, 208],\n       [206, 208, 209, 208, 208, 207, 207, 209],\n       [208, 208, 208, 208, 210, 209, 209, 209],\n       [208, 206, 207, 208, 208, 209, 210, 208],\n       [208, 208, 208, 208, 208, 208, 209, 210],\n       [208, 208, 208, 208, 208, 209, 209, 210]], dtype=uint8)\n\n\nLooking above, we see that the red, green, and blue lights are all turned on at the same level in the upper left-hand corner of the image. When we blend light from all three colors together, we get a shade of grey. This is something closer to black when the colors are all turned low, and something closer to white when the colors are all turned higher. So now this approximates what we see in the upper left corner of the image, corresponding to a shade of grey that is very close to white.\nTo solidify our understanding how these components work, let’s see another part of the image corresponding to rows 180-190 and columns 25-30. It’s very small, but by looking closely we should be able to connect it to the image above. The resulting image is too small for Colab to automatically treat as an image for display purposes, so we need to use the plt.imshow function to display the pixel values as pixels.\n\nplt.imshow(img[180:190, 25:30, :])\n\n\n\n\n\n\n\n\nThis small window of the image is part of the orange at the bottom of the movie poster. Let’s see the red, green, and blue components that make up this color.\n\nimg[180, 25, :]\n\narray([189, 106,   0], dtype=uint8)\n\n\nThe dark orange color comes from blending a good amount of red light (189/255), a bit of green (106/255), and no blue (0/255) together. Looking at a color wheel should help explain why orange comes from mixing a bit of green with a larger bit of red.\nDifferent image processing libraries have slightly different conventions for how to represent digital images. Most use the same ordering of the colors (red, green, blue), and some use fractions between 0 and 1 rather than integers between 0 and 255. Certain image formats include a fourth component, called an alpha channel, to represent image opacity. Other formats contain a single color channel to represent grayscale images. However, all of these formats have the same underlying concept of representing digital images through numbers that indicate pixel intensities. This is a very different way of thinking about images than the way that humans process visual signals and is something that we will explore in the next section.\nNow we know we can look closely at color within the posters, which we can combine with the metadata. What are additional questions that we can ask about movie posters?",
    "crumbs": [
      "Tutorials",
      "1.1 Tutorial I: Digital Images"
    ]
  },
  {
    "objectID": "tutorial-digital-images.html#distant-viewing-theory",
    "href": "tutorial-digital-images.html#distant-viewing-theory",
    "title": "1.1 Tutorial I: Digital Images",
    "section": "Distant Viewing: Theory",
    "text": "Distant Viewing: Theory\nComputers represent images by understanding them as three-dimensional arrays of numbers. This is very different from the way that images are interpreted and used by human viewers. Furthermore, the connection between these two representations is not at all obvious. There is no way to understand what is being represented by a small subset of pixel intensities without seeing a large part of the image as a whole. Even something as simple as the amount of blue light in a given pixel can be difficult to understand. A lot of blue could be the color blue, or could just be blended with red and green to make white. So, in order to do computational analyses with large collections of digital images, we first need to convert these raw pixel intensities into representations that match the interpretations of the images that we are interested in studying.\nThe theory behind distant viewing stems from this exact realization. Namely, that the way digital images are represented forces us to construct annotations that hold structured data that aligns with our research questions. These annotations, which can be created manually or using computational algorithms, are both destructive (there is information lost in the process of creating them) and open to interpretation (there is never a neutral way of creating annotations; decisions always need to be made about how they are created).\nWe will work towards more complex annotations, but let’s start with one of the most straightforward: image brightness. Pixel intensities tell us how much to turn on the red, green, and blue lights at each postion of the image to create a display of a given image. The higher these numbers are, it stands to reason, the brighter the image will be when shown on a digital display. So, one way to represent a meaningful annotation about an image is to take the average value of all pixel intensities. We can do this with the following code, which uses the numpy function mean to compute the average (mean) of all the values in an array.\n\nnp.mean(img)\n\nnp.float64(93.82433770014556)\n\n\nLittle argument probably needs to be made to convince someone that a lot of information is lost between this one number and all of the rich information that is present in the thumbnail image of the movie poster. There is no information about the content of the text in the image, the dominant orange color, the shape of the man and the horse, the white border, or the way each of these elements is arranged in the frame. So, clearly this process of creating annotations is a destructive one. The difference between the summarized annotation (a single number) and the information in the original image is known in information theory as a semantic gap. But what about the second part of the theory, that this measurement is non-netural and represents specific choices about how we want to view at a distance? While this may seem less obvious, there are many different ways of measuring the brightness of an image. For example, we might want to consider the median value of the pixel intensities rather than their average value as in the code below.\n\nnp.median(img)\n\nnp.float64(75.0)\n\n\nAlternatively, because the human eye is more sensitive to green than blue or red light, we might want to weight the brightness more heavily based on the color of light that is being used. Many movie posters have black or white borders, which could heavily influence the brightness of the image as a whole. Perhaps we would want to only take the brightness at the middle part of the image. And of course, why do we even care about the brightness of the image in the first place? Once we start thinking about all of these different options, it should become clear that there is no perfect way to represent any element of an image as structured data. Choices and tradeoffs are always being made. Eventually we need to make some of those choices and see what we can learn with them, while keeping the caveats about the nature of image annotations and the resulting semantic gaps always in the back of our mind. In other words, when we are analyzing images through computer vision, we are distant viewing.",
    "crumbs": [
      "Tutorials",
      "1.1 Tutorial I: Digital Images"
    ]
  },
  {
    "objectID": "tutorial-digital-images.html#annotating-image-brightness",
    "href": "tutorial-digital-images.html#annotating-image-brightness",
    "title": "1.1 Tutorial I: Digital Images",
    "section": "Annotating Image Brightness",
    "text": "Annotating Image Brightness\nWe have now carefully worked through the way the digital images are stored, understood the implication for this in terms of the theory of distant viewing, and shown one particular way of constructing an annotation through image brightness. Let’s now put this together to do an analysis of the movie posters based on their overall brightness. As a first step, we need to repeat the process used with the one poster above to all of the images in the dataset. In order to do this we use a loop in Python. This consists of the keyword for, followed by a block of indented code. Each of the lines of the indented code will be run once for every value of the iteration variable ind from the set of row numbers in the posters dataset. So, we will load the image of each poster into Python, compute the image brightness, and the save the brightness in a new column that we created in the dataset. To match the results in the book as closely as possible, we will divide the brightness by 255 so that the values range from 0 (completely black) to 1 (completely white).\n\nposters['avg_brightness'] = 0.0\nfor ind in posters.index:\n  img = dvt.load_image(\"data/\" + posters.loc[ind, 'path'])\n  posters.loc[ind, 'avg_brightness'] = np.mean(img) / 255\n\nNow that we have added the annotation of the image brightness to each of the rows of the poster data, we can arrange the posters from the brightest to the darkest using the sort_values method.\n\nposters = posters.sort_values('avg_brightness', ascending=False)\nposters\n\n\n\n\n\n\n\n\nyear\ntitle\npath\nperiod\navg_brightness\n\n\n\n\n3634\n1999\nMy Favorite Martian\nthm/1999_my_favorite_martian.jpg\n1995-1999\n0.959253\n\n\n4475\n2015\nSteve Jobs\nthm/2015_steve_jobs.jpg\n2015-2019\n0.957483\n\n\n4569\n2017\nDownsizing\nthm/2017_downsizing.jpg\n2015-2019\n0.957002\n\n\n2465\n1972\nInstant Replay (In Situ Installation)\nthm/1972_instant_replay_in_situ_installation.jpg\n1970-1974\n0.953151\n\n\n4140\n2009\nPrecious\nthm/2009_precious.jpg\n2005-2009\n0.949430\n\n\n...\n...\n...\n...\n...\n...\n\n\n4194\n2010\nPredators\nthm/2010_predators.jpg\n2010-2014\n0.036327\n\n\n3406\n1994\nWes Craven's New Nightmare\nthm/1994_wes_cravens_new_nightmare.jpg\n1990-1994\n0.034680\n\n\n3287\n1992\nConsenting Adults\nthm/1992_consenting_adults.jpg\n1990-1994\n0.034447\n\n\n1723\n2006\nSaw III\nthm/2006_saw_iii.jpg\n2005-2009\n0.033579\n\n\n592\n1983\nZelig\nthm/1983_zelig.jpg\n1980-1984\n0.030483\n\n\n\n\n4680 rows × 5 columns\n\n\n\nThe theory of distant viewing tells us that the creation of annotations, while a necessary step in computational analysis of images, is both destructive and subjective. So, before continuing to an aggregative analysis, it is useful to connect the annotations back to the images by actually looking at some of the posters. One way to do that is to look at posters with extreme values. In the code below we load the first image in the sorted dataset, which is the poster that has been assigned the highest brightness value. Remember that Python starts counting at zero. The zero in the first line corresponds to the first row of the data.\nFeel free to look at other particularly bright rows to get a fuller picture of what is being captured by the annotation.\n\nimg = dvt.load_image(\"data/\" + posters.iloc[0].path)\nplt.imshow(img)\n\n\n\n\n\n\n\n\nIt’s similarly useful to look at the darkest images in the dataset. To do that we modify the code to start at the number of rows in the data minus one (again, because of Python’s convention of starting to count at zero). You can change the minus 1 to minus n to look at the n’th least bright image in the data.\n\nimg = dvt.load_image(\"data/\" + posters.iloc[posters.shape[0] - 1].path)\nplt.imshow(img)\n\n\n\n\n\n\n\n\nAfter looking at some example posters, how do you feel about the annotation’s ability to capture poster brightness? At least at the extremes, what feature(s) of the poster seem to best explain/predict the brightness of the image?\nNow that we have some understanding of how the annotation works, and hopefully some confidence that it corresponds with some meaningful quantity, let’s do some aggregative analysis with the annotations. In the code below we group out dataset by period (half-decades from 1970 through 2019) and look at the mean average brightness of all posters from that period.\n\nposters.groupby(['period'])['avg_brightness'].mean()\n\nperiod\n1970-1974    0.575523\n1975-1979    0.525609\n1980-1984    0.471467\n1985-1989    0.443442\n1990-1994    0.408656\n1995-1999    0.373039\n2000-2004    0.417602\n2005-2009    0.434809\n2010-2014    0.393544\n2015-2019    0.409779\nName: avg_brightness, dtype: float64\n\n\nHow would you characterize the pattern here? Is there a meaningful pattern? If so, do you have any hypotheses about what might be behind them?\nLet’s do a similar analysis using the genres associated with each film. To do this, we first use the function pd.merge to combine our posters data with the genres table.\n\ndf = pd.merge(posters, genre, on=['year', 'title'])\ndf\n\n\n\n\n\n\n\n\nyear\ntitle\npath\nperiod\navg_brightness\ngenre\n\n\n\n\n0\n1999\nMy Favorite Martian\nthm/1999_my_favorite_martian.jpg\n1995-1999\n0.959253\nComedy\n\n\n1\n1999\nMy Favorite Martian\nthm/1999_my_favorite_martian.jpg\n1995-1999\n0.959253\nFamily\n\n\n2\n1999\nMy Favorite Martian\nthm/1999_my_favorite_martian.jpg\n1995-1999\n0.959253\nSci-Fi\n\n\n3\n2015\nSteve Jobs\nthm/2015_steve_jobs.jpg\n2015-2019\n0.957483\nBiography\n\n\n4\n2015\nSteve Jobs\nthm/2015_steve_jobs.jpg\n2015-2019\n0.957483\nDrama\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n11075\n1992\nConsenting Adults\nthm/1992_consenting_adults.jpg\n1990-1994\n0.034447\nMystery\n\n\n11076\n2006\nSaw III\nthm/2006_saw_iii.jpg\n2005-2009\n0.033579\nHorror\n\n\n11077\n2006\nSaw III\nthm/2006_saw_iii.jpg\n2005-2009\n0.033579\nMystery\n\n\n11078\n2006\nSaw III\nthm/2006_saw_iii.jpg\n2005-2009\n0.033579\nThriller\n\n\n11079\n1983\nZelig\nthm/1983_zelig.jpg\n1980-1984\n0.030483\nComedy\n\n\n\n\n11080 rows × 6 columns\n\n\n\nThen, we can repeate the analysis from the period data by grouping on genre and then taking the average brightness of each genre. Whereas it made sense to arrange the periods in chronological order to see a pattern, here it will be better to have Python arrange the genres by their mean average brightness. We do this with the sort_values method on the summarized data.\n\ndf.groupby(['genre'])['avg_brightness'].mean().sort_values()\n\ngenre\nHorror       0.293290\nMystery      0.309600\nThriller     0.333107\nSci-Fi       0.335817\nAction       0.368364\nFantasy      0.396500\nCrime        0.406723\nAdventure    0.411667\nBiography    0.433424\nDrama        0.437968\nAnimation    0.468851\nFamily       0.489931\nRomance      0.521365\nComedy       0.521993\nName: avg_brightness, dtype: float64\n\n\nWhat patterns do you notice in the genre codes here? Which seem to have the darkest posters and which seem to have the brightness ones? Can you summarize this pattern in any way? Any hypotheses about what is going on here?",
    "crumbs": [
      "Tutorials",
      "1.1 Tutorial I: Digital Images"
    ]
  },
  {
    "objectID": "tutorial-digital-images.html#saturation-and-chroma",
    "href": "tutorial-digital-images.html#saturation-and-chroma",
    "title": "1.1 Tutorial I: Digital Images",
    "section": "Saturation and Chroma",
    "text": "Saturation and Chroma\nWe have already seen that if we do a careful analysis, we can do quite a bit of work with a relatively straightforward annotation based on image brightness. Our goal though is to understand the use of color more broadly in movie posters, which will require creating additional annotations that capture other aspects of poster color. One way to do this is to first convert the raw pixel intensities into an alternative color space.\nThe RGB representation of pixels by the amount of red, green, and blue light needed to create the color at a specific point in space comes from the low-level engineering needs to image capture and display. As we have already seen, it is not a particularly meaningful way to think about our perception of color. Fortunately, there are other ways to represent color that more closely align with human perception and understanding. In order to understand how this works, let’s re-load the poster of the John Wayne movie The Legend.\n\nimg = dvt.load_image(\"data/\" + posters.path[10])\nplt.imshow(img)\n\n\n\n\n\n\n\n\nRecall that above we located a specific pixel that corresponds to the burnt orange color in the poster. It has a RGB representation of the following:\n\nimg[180, 25, :]\n\narray([189, 106,   0], dtype=uint8)\n\n\nWe can convert the RGB format into an HSV format using the cvt2.cvtColor function by specifying the type of color space transformation (COLOR_RGB2HSV) to use as a second argument. We will do some conversion of the scales of the output to convert them into a scale from 0 to 1, which will better match other sources as well as the longer discussion of this case study in Chapter 3 of Distant Viewing.\n\nimg_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\nimg_hsv = img_hsv.astype(np.float64)\nimg_hsv[:, :, 0] = img_hsv[:, :, 0] / 179.0\nimg_hsv[:, :, 1] = img_hsv[:, :, 1] / 255.0\nimg_hsv[:, :, 2] = img_hsv[:, :, 2] / 255.0\nimg_hsv.shape\n\n(229, 150, 3)\n\n\nNotice that the shape of the output is exactly the same as the original image. The rows and columns still correspond to the same locations as the RGB model; it is only the triple of numbers at that location that have changed. Let’s see what our burnt orange pixel looks like now:\n\nimg_hsv[180, 25, :]\n\narray([0.09497207, 1.        , 0.74117647])\n\n\nThe first component is around 0.095. This correspond to the hue of the pixel, with this number corresponding to the color orange. Hue is a bit complex and we will further investigate how it works in the next section. The second number is the saturation, which represents how rich the color is. A light pastel, such as a pale pink, will have a low saturation. Here, we see that the saturation is the maximum value of 1. Finally, the value is an alternative representation of brightness, which here we see is equal to 0.74.\nLet’s now focus on the saturation of the posters and do a similar analysis to the ones above with brightness. To more closely follow the analysis in the book, we will compute the related quantity called chroma instead of working with saturation directly. This can be computed by multiplying the saturation by the value. We used this quantity because it more closely aligns with the idea of the richness of color that we wanted to capture. For example, we see that the burnt orange color in our poster for The Legend has a saturation of 1, but it’s chroma is only 0.74 (saturation times value). Only a “pure” orange color, like what you would see on a color wheel, would have a chroma of 1.\nNow, let’s cycle through the posters and add the average chroma value to each of them just as we did with the image brightness.\n\nposters['avg_chroma'] = 0.0\nfor ind in posters.index:\n  img = dvt.load_image(\"data/\" + posters.loc[ind, 'path'])\n  img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n  img_hsv = img_hsv.astype(np.float64)\n  img_hsv[:, :, 0] = img_hsv[:, :, 0] / 179.0\n  img_hsv[:, :, 1] = img_hsv[:, :, 1] / 255.0\n  img_hsv[:, :, 2] = img_hsv[:, :, 2] / 255.0\n  posters.loc[ind, 'avg_chroma'] = np.mean(img_hsv[:, :, 1] * img_hsv[:, :, 2])\n\nAnd again we will arrange the posters data from the most to the least highest levels of chroma.\n\nposters = posters.sort_values('avg_chroma', ascending=False)\nposters\n\n\n\n\n\n\n\n\nyear\ntitle\npath\nperiod\navg_brightness\navg_chroma\n\n\n\n\n3987\n2006\nThe Omen\nthm/2006_the_omen.jpg\n2005-2009\n0.329209\n0.956298\n\n\n4061\n2007\nDaddy's Little Girls\nthm/2007_daddys_little_girls.jpg\n2005-2009\n0.558258\n0.931692\n\n\n409\n1980\nThe Shining\nthm/1980_the_shining.jpg\n1980-1984\n0.550869\n0.797270\n\n\n4556\n2017\nHow to Be a Latin Lover\nthm/2017_how_to_be_a_latin_lover.jpg\n2015-2019\n0.480406\n0.768021\n\n\n3981\n2006\nCurious George\nthm/2006_curious_george.jpg\n2005-2009\n0.534758\n0.759104\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2502\n1973\nPsyched by the 4D Witch (A Tale of Demonology)\nthm/1973_psyched_by_the_d_witch_a_tale_of_demo...\n1970-1974\n0.760277\n0.000000\n\n\n2637\n1978\nThe Adventures of the Jensen Boys\nthm/1978_the_adventures_of_the_jensen_boys.jpg\n1975-1979\n0.282204\n0.000000\n\n\n2473\n1972\nPeed Into the Wind\nthm/1972_peed_into_the_wind.jpg\n1970-1974\n0.594636\n0.000000\n\n\n131\n1973\nNever Look Back\nthm/1973_never_look_back.jpg\n1970-1974\n0.661440\n0.000000\n\n\n2485\n1972\nThe Only Way Home\nthm/1972_the_only_way_home.jpg\n1970-1974\n0.694055\n0.000000\n\n\n\n\n4680 rows × 6 columns\n\n\n\nNow, let’s once again look at some of the posters that correspond to extreme values. It is always an important step when working with new annotations to go back to the original images, and actually look at them to see how the numeric representation of the image corresponds to our own viewing and interpretation. As before, please feel free to experiment by looking at other posters with particularly high values.\n\nimg = dvt.load_image(\"data/\" + posters.iloc[0].path)\nplt.imshow(img)\n\n\n\n\n\n\n\n\nWe can also do this with posters have the lowest average chroma. Many posters have an average chroma equal to zero. Can you think of what feature these all share in common before looking at the examples?\n\nimg = dvt.load_image(\"data/\" + posters.iloc[posters.shape[0] - 1].path)\nplt.imshow(img)\n\n\n\n\n\n\n\n\nLet’s see how the average chroma corresponds to the genres associated with each of the movie posters. Because there is such a shift in brightness from first twenty years of the data (a lot was in black and white), we will filter the data after merging in the genres to only include years from 1990 onwards.\n\ndf = pd.merge(posters, genre, on=['year', 'title'])\ndf = df[df[\"year\"] &gt;= 1990]\ndf.groupby(['genre'])['avg_chroma'].mean().sort_values()\n\ngenre\nMystery      0.115066\nHorror       0.117193\nThriller     0.134544\nBiography    0.136763\nSci-Fi       0.152587\nCrime        0.153525\nDrama        0.154572\nAction       0.164537\nRomance      0.168240\nFantasy      0.170068\nComedy       0.199362\nFamily       0.199996\nAdventure    0.204303\nAnimation    0.275066\nName: avg_chroma, dtype: float64\n\n\nTake some time to look at the results. What patterns do you notice here? Does anything seem either (i) particularly surprising or (ii) particularly unsurprising? Both of these are useful observations for understanding the connection between the messages conveyed through the poster’s color and the associated genres.",
    "crumbs": [
      "Tutorials",
      "1.1 Tutorial I: Digital Images"
    ]
  },
  {
    "objectID": "tutorial-digital-images.html#dominant-color",
    "href": "tutorial-digital-images.html#dominant-color",
    "title": "1.1 Tutorial I: Digital Images",
    "section": "Dominant Color",
    "text": "Dominant Color\nHaving looked at the brightness/value and saturation/chroma of the posters, we now look to the third element of color, know as hue. To get started, let’s take a different example poster. Here, we will load the poster from the movie Take the Lead (2006), which has several different hues of color in it.\n\nimg = dvt.load_image(\"data/\" + posters.path[4016])\nplt.imshow(img)\n\n\n\n\n\n\n\n\nNow, let’s compute the HSV coordinates of this image just as we did in the previous section. Additionally, we will reshape the pixel data so that the array has one row for each pixel and only three columns. This is just some re-arranging to make the rest of the code easier to write and understand.\n\nimg_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\nimg_hsv = img_hsv.astype(np.float64)\nimg_hsv[:, :, 0] = img_hsv[:, :, 0] / 179.0\nimg_hsv[:, :, 1] = img_hsv[:, :, 1] / 255.0\nimg_hsv[:, :, 2] = img_hsv[:, :, 2] / 255.0\nimg_hsv = img_hsv.reshape((-1, 3), order = \"F\")\nimg_hsv.shape\n\n(33300, 3)\n\n\nThe hue is a number between 0 and 1 that indicate what we would colloqually call “color”. Unlike brightness, saturation, chroma, and value, these numbers are best thought of being arranged in a circle (see the associated slides for a visualization). A value of 0 corresponds with red, .33 with green, .5 with cyan, and .66 with blue. Values close to 1 wrap back through purple and link back into red. So, the hues 0.01 and 0.99 are actually quite similar.\nTo understand a bit better, let’s look at a histogram showing the distribution of the hues in the image. We need to be careful, though, because our interpretation of hue is only applicable when the chroma is sufficently large. If the chroma value is small, there is little color to show anyway and the differences between hues may be difficult or impossible to differentate. In the code below, we should the distribution of hues with a chroma above 0.3.\n\nplt.hist(img_hsv[img_hsv[:,1] * img_hsv[:,2] &gt; 0.3, 0], bins=100)\nplt.show()\n\n\n\n\n\n\n\n\nWe should see a lot of values near 0.3; these correspond to the green in the poster, which takes up a lot of space in the image. The peak near 0.66 is associated with the blue in the image, mainly on the silhouettes of the two characters on the poster. The smaller amount near one, and also wrapping around at 1, is the orange/red color in the title of the movie.\nTaking averages of hues does not in general provide meaningful summaries. For an extreme example, if we take the average of two shades of red that have hues of 0.99 and 0.01, this would yield a hue of 0.5, cyan, a color directly between green and blue. Alternatively, we can create an annotation of hue by breaking the range of hues up into standard color names and then count the proportion of each poster that corresponds with each color. To do this, we will load another dataset that we created with common cut-off values for each of the hues.\n\nhue = pd.read_csv(\"data/movies_50_years_hue.csv\")\nhue\n\n\n\n\n\n\n\n\ncnom\nstart\nend\nmid\n\n\n\n\n0\nred\n0.000000\n0.015625\n0.007812\n\n\n1\norange\n0.015625\n0.109375\n0.062500\n\n\n2\nyellow\n0.109375\n0.203125\n0.156250\n\n\n3\ngreen\n0.203125\n0.453125\n0.328125\n\n\n4\ncyan\n0.453125\n0.546875\n0.500000\n\n\n5\nblue\n0.546875\n0.765625\n0.656250\n\n\n6\nviolet\n0.765625\n0.953125\n0.859375\n\n\n7\nred\n0.953125\n1.000000\n0.976562\n\n\n\n\n\n\n\nNext, we take the hues in img_hsv and count the numbes of pixels that are in each of these buckets after filtering for a sufficently high chroma to have a meaningful hue.\n\nbins = np.append(0, hue.end.values)\ncnt, _ = np.histogram(img_hsv[(img_hsv[:,1] * img_hsv[:,2] &gt; 0.3), 0], bins = bins)\ncnt[0] = cnt[0] + cnt[7]\ncnt = cnt[:7]\ncnt\n\narray([  443,   764,   443, 14040,   692,  4974,    71])\n\n\nThere are a number of different ways to summarize these counts. We will create two annotations from them. First, we associate each poster with a dominant color corresponding to the hue name that is most strongly represented in the poster. This can be done with the following code. As we see, the code associates the Take the Lead poster with the color green as would be have expected from the histogram.\n\nhue.cnom.values[np.argmax(cnt)]\n\n'green'\n\n\nThe other helpful quantity that we will save is the proportion of the entire poster that corresponds to this most dominant color. Using the code below, we see that that over 42% of this poster corresponds to hues which we have categorized as “green”.\n\ncolor_percent = np.max(cnt) / img_hsv.shape[0] * 100\ncolor_percent\n\nnp.float64(42.16216216216216)\n\n\nNow that we have seen how to do this with a single image, let’s cycle through all of the posters and compute the name of the dominant color and the percentage of the poster corresponding to each color for each of the posters.\n\nposters['dom_color'] = ''\nposters['dom_color_percent'] = 0.0\nfor ind in posters.index:\n  img = dvt.load_image(\"data/\" + posters.loc[ind, 'path'])\n  img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n  img_hsv = img_hsv.astype(np.float64)\n  img_hsv[:, :, 0] = img_hsv[:, :, 0] / 179.0\n  img_hsv[:, :, 1] = img_hsv[:, :, 1] / 255.0\n  img_hsv[:, :, 2] = img_hsv[:, :, 2] / 255.0\n  img_hsv = img_hsv.reshape((-1, 3), order = \"F\")\n  bins = np.append(0, hue.end.values)\n  cnt, _ = np.histogram(img_hsv[(img_hsv[:,1] * img_hsv[:,2] &gt; 0.3), 0], bins = bins)\n  cnt[0] = cnt[0] + cnt[7]\n  cnt = cnt[:7]\n  posters.loc[ind, 'dom_color'] = hue.cnom.values[np.argmax(cnt)]\n  posters.loc[ind, 'dom_color_percent'] = color_percent = np.max(cnt) / img_hsv.shape[0] * 100\n\nAs with the other two annotations, we can start by looking at the the posters the have the most amount of dominant color for each given hue. Here, for example is the code to show the image with the largest amount of blue. Try to change the code to see other colors such as “red”, “yellow”, and “green”.\n\nimg = dvt.load_image(\"data/\" + posters[posters.dom_color == \"blue\"].sort_values(\n    'dom_color', ascending=False\n).iloc[0].path)\nplt.imshow(img)\n\n\n\n\n\n\n\n\nNow, let’s do some analysis with these annotations. We will join the genre data back into the annotations and, as before, filter to only films from 1990 onwards. We will also only consider posters that have at least 5% of whatever the selected dominant color is.\n\ndf = pd.merge(posters, genre, on=['year', 'title'])\ndf = df[df[\"year\"] &gt;= 1990]\ndf = df[df['dom_color_percent'] &gt; 5]\ndf\n\n\n\n\n\n\n\n\nyear\ntitle\npath\nperiod\navg_brightness\navg_chroma\ndom_color\ndom_color_percent\ngenre\n\n\n\n\n0\n2006\nThe Omen\nthm/2006_the_omen.jpg\n2005-2009\n0.329209\n0.956298\nred\n98.228228\nHorror\n\n\n1\n2006\nThe Omen\nthm/2006_the_omen.jpg\n2005-2009\n0.329209\n0.956298\nred\n98.228228\nThriller\n\n\n2\n2007\nDaddy's Little Girls\nthm/2007_daddys_little_girls.jpg\n2005-2009\n0.558258\n0.931692\nyellow\n93.696697\nDrama\n\n\n3\n2007\nDaddy's Little Girls\nthm/2007_daddys_little_girls.jpg\n2005-2009\n0.558258\n0.931692\nyellow\n93.696697\nRomance\n\n\n6\n2017\nHow to Be a Latin Lover\nthm/2017_how_to_be_a_latin_lover.jpg\n2015-2019\n0.480406\n0.768021\nyellow\n63.965368\nComedy\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9928\n2008\nValkyrie\nthm/2008_valkyrie.jpg\n2005-2009\n0.627815\n0.045591\nred\n6.334752\nDrama\n\n\n9929\n2008\nValkyrie\nthm/2008_valkyrie.jpg\n2005-2009\n0.627815\n0.045591\nred\n6.334752\nThriller\n\n\n10068\n2002\nCatch Me If You Can\nthm/2002_catch_me_if_you_can.jpg\n2000-2004\n0.763134\n0.041342\nblue\n5.009009\nBiography\n\n\n10069\n2002\nCatch Me If You Can\nthm/2002_catch_me_if_you_can.jpg\n2000-2004\n0.763134\n0.041342\nblue\n5.009009\nCrime\n\n\n10070\n2002\nCatch Me If You Can\nthm/2002_catch_me_if_you_can.jpg\n2000-2004\n0.763134\n0.041342\nblue\n5.009009\nDrama\n\n\n\n\n5086 rows × 9 columns\n\n\n\nNow, we can compute the proportion of posters from each genre that have red as a dominant color using the code below.\n\ndf['percent_color'] = (df['dom_color'] == \"red\")\ndf.groupby(['genre'])['percent_color'].mean().sort_values()\n\ngenre\nAnimation    0.061905\nAdventure    0.076106\nBiography    0.076923\nRomance      0.100543\nFantasy      0.101604\nComedy       0.117041\nSci-Fi       0.134615\nDrama        0.134901\nFamily       0.145228\nAction       0.169643\nMystery      0.180328\nCrime        0.190202\nThriller     0.223529\nHorror       0.264463\nName: percent_color, dtype: float64\n\n\nDo you see any interesting patterns in the data above? After looking closely, try to change the color of interest and see if any other patterns arise.",
    "crumbs": [
      "Tutorials",
      "1.1 Tutorial I: Digital Images"
    ]
  },
  {
    "objectID": "tutorial-digital-images.html#face-detection",
    "href": "tutorial-digital-images.html#face-detection",
    "title": "1.1 Tutorial I: Digital Images",
    "section": "Face Detection",
    "text": "Face Detection\nWhen introducing the ideas behind distant viewing and the computational analysis of large collections of digital images, we like to start with annotations that capture color. There’s a tangible connection between elements such as brightness, saturation, and hue that we can instantly connect to the way that digital images are created and stored. At the same time, color is not at all simple. In many ways, it is a particularly difficult annotation to work with because there are so many different ways to count, bucket, and summarize the results. Understanding the connection between the raw pixel intensites, the resulting annoations, and the semantic gap between the two is key to understanding the challenges and possibilites of working with collections of digital images, and distant viewing.\nOf course, many applications of distant viewing will want to integrate other kinds of annotations, many of which require using more advanced machine learning techniques such as neural networks and large language models. In this final section of analysis, we will show how to use a face detection model to add an additional annotation to our movie posters dataset using the distant viewing toolkit (dvt).\nThe distant viewing toolkit simplifes the process of applying several common computer visional algorithms to still and moving images. Most of the difficult work of standardizing the inputs, downloading models, and getting all of the results into the same format is taken care of in dvt. All that we need to do is load the annotator that we are interested in, load the images of interest, and create the annotations by calling a specific method from each annotator. To start, we will use the code below to load and save a AnnoFaces object for detecting faces in images. The first time this function is called, Python will download the underlying model that does this processing.\nWhy might we be interested in detecting faces in movie posters? Which questions could we ask?\n\nanno_face = dvt.AnnoFaces()\n\n/Users/admin/gh/dvscripts/env/lib/python3.12/site-packages/dvt/face.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.mtcnn.load_state_dict(torch.load(model_path_mtcnn))\n/Users/admin/gh/dvscripts/env/lib/python3.12/site-packages/dvt/face.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.vggface.load_state_dict(torch.load(model_path_vggface))\n\n\nIn order to get good results from face detection, unlike the color analysis, we need to work with the full resolution version of the movie posters. We downloaded a full-size image for this purpose from the movie Love Story, the top-grossing film from 1970. Let’s read this image into Python using the function load_image. We can display this in the Colab notebook, noting that it is much larger and sharper than the thumbnails we were using previously.\n\nimg = dvt.load_image('data/love_story.jpg')\nplt.imshow(img)\n\n\n\n\n\n\n\n\nIn order to run the face detection annotator on the image, we use the run method of the annotator and provide the image that we have loaded as an argument to the function.\n\nout_face = anno_face.run(img, visualize=True)\n\nThe output of the annotation contains two objects. The first is the original image with boxes around the detected faces. This is useful for the qualatitive analysis and assessment of how well the algorithm works on our data. We see in this example that the algorithm has found the two faces present in the poster.\n\nplt.imshow(out_face['img'])\n\n\n\n\n\n\n\n\nA structured data version of the detected faces is provided in the object named boxes. It can be turned into a data frame with the pd.DataFrame function. Here, we see that it tells us the pixel coordinates (from the upper-left hand corner) of the detected faces along with a probability score, which tells us how confident the prediction is that there is actually a face in the given location.\n\npd.DataFrame(out_face['boxes'])\n\n\n\n\n\n\n\n\nface_id\nx\nxend\ny\nyend\nprob\n\n\n\n\n0\n0\n95\n211\n55\n209\n0.999969\n\n\n1\n1\n163\n259\n162\n287\n0.999953\n\n\n\n\n\n\n\nSo, as with the color annotations, the next step is to run this annotation over all of the posters and collect the results. The model for face detection is quite a bit slower than the color annotators and requires the larger version of the movie poster data. If we were to have the full images instead of the thumbnails the following code (with the hash signs removed from the start of the lines, which we added here to ensure that we do not accidentally run the code) would create the desired dataset.\n\n#output = []\n#for idx, ip in enumerate(posters.index):\n#  path = posters.loc[ind, 'path']\n#  img = dvt.load_image(\"data/\" + path)\n#  out_face = anno_face.run(img, visualize=False)\n#  if 'boxes' in out_face:\n#    df = pd.DataFrame(out_face['boxes'])\n#    df['path'] = path\n#    output.append(df)\n#\n#faces = pd.concat(output)\n#faces = pd.merge(faces, meta)\n#faces = faces[['year', 'title', 'face_id', 'x', 'xend', 'y', 'yend', 'prob']]\n#faces.to_csv(\"data/movies_50_years_face.csv\", index=False)\n\nGiven the time and data constraints, we have provided a version of the annotated faces that we can read directly into Python as a CSV file. Note that it is very similar to output directly from the AnnoFaces object, just with the year and title of the movie added as the first two columns.\n\nfaces = pd.read_csv(\"data/movies_50_years_face.csv\")\nfaces\n\n\n\n\n\n\n\n\nyear\ntitle\nface_id\nx\nxend\ny\nyend\nprob\n\n\n\n\n0\n1970\nLove Story\n0\n95\n211\n55\n209\n0.999969\n\n\n1\n1970\nLove Story\n1\n163\n259\n162\n287\n0.999953\n\n\n2\n1970\nAirport\n0\n423\n458\n521\n566\n0.999998\n\n\n3\n1970\nAirport\n1\n420\n460\n311\n363\n0.999976\n\n\n4\n1970\nAirport\n2\n423\n458\n249\n295\n0.999964\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13139\n2019\nThe Kid\n2\n932\n1072\n1539\n1721\n0.980665\n\n\n13140\n2019\nThe Kid\n3\n936\n1083\n476\n687\n0.960602\n\n\n13141\n2019\nThe Kid\n4\n271\n421\n1519\n1725\n0.922275\n\n\n13142\n2019\nThe Kid\n5\n514\n528\n760\n777\n0.888940\n\n\n13143\n2019\nThe Kid\n6\n615\n664\n527\n589\n0.731668\n\n\n\n\n13144 rows × 8 columns\n\n\n\nThe annotations above provide one row for each detected face in one of the movie posters. In order to analyze these annotations, we need to aggregate them so that we have a single summary value for each poster. One way to do that is to filter the faces to only those above a threshold probability score and then count how many faces are present in each poster. Then, we can add this count back into the posters table. We need to be careful about doing this so as to make sure that we are not missing the posters that have zero faces. The sequence of pandas functions below puts all of these steps together, as well as joining to the genre table, which we will use shortly. We usually pick a pretty high threshold (.95 in this case) given our experience with face detection algorithms and knowing this data.\n\nface_cnt = faces[faces['prob'] &gt; 0.95][['year', 'title']].value_counts().reset_index(name='num_face')\nface_cnt = pd.merge(posters, face_cnt, how='left')\nface_cnt = pd.merge(face_cnt, genre)\nface_cnt['num_face'] = face_cnt['num_face'].fillna(0)\nface_cnt = face_cnt.sort_values('num_face', ascending=False)\nface_cnt\n\n\n\n\n\n\n\n\nyear\ntitle\npath\nperiod\navg_brightness\navg_chroma\ndom_color\ndom_color_percent\nnum_face\ngenre\n\n\n\n\n4359\n1999\nBeing John Malkovich\nthm/1999_being_john_malkovich.jpg\n1995-1999\n0.390828\n0.167236\norange\n7.649452\n107.0\nDrama\n\n\n4358\n1999\nBeing John Malkovich\nthm/1999_being_john_malkovich.jpg\n1995-1999\n0.390828\n0.167236\norange\n7.649452\n107.0\nComedy\n\n\n4360\n1999\nBeing John Malkovich\nthm/1999_being_john_malkovich.jpg\n1995-1999\n0.390828\n0.167236\norange\n7.649452\n107.0\nFantasy\n\n\n2488\n2009\n500 Days of Summer\nthm/2009__days_of_summer.jpg\n2005-2009\n0.687005\n0.231621\nblue\n17.269841\n101.0\nRomance\n\n\n2486\n2009\n500 Days of Summer\nthm/2009__days_of_summer.jpg\n2005-2009\n0.687005\n0.231621\nblue\n17.269841\n101.0\nComedy\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3196\n2016\nThe Secret Life of Pets\nthm/2016_the_secret_life_of_pets.jpg\n2015-2019\n0.518787\n0.203276\nblue\n14.910364\n0.0\nComedy\n\n\n3195\n2016\nThe Secret Life of Pets\nthm/2016_the_secret_life_of_pets.jpg\n2015-2019\n0.518787\n0.203276\nblue\n14.910364\n0.0\nAdventure\n\n\n3194\n2016\nThe Secret Life of Pets\nthm/2016_the_secret_life_of_pets.jpg\n2015-2019\n0.518787\n0.203276\nblue\n14.910364\n0.0\nAnimation\n\n\n3190\n2004\nThe Alamo\nthm/2004_the_alamo.jpg\n2000-2004\n0.315891\n0.203356\norange\n23.945946\n0.0\nDrama\n\n\n0\n2006\nThe Omen\nthm/2006_the_omen.jpg\n2005-2009\n0.329209\n0.956298\nred\n98.228228\n0.0\nHorror\n\n\n\n\n11080 rows × 10 columns\n\n\n\nAs we have done with the color annotations, it is a good idea to look at some of the results. The table above shows that the poster for the 1999 film Being John Malkovich has a total of 107 faces. This may seem like an error, but let’s look at the movie poster to check.\n\nimg = dvt.load_image(\"data/\" + face_cnt.iloc[0].path)\nplt.imshow(img)\n\n\n\n\n\n\n\n\nSo, in fact, it does seem like having a hundred detected faces is reasonable after looking at the poster. It’s hard to confirm that the exact number is correct (it’s almost certainly not), but at least we feel good that the algorithm is doing something reasonable. Feel free to try some other poster values and see if they also have a large number of faces. In a full analysis, it would be a good idea to go through and check how many of the detected faces were correctly located, but for now we will take these qualitative results as a good starting point and move on to a final analysis.\nLet’s start by seeing if there are any temporal patterns in the average number of faces present in each of the posters.\n\nposters.groupby(['period'])['avg_brightness'].mean()\n\nperiod\n1970-1974    0.575523\n1975-1979    0.525609\n1980-1984    0.471467\n1985-1989    0.443442\n1990-1994    0.408656\n1995-1999    0.373039\n2000-2004    0.417602\n2005-2009    0.434809\n2010-2014    0.393544\n2015-2019    0.409779\nName: avg_brightness, dtype: float64\n\n\nWhat pattern(s) arise in the number of faces on the posters? Can you think of any reason that there might be more faces on average during the 1970s compared to more recent decades?\nAs with the color-based annotations, we can take the mean value of these counts by genre and see if there are any interesting patterns.\n\nface_cnt.groupby(['genre'])['num_face'].mean().sort_values()\n\ngenre\nHorror       1.037895\nAnimation    1.044776\nMystery      1.467192\nSci-Fi       1.562005\nBiography    1.598086\nThriller     1.617143\nAdventure    2.064516\nCrime        2.126492\nAction       2.141925\nDrama        2.202097\nFamily       2.357868\nFantasy      2.380597\nRomance      2.710419\nComedy       2.951996\nName: num_face, dtype: float64\n\n\nTake a few minutes to look at the table. Do you see any consistent patterns? Can you think of any hypotheses about what might be causing these? Can you think of any cautions that might provide any caveats to the analysis of the face detection algorithm?\nAs we saw above, there are several posters that have a very large number of faces. It might be enlightening to look at the median number of faces by genre in addition to the average number.\n\nface_cnt.groupby(['genre'])['num_face'].median().sort_values()\n\ngenre\nAnimation    0.0\nHorror       0.0\nAction       1.0\nAdventure    1.0\nBiography    1.0\nDrama        1.0\nFantasy      1.0\nMystery      1.0\nSci-Fi       1.0\nThriller     1.0\nComedy       2.0\nCrime        2.0\nFamily       2.0\nRomance      2.0\nName: num_face, dtype: float64\n\n\nHow do the median values compare to the average values? Do they help tell a story about faces that are present in the poster?\nAs a final analysis, to show the many different ways that we can analyze the same annotations, we will compute the proportion of posters that have at least one face on them and summarize by genre.\n\nface_cnt['face_present'] = (face_cnt['num_face'] &gt; 0)\nface_cnt.groupby(['genre'])['face_present'].mean().sort_values()\n\ngenre\nAnimation    0.406716\nHorror       0.418947\nSci-Fi       0.567282\nMystery      0.574803\nAdventure    0.639113\nThriller     0.664286\nBiography    0.669856\nFamily       0.690355\nFantasy      0.703980\nAction       0.738989\nDrama        0.746425\nCrime        0.821002\nComedy       0.831733\nRomance      0.875507\nName: face_present, dtype: float64\n\n\nDo these results help add any nuance to the previous results? What kinds of questions could we pose by putting them together?\nIn the book, we decided to focus on one type of annotation in each chapter. Chapter 3 analyzed color and movie posters, while Chapter 5 on Bewitched and I Dream of Jeannie relied on face detection to look at charachters on screen. Here, we expand on the book to begin to demonstrate how layering types of annotations begins to add new areas of exploration and to nuance our computaitonal analysis of images.",
    "crumbs": [
      "Tutorials",
      "1.1 Tutorial I: Digital Images"
    ]
  },
  {
    "objectID": "tutorial-digital-images.html#conclusions-and-next-steps",
    "href": "tutorial-digital-images.html#conclusions-and-next-steps",
    "title": "1.1 Tutorial I: Digital Images",
    "section": "Conclusions and Next Steps",
    "text": "Conclusions and Next Steps\nThe tutorial has covered a lot of material. We’ve introduced the basics of running Python and an understanding of how digital images are represented as arrays of pixel intensites. From there, we motived the theoretical stakes of distant viewing. Then, we put this into practice by generating increasingly complex annotations based on our understanding of color and, finally, by using a neural-network based machine learning algorithm to detected faces present in each of the movie posters. Throughout, we have tried to connect each of our analyess back to research questions regarding the visual cultures around movie posters across different genres over a 50-year period.\nThere are many directions to explore after following this notebook. If you want to see how to complete the analysis presented here, including the important steps of connecting our observations to existing archival and scholarly sources, we suggest checking out Chapter 3 of our Distant Viewing book. A link is available at the top of this notebook. If you are new to programming, learning a bit more Python from core principles is a good start. If your interests including moving images, we have a follow-up notebook that works through an example using a set of U.S. television shows from the 1960s and 1970s. Otherwise, we suggest checking out the other annotations available in the distant viewing toolkit and applying them to your own collections.",
    "crumbs": [
      "Tutorials",
      "1.1 Tutorial I: Digital Images"
    ]
  },
  {
    "objectID": "tutorial-moving-images.html",
    "href": "tutorial-moving-images.html",
    "title": "1.2 Tutorial II: Moving Images",
    "section": "",
    "text": "Setup\nThis notebook explores the theory and methods introduced in the book Distant Viewing (MIT Press, 2023) to study visual style in two network era sitcoms. Specifically, we will look at every televised episode of the series Bewitched (1964-1972) and I Dream of Jeannie (1965-1970). In the notebook we will first walk through the methodology using a short 45 second sample video and going slowly through all of the steps. Then, due to time, file size, and copyright constraints, we will load a precomputed set of image annotations mirroring those from the 45 second sample and then use this larger set for the purpose of analysis. Here are the specific learning outcomes for the tutorial:\nThis notebook does not require any previous knowledge of Python or computer vision. However, it moves fairly quickly through the preliminary steps of working with digital images and only explains the most important aspects of the Python code in each step. For a more in-depth introduction to how computers view images images and python, we recommend first following the Distant Viewing Tutorial: Movie Posters and Color Analysis notebook using the movie posters corpus, which can be accessed here. For more about the Python package that we built, Distant Viewing Toolkit (dvt), please visit our GitHub repository.\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport dvt\nimport cv2",
    "crumbs": [
      "Tutorials",
      "1.2 Tutorial II: Moving Images"
    ]
  },
  {
    "objectID": "tutorial-moving-images.html#network-era-sitcom-dataset",
    "href": "tutorial-moving-images.html#network-era-sitcom-dataset",
    "title": "1.2 Tutorial II: Moving Images",
    "section": "Network-Era Sitcom Dataset",
    "text": "Network-Era Sitcom Dataset\nBefore we start looking at the computational steps needed to work with moving image data, it’s helpful to understand the humanities research questions that motivate this work. Here, we are concerned with two popular U.S. sitcoms from the 1960s and early 1970s: Bewitched (1964-1972) and I Dream of Jeannie (1965-1970). Here is metadata about each of the episodes from the entire runs of the two shows:\n\nmeta = pd.read_csv(\"data/ttl_sitcom_metadata.csv\")\nmeta\n\n\n\n\n\n\n\n\nseries\nseason\nnumber\ntitle\ndirector\nwriter\nair_date\ndescription\n\n\n\n\n0\nBewitched\n1\n1\nI, Darrin, Take This Witch, Samantha\nWilliam Asher\nSol Saks\n1964-09-17\nIn the pilot episode, strangers Samantha (Eliz...\n\n\n1\nBewitched\n1\n2\nBe It Ever So Mortgaged\nWilliam Asher\nBarbara Avedon\n1964-09-24\nEndora is still surprised that Sam is willing ...\n\n\n2\nBewitched\n1\n3\nIt Shouldn't Happen to a Dog\nWilliam Asher\nJerry Davis\n4-10-01)[3\nSamantha is preparing dinner for Darrin's pote...\n\n\n3\nBewitched\n1\n4\nMother Meets What's-His-Name\nWilliam Asher\nDanny Arnold\n1964-10-08\nSamantha is visited by Gladys Kravitz, June Fo...\n\n\n4\nBewitched\n1\n5\nHelp, Help, Don't Save Me\nWilliam Asher\nDanny Arnold\n1964-10-15\nDarrin has been spending all hours working on ...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n388\nI Dream of Jeannie\n5\n22\nEternally Yours, Jeannie\nJoseph Goodson\nJames Henerson\n1970-03-17\nTony gets a letter from his old high school sw...\n\n\n389\nI Dream of Jeannie\n5\n23\nAn Astronaut in Sheep's Clothing\nBruce Kessler\nJames Henerson\n1970-03-24\nAfter Jeannie blinks Tony a drum major's unifo...\n\n\n390\nI Dream of Jeannie\n5\n24\nHurricane Jeannie\nClaudio Guzman\nJames Henerson\n1970-04-28\nA hurricane traps Tony, Jeannie, Roger and Dr....\n\n\n391\nI Dream of Jeannie\n5\n25\nOne Jeannie Beats Four of a Kind\nMichael Ansara\nPerry Grant, Richard Bensfield\n1970-05-19\nGeneral Schaeffer tells Major Healey of a card...\n\n\n392\nI Dream of Jeannie\n5\n26\nMy Master, the Chili King\nClaudio Guzman\nJames Henerson\n1970-05-26\nTony's cousin Arvel (Gabriel Dell) persuades J...\n\n\n\n\n393 rows × 8 columns\n\n\n\nThese two series are often compared and constrasted to one another, with I Dream of Jeannie being seen as an attempt by NBC to copy the success of ABC’s Bewitched, which started one season prior. While a lot has been written about the cultural significance of these two shows, prior research had not seriously considered the visual style of the two shows. We are interested in how the way that the shows are shot and edited contribute to our understanding of questions such as who is/are the main characters, to what extent does the visual style contribute to relationships between the characters, and how do these aspects change over time. The latter is particularly interesting because both shows started their broadcasts in black-and-white before transitioning to color for the majority of the later seasons.",
    "crumbs": [
      "Tutorials",
      "1.2 Tutorial II: Moving Images"
    ]
  },
  {
    "objectID": "tutorial-moving-images.html#sample-movie-file",
    "href": "tutorial-moving-images.html#sample-movie-file",
    "title": "1.2 Tutorial II: Moving Images",
    "section": "Sample Movie File",
    "text": "Sample Movie File\nNow that we have some ideas of the big questions we are interested in, let’s look at a short sample from one of the series to understand the computational steps needed to work with moving images. When doing computational analyses with visual and audiovisual materials, it is important to frequently go back and look at specific examples to ensure that we understand how our large-scale analysis connects back to the actual human experience of viewing. In this notebook we are going to start by working slowly with the process of creating annotations that summarize a short 45 second clip from the first episode of the third season of the show Bewitched. Running the code below shows an embedded version of the clip that can be watched within this page:\n\n\n\nWe suggest watching the clip a couple of times. Try to pay attention (and maybe even write down) the number of shots in the clip and how they are framed. In the next few sections, we will use computer vision algorithms to try to capture these features as structured annotations.",
    "crumbs": [
      "Tutorials",
      "1.2 Tutorial II: Moving Images"
    ]
  },
  {
    "objectID": "tutorial-moving-images.html#working-with-video-frames-as-images",
    "href": "tutorial-moving-images.html#working-with-video-frames-as-images",
    "title": "1.2 Tutorial II: Moving Images",
    "section": "Working with Video Frames as Images",
    "text": "Working with Video Frames as Images\nVideo file formats such as mp4, avi, and ogg typically store moving images in a complex format that is highly optimized to reduce file sizes and decompression time. When working with moving images computationally in programs such as Python, however, we typically decompress these files and turn them into a sequence of individual images stored as arrays of pixels, with one image for each frame in the input video. The distant viewing toolkit (dvt) contains several functions to efficently work with video files in Python. In this section, we will demonstrate how these work.\nFirst, we can use the video_info function to access metadata about a video file. Here, we will load the metadata for our sample video file, which displays the number of frames, the height and width of each frame in pixels, and rate of playback given as frames per second (fps).\n\ninfo = dvt.video_info('data/bewitched_sample.mp4')\ninfo\n\n{'frame_count': 1319.0,\n 'height': 480.0,\n 'width': 710.0,\n 'fps': 29.97002997002997}\n\n\nAs we see in the metadata, even this short clip has 1319 individual frames. When working with an entire television episode, let along a feature-length movie file, it quickly becomes difficult to load all of frames from a video into Python at once. The image sizes, particularly once we start looking at materials in high-definition, are just too large to hold these all in even a computer’s memory at the same time.\nThe distant viewing toolkit (dvt) offers an alternative approach using the yield_video function. It allows us to write a loop object which loads in each frame of the video file one by one. Specifically, each time the yield function is called, it returns the next frame as an array of pixels, an integer describing the frame number from the start of the video file, and a time code giving the number of seconds since the start of the video.\nThe code below show an example of how the yield_video function works. We cycle through each of the frames. For each frame we store the frame number, the timestamp, and the average pixel values (a measurement of the frame’s brightness). Then, we turn this into a tabular data frame with one row for each frame in the video.\n\noutput = {'frame': [], 'time': [], 'brightness': []}\nfor img, frame, msec in dvt.yield_video(\"data/bewitched_sample.mp4\"):\n  output['frame'].append(frame)\n  output['time'].append(msec)\n  output['brightness'].append(np.mean(img))\n\noutput = pd.DataFrame(output)\noutput\n\n\n\n\n\n\n\n\nframe\ntime\nbrightness\n\n\n\n\n0\n0\n0.000000\n87.132526\n\n\n1\n1\n0.033367\n87.221070\n\n\n2\n2\n0.066733\n87.496499\n\n\n3\n3\n0.100100\n87.620548\n\n\n4\n4\n0.133467\n87.445358\n\n\n...\n...\n...\n...\n\n\n1314\n1314\n43.843800\n68.011383\n\n\n1315\n1315\n43.877167\n68.054915\n\n\n1316\n1316\n43.910533\n68.043498\n\n\n1317\n1317\n43.943900\n68.028847\n\n\n1318\n1318\n43.977267\n67.969112\n\n\n\n\n1319 rows × 3 columns\n\n\n\nBrightness is one of the simpliest annotations that we can use to summarize an image. But, even this simple measurement can already provide a rough way of representing our video clip. To see this, let’s plot the brightness of the frame over time.\n\nplt.plot(output['time'], output['brightness'])\nplt.show()\n\n\n\n\n\n\n\n\nLooking at the brightness, particularly the large jumps in the values, can you identify the shot breaks that you found when watching the video? There is a steady increase in the brightness from about 15 seconds to 20 seconds. What does this correspond to in the video file?",
    "crumbs": [
      "Tutorials",
      "1.2 Tutorial II: Moving Images"
    ]
  },
  {
    "objectID": "tutorial-moving-images.html#shot-boundary-detection",
    "href": "tutorial-moving-images.html#shot-boundary-detection",
    "title": "1.2 Tutorial II: Moving Images",
    "section": "Shot Boundary Detection",
    "text": "Shot Boundary Detection\nOne of the first tasks that we often want to perform on a video file is to identify the breaks between shots. This process is called shot boundary detection. As we saw in the brightness plot above, some simple heuristics can be used to approximately identify many kinds of shot breaks. If we want more accurate predictions, which do not falsely identify quick motion as a shot boundary or fail to find subtle breaks such as cross-fades, we need to make use of a more complex algorithm. The distant viewing toolkit (dvt) includes a neural-network based shot boundary detection algorithm that we have found works well across many different genres and input types.\nTo run the built-in shot boundary detection algorithm, we need to first create a new annotator with the AnnoShotBreaks function. Then, we run the annotator over the video file. This is the only annotator in the toolkit that works directly with a video file rather than individual images or frames. The Python code will print out its progress through the file as it runs, returning a dictionary object with the predicted boundaries.\n\nanno_breaks = dvt.AnnoShotBreaks()\nout_breaks = anno_breaks.run(\"data/bewitched_sample.mp4\")\n\n/Users/admin/gh/dvscripts/env/lib/python3.12/site-packages/dvt/shots.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.model.load_state_dict(torch.load(model_path))\n\n\nProcessing video frames 50/1319Processing video frames 100/1319Processing video frames 150/1319Processing video frames 200/1319Processing video frames 250/1319Processing video frames 300/1319Processing video frames 350/1319Processing video frames 400/1319Processing video frames 450/1319Processing video frames 500/1319Processing video frames 550/1319Processing video frames 600/1319Processing video frames 650/1319Processing video frames 700/1319Processing video frames 750/1319Processing video frames 800/1319Processing video frames 850/1319Processing video frames 900/1319Processing video frames 950/1319Processing video frames 1000/1319Processing video frames 1050/1319Processing video frames 1100/1319Processing video frames 1150/1319Processing video frames 1200/1319Processing video frames 1250/1319Processing video frames 1300/1319Processing video frames 1319/1319\n\n\nWe will convert the output of the shot boundary detection into a tabular dataset as well as add the start time and end time information using the metadata that we grabbed from the video_info function above. Here is what the algorithm found for this video clip:\n\nshot = pd.DataFrame(out_breaks['scenes'])\nshot['mid'] = (shot['start'] + shot['end']) // 2\nshot['start_t'] = shot['start'] / info['fps']\nshot['end_t'] = shot['end'] / info['fps']\nshot\n\n\n\n\n\n\n\n\nstart\nend\nmid\nstart_t\nend_t\n\n\n\n\n0\n0\n98\n49\n0.000000\n3.269933\n\n\n1\n99\n184\n141\n3.303300\n6.139467\n\n\n2\n185\n427\n306\n6.172833\n14.247567\n\n\n3\n428\n1006\n717\n14.280933\n33.566867\n\n\n4\n1007\n1213\n1110\n33.600233\n40.473767\n\n\n5\n1214\n1318\n1266\n40.507133\n43.977267\n\n\n\n\n\n\n\nTake a few minutes to confirm that these shot boundaries correspond to the ones that you found when watching the video. You can even rewatch (and pause at each break) the video and see that it corresponds with the breaks that were automatically identified in the shot boundary detection.\nIn the next section we will look at face detection and identification. This is a common task when working with movie image data but ultimately works using individual images. To make this a bit easier to work with at first, we will use the following code to build a dataset with one frame from each of the detected shots in the video clip file. Specifically, we grab the middle frame from each cut, as specified in the shot data frame above.\n\nimg_list = []\nfor img, frame, msec in dvt.yield_video(\"data/bewitched_sample.mp4\"):\n  if frame in shot['mid'].values:\n    img_list.append(img)\n\nTo get a sense of what these look like, we can combine the images and convert them into thumbnails to get a view of what the six shots from our sample video file look like.\n\nimg_comb = np.hstack(img_list)\nimg_comb = cv2.resize(img_comb, (img_comb.shape[1] // 5, img_comb.shape[0] // 5))\nplt.imshow(img_comb)\n\n\n\n\n\n\n\n\nWe will use the full versions of these six images in the next section as we identify the location, size, and identity of the characters in each shot.",
    "crumbs": [
      "Tutorials",
      "1.2 Tutorial II: Moving Images"
    ]
  },
  {
    "objectID": "tutorial-moving-images.html#face-detection-and-facial-recognition",
    "href": "tutorial-moving-images.html#face-detection-and-facial-recognition",
    "title": "1.2 Tutorial II: Moving Images",
    "section": "Face Detection and Facial Recognition",
    "text": "Face Detection and Facial Recognition\nLocating and identifying the faces present in a particular frame of a video file is a common way of understanding the narrative structure and visual style of the source material. In this section, we will see how to work with faces using the functions within the distant viewing toolkit (dvt) applied to the six images from the sample video file that we extracted in the previous section. After seeing how to do this with a static set of images, in the following section we will see how to put this together in a way that would scale to larger video files.\nTo get started, we will load the face detection annotator AnnoFaces that is included in the distant viewing toolkit. The first time this is run, the function will automatically download the relevant model files and save them locally in our Colab workspace.\n\nanno_face = dvt.AnnoFaces()\n\n/Users/admin/gh/dvscripts/env/lib/python3.12/site-packages/dvt/face.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.mtcnn.load_state_dict(torch.load(model_path_mtcnn))\n/Users/admin/gh/dvscripts/env/lib/python3.12/site-packages/dvt/face.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.vggface.load_state_dict(torch.load(model_path_vggface))\n\n\nTo apply the annotation, we pass an image file to the run method of the annotator. Optionally, we can set the flag visualize to True in order to return a visualization of the detected face(s). Let’s run the annotation over the third image (remembering that Python starts counting at zero, so the third image is selected by selecting the image in position 2).\n\nout_face = anno_face.run(img_list[2], visualize=True)\n\nBecause we set the visualization flag to True, the returned results include an object called img that has a copy of the input frame along with a box around any detected faces. Here we see that the algorithm has detected one face, which is likely the number of faces a human annotator would also have found in the frame.\n\nplt.imshow(out_face['img'])\n\n\n\n\n\n\n\n\nThe visualization of the detected face is great for a one-off understanding of the algorithm. In order to store this information in a way that we can use for computational analyses, we need to have the location of the detected face represented in a strutured format. This is found in the boxes element of the output, which is designed to be convertable into a pandas data frame. Here is all of the structured information about the face detected in the frame above:\n\npd.DataFrame(out_face['boxes'])\n\n\n\n\n\n\n\n\nface_id\nx\nxend\ny\nyend\nprob\n\n\n\n\n0\n0\n279\n413\n119\n299\n0.999931\n\n\n\n\n\n\n\nIf you followed along with the introductory notebook using movie posters, this is the same information that we used to detect the number of faces present on each poster. One element that makes working with moving images different is that we usually have the same people showing up over and over again across frames and shots. Labeling the identity of the people present in each shot is an essential step in understanding the narrative structure of the material. The distant viewing toolkit (dvt) also includes information for doing facial recognition. This requires a bit more care as we need to start by identifying the characters that we want to identify in the first place.\nBy default, when we run the face detection algorithm, each detected faces is also assocaited with a sequence of 512 numbers called an embedding. The individual numbers do not have a direct meaning, but are instead defined in a relative way such that if two images of the same person are each associated with an embedding, we would expect the embeddings to be more similar to one another than they are to the embeddings of other faces. As an example, we can see the shape of the embedding object from our example frame above.\n\nout_face['embed'].shape\n\n(1, 512)\n\n\nOne way to use these embeddings is to first identify portraits of the actors that we are interested in from the video file, and then associate each of these with a character name. Then, we can compute the embeddings of these faces and store them. Finally, each time we detect a face in a frame, we can check how similar the embedding of the detected face is compared to the faces in our reference set. If one of the reference characters is sufficently close, we will assume that we have found the associated character.\nOne of the objects that we downloaded in the setup section of this notebook was a folder with portraits of the four main actors from Bewitched named with the name of their character in the show. In the code below we cycle through these images and record the name and embedding of each of the faces. Also, for reference, we store a thumbnail of the actor’s portrait.\n\nface_embed = []\nface_name = []\nface_img = []\nfor path in sorted(os.listdir(\"data/faces\")):\n  img = dvt.load_image(\"data/faces/\" + path)\n  out_face = anno_face.run(img)\n  face_embed += [out_face['embed'][0,:]]\n  face_name += [path[:-4]]\n  face_img += [cv2.resize(img, (200, 250))]\n\n\nface_embed = np.vstack(face_embed)\nface_name = np.array(face_name)\nface_img = np.hstack(face_img)\n\nHere are the portraits of the characters. They are ordered in alphabetical order: Darrin, Endora, Larry, and Sam.\n\nplt.imshow(face_img)\n\n\n\n\n\n\n\n\nIn order to measure the similarity of two embeddings, we use a mathematical technique called a dot product. This number will be 1 if two embeddings are exactly the same and -1 if they are exactly opposite one another. Typically, two faces that are of different people will have a similar score somewhere close to zero. Let’s start by seeing the similarity scores between the set of four portraits.\n\nnp.dot(face_embed, face_embed.T)\n\narray([[ 1.        ,  0.06135434, -0.19991453,  0.1696034 ],\n       [ 0.06135434,  1.0000001 , -0.09233201, -0.03006548],\n       [-0.19991453, -0.09233201,  0.9999998 , -0.03063131],\n       [ 0.1696034 , -0.03006548, -0.03063131,  0.9999999 ]],\n      dtype=float32)\n\n\nThe first column is Darrin, and then compares Darrin to each of the four images. Since he is the first image, we see .99999 because the photo is being compared to itself. Then, image of Darin is compared to Endora (.06135), Larry (-.19993), and Samantha (.16960).\nThe values along the diagonal are all 1 (or very close to one) because we are comparing one embedding to itself. All of the other similarity scores are somewhere between -0.2 and +.17, which is as expected since each of the portraits shows a unique actor.\nNow, let’s compare these reference faces to the embedding of the individual frame that we started this section with: the middle frame from the third shot showing Samantha in a pink bedroom (img_list[2]).\n\nout_face = anno_face.run(img_list[2])\ndist = np.dot(face_embed, out_face['embed'].T)\ndist\n\narray([[ 0.15695155],\n       [ 0.08723371],\n       [-0.04768787],\n       [ 0.61289704]], dtype=float32)\n\n\nHere we see that there is a much large similarity score (0.59) between this face and the last character in our set. And if we look at the image, we see in fact that this is the same character: Samantha. We can associate the face with the character name algorithmically using the following code.\n\nface_name[np.argmax(dist, axis = 0)]\n\narray(['sam'], dtype='&lt;U6')\n\n\nThe code above associates the face with the character that has the highest similarity. It would be a good idea to also store the similarity score and then only trust the relationship if this score is sufficently large, which we can do with the following code.\n\nnp.max(dist, axis = 0)\n\narray([0.61289704], dtype=float32)\n\n\nNow that we understand how this process works for a single image, let’s apply this technique to each of the shots in the sample file.",
    "crumbs": [
      "Tutorials",
      "1.2 Tutorial II: Moving Images"
    ]
  },
  {
    "objectID": "tutorial-moving-images.html#building-annotations-from-a-video-file",
    "href": "tutorial-moving-images.html#building-annotations-from-a-video-file",
    "title": "1.2 Tutorial II: Moving Images",
    "section": "Building Annotations from a Video File",
    "text": "Building Annotations from a Video File\nWe can combine the techniques above with the yield_video function from dvt to identify faces in each of the shots. It’s possible to run the face detection algorithm over every frame, and this has some advantages. For the interest of time and simplicity, we will only use our face detection algorithm on the middle frame of each detected shot.\n\noutput = []\nfor img, frame, msec in dvt.yield_video(\"data/bewitched_sample.mp4\"):\n  if frame in shot['mid'].values:\n    out = anno_face.run(img)\n    if out['boxes']:\n      out_df = pd.DataFrame(out['boxes'])\n      out_df['frame'] = frame\n      out_df['time'] = msec\n      dist = np.dot(face_embed, out['embed'].T)\n      out_df['character'] = face_name[np.argmax(dist, axis = 0)]\n      out_df['confidence'] = np.max(dist, axis = 0)\n      output.append(out_df)\n\noutput = pd.concat(output)\noutput = output[output['prob'] &gt; 0.9]\noutput\n\n\n\n\n\n\n\n\nface_id\nx\nxend\ny\nyend\nprob\nframe\ntime\ncharacter\nconfidence\n\n\n\n\n0\n0\n303\n420\n87\n256\n0.998564\n49\n1.634967\nsam\n0.730811\n\n\n0\n0\n241\n455\n181\n451\n0.999004\n141\n4.704700\nendora\n0.845351\n\n\n0\n0\n279\n413\n119\n299\n0.999931\n306\n10.210200\nsam\n0.612897\n\n\n0\n0\n110\n175\n165\n240\n0.998715\n717\n23.923900\ndarrin\n0.377601\n\n\n1\n1\n407\n465\n116\n193\n0.996172\n717\n23.923900\nlarry\n0.509248\n\n\n0\n0\n230\n444\n82\n374\n0.999645\n1110\n37.037000\ndarrin\n0.685214\n\n\n0\n0\n225\n428\n151\n410\n0.999964\n1266\n42.242200\nlarry\n0.803259\n\n\n\n\n\n\n\nTake a few minutes to go back to the thumbnail images from each of these detected faces and see how they line up with the characters that are actually present (if you are not familiar with Bewitched, use the portraits to learn each of the character names). You should see that they line up well, though the fourth shot has a fairly low confidence for Darrin due to the fact that his face is fairly small in the middle frame. We could do better if we added the first frame of the shot, which is more tightly focused on Darrin. We always recommend taking a look and reviewing the results as you go. Then, we can adjust our approach as needed based on the audiovisual data that we are working with and the areas of inquiry animating our analysis.",
    "crumbs": [
      "Tutorials",
      "1.2 Tutorial II: Moving Images"
    ]
  },
  {
    "objectID": "tutorial-moving-images.html#full-corpus-analysis",
    "href": "tutorial-moving-images.html#full-corpus-analysis",
    "title": "1.2 Tutorial II: Moving Images",
    "section": "Full Corpus Analysis",
    "text": "Full Corpus Analysis\nWe now have all of the methods and code needed to identify shots and faces in a moving image file. If we had an entire episode of Bewitched, we could run the exact same sequence of steps above on the full episode without changing anything. The only difference would be that the results would take much longer to finish running. If we had a folder with every episode of the series, we would just need to add one extra layer to our loop to cycle over each video file and make sure to include information about the filename on each row of the output. Finally, if we wanted to extend this to another series, we would just need to add the portraits of any characters that we wish to identify to our folder of reference images.\nWe return to the two shows — Bewitched and I Dream of Jeanine — that are central to Chapter 5 of Distant Viewing. The chapter centers around a comparison of the two magical sit-coms, which we can do by putting together shot boundary and then face detection and recognition.\nThe full set of video files for the two shows are quite large and the files are under copyright. It also takes a long time to process all of these files, particularly within a free Colab session. Since it is not possible to run the annotations directly on the full set here, we will instead work with the pre-computed annotations that were downloaded during the notebook setup. The structure of the dataset includes one row for each shot, with data about the timing of the shot and the number of faces.\n\nshots = pd.read_csv(\"data/ttl_sitcom_shots.csv\")\nshots\n\n\n\n\n\n\n\n\nseries\nvideo\nsid\nframe_start\nframe_stop\ntime\nnum_face\n\n\n\n\n0\nBewitched\nbw_s01_e01\n0\n0\n331\n13.84\n2\n\n\n1\nBewitched\nbw_s01_e01\n1\n332\n508\n7.38\n2\n\n\n2\nBewitched\nbw_s01_e01\n2\n509\n586\n3.25\n2\n\n\n3\nBewitched\nbw_s01_e01\n3\n587\n657\n2.96\n2\n\n\n4\nBewitched\nbw_s01_e01\n4\n658\n785\n5.34\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n102328\nI Dream of Jeannie\nidoj_s05_e26\n161\n32340\n32474\n5.63\n2\n\n\n102329\nI Dream of Jeannie\nidoj_s05_e26\n162\n32475\n32575\n4.21\n7\n\n\n102330\nI Dream of Jeannie\nidoj_s05_e26\n163\n32576\n33620\n43.58\n4\n\n\n102331\nI Dream of Jeannie\nidoj_s05_e26\n164\n33621\n33639\n0.79\n3\n\n\n102332\nI Dream of Jeannie\nidoj_s05_e26\n165\n33640\n33711\n3.00\n1\n\n\n\n\n102333 rows × 7 columns\n\n\n\nOne quick metric that we can compute is the average shot length for each of the two series. This is a commonly used measurement to understand the pacing of a movie or television show. Here we see that Bewitched is slightly faster, with an average shot length of 5.3 seconds compared to the 6.2 seconds of I Dream of Jeannie.\n\nshots.groupby(['series'])['time'].mean().sort_values()\n\nseries\nBewitched             5.323480\nI Dream of Jeannie    6.249331\nName: time, dtype: float64\n\n\nA related measurement is the median shot length, which here shows that the median shot length is slightly longer in Bewitched compared to Jeannie. So, while a shot in Bewitched is slightly longer, there may be a set of particularly long shows in Jeannie that causes the average of that show to be longer.\n\nshots.groupby(['series'])['time'].median().sort_values()\n\nseries\nI Dream of Jeannie    3.0\nBewitched             3.3\nName: time, dtype: float64\n\n\nOne of the interesting patterns that we noticed when writting the chapter based on these two series is that the average shot length is closely related to the number of faces present on the screen. We decided to truncate the number of faces to three (so that anything greater than 3 faces becomes 3) given our knowledge of the two shows. Notice how the average shot length increases with the number of faces on screen in both series.\n\nshots.loc[shots['num_face'] &gt; 3, 'num_face'] = 3\nshots.groupby(['series', 'num_face'])['time'].mean()\n\nseries              num_face\nBewitched           0            2.862049\n                    1            3.787825\n                    2            7.794772\n                    3           11.485585\nI Dream of Jeannie  0            3.288346\n                    1            3.571409\n                    2            9.444328\n                    3           14.627574\nName: time, dtype: float64\n\n\nWe saw an example of this pattern in our sample video file, where the one shot with two characters was much longer than the shot with a single character. This helps validate that the shot length is related to visual style. At least for these two series, it is related to how often we see a close up on an individual character talking (or acting) versus seeing multiple characters interacting together in a longer shot.\nWe also have a dataset that indicates the specific characters detected in shots from the two series. This uses a face detection algorithm similar to the one that we saw used in our sample video file. Here, we have one row for each detected character in a shot. There are four main characters in each of the series.\n\ncharacters = pd.read_csv(\"data/ttl_sitcom_characters.csv\")\ncharacters\n\n\n\n\n\n\n\n\nseries\nvideo\nsid\ntime\ncharacter\n\n\n\n\n0\nBewitched\nbw_s01_e01\n1\n7.38\nDarrin\n\n\n1\nBewitched\nbw_s01_e01\n11\n3.33\nDarrin\n\n\n2\nBewitched\nbw_s01_e01\n49\n2.58\nDarrin\n\n\n3\nBewitched\nbw_s01_e01\n52\n1.04\nDarrin\n\n\n4\nBewitched\nbw_s01_e01\n53\n4.33\nDarrin\n\n\n...\n...\n...\n...\n...\n...\n\n\n59798\nI Dream of Jeannie\nidoj_s05_e26\n143\n17.22\nRoger\n\n\n59799\nI Dream of Jeannie\nidoj_s05_e26\n146\n7.13\nRoger\n\n\n59800\nI Dream of Jeannie\nidoj_s05_e26\n151\n4.84\nRoger\n\n\n59801\nI Dream of Jeannie\nidoj_s05_e26\n161\n5.63\nRoger\n\n\n59802\nI Dream of Jeannie\nidoj_s05_e26\n163\n43.58\nRoger\n\n\n\n\n59803 rows × 5 columns\n\n\n\nWe can use the average amount of time that a character is on screen in a given episode as a rough measurement of the character’s visual importance in the show. Here, we compute this metric for each of the characters and sort in descending order within the series.\n\ntemp = characters.groupby(['series', 'video', 'character'])['time'].agg('sum').reset_index()\ntemp = temp.groupby(['series', 'character'])['time'].agg('mean').reset_index()\ntemp['time'] = temp['time'] / 60\ntemp.sort_values(['series', 'time'], ascending=False)\n\n\n\n\n\n\n\n\nseries\ncharacter\ntime\n\n\n\n\n7\nI Dream of Jeannie\nTony\n10.972683\n\n\n6\nI Dream of Jeannie\nRoger\n5.930222\n\n\n4\nI Dream of Jeannie\nAlfred\n4.698559\n\n\n5\nI Dream of Jeannie\nJeannie\n3.920746\n\n\n3\nBewitched\nSamantha\n6.986240\n\n\n0\nBewitched\nDarrin\n6.859456\n\n\n2\nBewitched\nLarry\n4.086578\n\n\n1\nBewitched\nEndora\n2.751711\n\n\n\n\n\n\n\nAnd, already we see one of the main conclusions of the analysis of these series: while they are thought of as very similar, their narrative structures are quite different. Jeannie is actually centered on the male lead Tony; the titual character Jeannie is often little more than a plot device who sets the action in motion. In constrast, Bewitched is most focused on the relationship between Samantha and Darrin, which each of them having nearly the same amount of overall screen time.\nOur annotations - shot detection, face detection, and face recognition - can now be analyzed from many different angles to explore different aspects of the shows, which we delve further into in Chapter 5. Key here is that just three annotations open up analytical possibilities, and can address humanities questions about audiovisual data.",
    "crumbs": [
      "Tutorials",
      "1.2 Tutorial II: Moving Images"
    ]
  },
  {
    "objectID": "tutorial-moving-images.html#conclusions-and-next-steps",
    "href": "tutorial-moving-images.html#conclusions-and-next-steps",
    "title": "1.2 Tutorial II: Moving Images",
    "section": "Conclusions and Next Steps",
    "text": "Conclusions and Next Steps\nThis notebook has given an introduction to use the distant viewing toolkit (dvt) to annotate movie image data from a digital video file. We’ve seen how to get basic metadata about a video file from within Python, how to cycle through the individual frames of a video file, and how to detect shot breaks using a custom algorithm. Although not specific to moving images, we also covered the process of using computer vision algorithms to do facial recognition, which is a common task in many applications but particularly useful when working with film and television corpora. Finally, we loaded a larger dataset showing all of the detected shots and characters across the entire runs of two U.S. Network-Era sitcoms and performed several example analyses on them.\nFor readers interested in more details about the specific case study of these two sitcoms, we suggest reading the fifth chapter of the Distant Viewing book, available under an open access license. The first two chapters of the book may also be of interest as they offer a more general theoretical and methodological approach to the computational analysis of digital images.",
    "crumbs": [
      "Tutorials",
      "1.2 Tutorial II: Moving Images"
    ]
  },
  {
    "objectID": "image.html",
    "href": "image.html",
    "title": "Images",
    "section": "",
    "text": "The scripts in this section show how to generate annotations from a collection of images. As an example, we use a small set of images from the FSA-OWI collection. If you are running the code on your own, you can use any of the following:\n\nThe Met Open Collections (met): Highlighted oil paintings from the many images of art works released under a public domain license by the the Metropolitan Museum of Art. [more info]\nUSDA Pomological Watercolors (usda): A selection of watercolors produced the USDA to document all known fruit varieties. [more info]\nFSA-OWI Archive (fsaowi): A selection of images from the photographic collection documenting life in the United States during the Great Depression and the Second World War. [more info]\nWomen: What to Tell Children (women-still): Still images from the shots detected in the 12th episode of the television show Women. Produced by WNED, a PBS member television station in Buffalo, New York, United States. [more info]\nDocumerica (documerica): Selection of photographs produced by the U.S. Environmental Protection Agency (EPA) in an effort to improve the health of the environment and American citizens. [more info]\nImageNet Large Scale Visual Recognition Challenge (ILSVRC) (imagenet): One example image from each of the 1000 categories used in the ILSVRC competition. [more info]\nMicrosoft COCO: Common Objects in Context (mscoco): One example image from each of the categories in the Microsoft COCO computer vision challenge. [more info]\n\nTo get results in your browser for all of these collections and your own data, see the Distant Viewing Explorer.",
    "crumbs": [
      "Images"
    ]
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "2.1 Metrics",
    "section": "",
    "text": "To learn more about the use of brightness, color, and complexity in image analysis, see the tutorial in Section 1.1.",
    "crumbs": [
      "Images",
      "2.1 Metrics"
    ]
  },
  {
    "objectID": "color.html",
    "href": "color.html",
    "title": "2.2 Color",
    "section": "",
    "text": "To learn more about the use of color and complexity in image analysis, see the tutorial in Section 1.1.",
    "crumbs": [
      "Images",
      "2.2 Color"
    ]
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "2.3 Image Classification",
    "section": "",
    "text": "This is a minimal example script showing how to do image classification using a set of images that are stored on the same machine where we are running the models. To start, we will load in a few modules that will be needed for the task.\n\nfrom os import listdir\nfrom os.path import splitext, join\nfrom transformers import AutoImageProcessor, ResNetForImageClassification\nfrom PIL import Image\n\nimport torch\nimport polars as pl\n\nNext, we load the model that we are interested in using. There are a large number of image classification algorithms on HuggingFace; most can be used exactly the same way by simply changing the name of the model in the function calls below.\n\nimage_processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n\n\nWith the models loaded, the next step is to build a set of paths to the images that we are interested in using. Here, we will create a list of all of the image files in the directory containing the FSA-OWI images. Then, to save time, we take just the first five images to work with. We could run over all of the images or use a different collection by changing the variables below.\n\ncollection = 'fsaowi'\nnum_images = 5\n\npaths = sorted(listdir(join('img', collection)))\npaths = [x for x in paths if splitext(x)[1] == '.jpg']\npaths = paths[:num_images]\n\nNow, we will load each of the images and run the model over it. For each output, we save the top 10 predictions, with both their names and probabilties.\n\ntop_n = 10\noutput = {'path': [], 'label': [], 'prob': []}\nfor path in paths:\n    image = Image.open(join('img', collection, path))\n    image = image.convert('RGB')\n    inputs = image_processor(image, return_tensors=\"pt\")\n    with torch.no_grad():\n        logits = model(**inputs).logits\n    index = logits.argsort(-1, descending=True)\n    index = index.tolist()[0][:top_n]\n    label = [model.config.id2label[x] for x in index]\n    odds = torch.exp(logits[0])\n    prob = odds / (1 + odds)\n    prob = sorted(prob.tolist(), reverse=True)[:top_n]\n    output['path'] += ([path] * top_n)\n    output['label'] += label\n    output['prob'] += prob\n\nThe output is constructed such that we can call the from_dict method from polars to construct a data frame. If needed, this can be saved as a CSV file with the write_csv method of the resulting data frame.\n\ndt = pl.from_dict(output)\ndt\n\n\nshape: (50, 3)\n\n\n\npath\nlabel\nprob\n\n\nstr\nstr\nf64\n\n\n\n\n\"LC-USE6-D-009289.jpg\"\n\"lab coat, laboratory coat\"\n0.492189\n\n\n\"LC-USE6-D-009289.jpg\"\n\"stretcher\"\n0.119932\n\n\n\"LC-USE6-D-009289.jpg\"\n\"cash machine, cash dispenser, …\n0.075144\n\n\n\"LC-USE6-D-009289.jpg\"\n\"iron, smoothing iron\"\n0.052356\n\n\n\"LC-USE6-D-009289.jpg\"\n\"sliding door\"\n0.051268\n\n\n…\n…\n…\n\n\n\"LC-USF33-011555.jpg\"\n\"scoreboard\"\n0.026516\n\n\n\"LC-USF33-011555.jpg\"\n\"banana\"\n0.023533\n\n\n\"LC-USF33-011555.jpg\"\n\"butcher shop, meat market\"\n0.021033\n\n\n\"LC-USF33-011555.jpg\"\n\"crutch\"\n0.015187\n\n\n\"LC-USF33-011555.jpg\"\n\"television, television system\"\n0.011028\n\n\n\n\n\n\nFinally, we can visualize the results using the following code:\n\nfor path in paths:\n    image = Image.open(join('img', collection, path))\n    image = image.convert('RGB')\n    res = dt.filter(pl.col(\"path\") == path)\n    print(\n        \"Predicted to be a {0:1} with probability {1:.04f}:\".format(\n        res['label'][0],\n        res['prob'][0]\n    ))\n    display(image)\n\nPredicted to be a lab coat, laboratory coat with probability 0.4922:\n\n\n\n\n\n\n\n\n\nPredicted to be a shovel with probability 0.9935:\n\n\n\n\n\n\n\n\n\nPredicted to be a tow truck, tow car, wrecker with probability 0.6395:\n\n\n\n\n\n\n\n\n\nPredicted to be a shovel with probability 0.1865:\n\n\n\n\n\n\n\n\n\nPredicted to be a ballplayer, baseball player with probability 0.2139:",
    "crumbs": [
      "Images",
      "2.3 Image Classification"
    ]
  },
  {
    "objectID": "object.html",
    "href": "object.html",
    "title": "2.4 Object Detection",
    "section": "",
    "text": "This is a minimal example script showing how to do object detection using a set of images that are stored on the same machine where we are running the models. To start, we will load in a few modules that will be needed for the task.\n\nfrom os import listdir\nfrom os.path import splitext, join\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\nfrom PIL import Image, ImageDraw, ImageFont\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import to_hex\n\nimport torch\nimport polars as pl\n\nNext, we load the model that we are interested in using. There are a large number of object detection algorithms on HuggingFace; most can be used exactly the same way by simply changing the name of the model in the function calls below.\n\nimage_processor = DetrImageProcessor.from_pretrained(\n    \"facebook/detr-resnet-50\", revision=\"no_timm\"\n)\nmodel = DetrForObjectDetection.from_pretrained(\n    \"facebook/detr-resnet-50\", revision=\"no_timm\"\n)\n\nWith the models loaded, the next step is to build a set of paths to the images that we are interested in using. Here, we will create a list of all of the image files in the directory containing the FSA-OWI images. Then, to save time, we take just the first five images to work with. We could run over all of the images or use a different collection by changing the variables below.\n\ncollection = 'fsaowi'\nnum_images = 5\n\npaths = sorted(listdir(join('img', collection)))\npaths = [x for x in paths if splitext(x)[1] == '.jpg']\npaths = paths[:num_images]\n\nNow, we will load each of the images and run the model over it. For each output, we save the top 10 predictions, with both their names and probabilties.\n\noutput = {\n    'path': [],\n    'label': [],\n    'scores': [],\n    'xmin': [],\n    'xmax': [],\n    'ymin': [],\n    'ymax': []\n}\nfor path in paths:\n    image = Image.open(join('img', collection, path))\n    image = image.convert('RGB')\n    inputs = image_processor(image, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n    target_sizes = torch.tensor([image.size[::-1]])\n    results = image_processor.post_process_object_detection(\n      outputs, target_sizes=target_sizes, threshold=0.9\n    )\n    output['path'] += [path] * list(results[0]['scores'].size())[0]\n    output['label'] += [\n      model.config.id2label[x] for x in results[0]['labels'].tolist()\n    ]\n    output['scores'] += results[0]['scores'].tolist()\n    output['xmin'] += results[0]['boxes'][:,0].tolist()\n    output['ymin'] += results[0]['boxes'][:,1].tolist()\n    output['xmax'] += results[0]['boxes'][:,2].tolist()\n    output['ymax'] += results[0]['boxes'][:,3].tolist()\n\nThe output is constructed such that we can call the from_dict method from polars to construct a data frame. If needed, this can be saved as a CSV file with the write_csv method of the resulting data frame.\n\ndt = pl.from_dict(output)\ndt\n\n\nshape: (35, 7)\n\n\n\npath\nlabel\nscores\nxmin\nxmax\nymin\nymax\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"LC-USE6-D-009289.jpg\"\n\"person\"\n0.957854\n506.371185\n572.894043\n367.898712\n431.890015\n\n\n\"LC-USE6-D-009289.jpg\"\n\"person\"\n0.99866\n121.295219\n409.498047\n82.548737\n542.25769\n\n\n\"LC-USE6-D-009289.jpg\"\n\"bed\"\n0.987241\n64.187057\n572.490295\n410.63327\n591.391235\n\n\n\"LC-USE6-D-009289.jpg\"\n\"bottle\"\n0.997298\n342.294922\n410.379364\n203.735657\n324.754761\n\n\n\"LC-USF33-003470.jpg\"\n\"car\"\n0.994868\n87.010506\n219.104797\n169.241333\n225.851654\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"LC-USF33-011555.jpg\"\n\"person\"\n0.959079\n267.734253\n386.68515\n244.484222\n433.476135\n\n\n\"LC-USF33-011555.jpg\"\n\"person\"\n0.969629\n99.657631\n166.626617\n121.926125\n305.381775\n\n\n\"LC-USF33-011555.jpg\"\n\"person\"\n0.957314\n345.315521\n431.0802\n327.404694\n435.424408\n\n\n\"LC-USF33-011555.jpg\"\n\"person\"\n0.977036\n376.891052\n481.247681\n179.886353\n360.448914\n\n\n\"LC-USF33-011555.jpg\"\n\"person\"\n0.94378\n383.915253\n463.273865\n28.586586\n185.458908\n\n\n\n\n\n\nFinally, to visualize the output we can use the following code to draw the bounding boxes on the images.\n\nfont_size = 15\n\nfor path in paths:\n    image = Image.open(join('img', collection, path))\n    image = image.convert('RGB')\n    res = dt.filter(pl.col(\"path\") == path)\n\n    cmap = plt.get_cmap('viridis')\n    label_vals = list(set(res['label'].to_list()))\n    colors = [\n      to_hex(cmap(i / len(label_vals))) for i in range(len(label_vals))\n    ]\n    label_color_map = {\n      label: colors[i % cmap.N] for i, label in enumerate(label_vals)\n    }\n\n    draw = ImageDraw.Draw(image)\n    font = ImageFont.load_default(size = font_size)\n\n    for row in res.rows(named=True):\n        color = label_color_map[row['label']]\n        \n        draw.rectangle(\n          [row['xmin'], row['ymin'], row['xmax'], row['ymax']],\n          outline=color,\n          width=2\n        )\n        text_len = draw.textlength(row['label'], font=font)\n        text_background = [\n          row['xmax'] - text_len,\n          row['ymax'],\n          row['xmax'],\n          row['ymax'] + font_size + 4\n        ]\n        draw.rectangle(text_background, fill=color)\n        draw.text(\n          (row['xmax'] - text_len, row['ymax']),\n          row['label'],\n          fill='white',\n          font=font\n        )\n\n    display(image)",
    "crumbs": [
      "Images",
      "2.4 Object Detection"
    ]
  },
  {
    "objectID": "depth.html",
    "href": "depth.html",
    "title": "2.5 Depth Estimation",
    "section": "",
    "text": "This is a minimal example script showing how to do depth estimation using a set of images that are stored on the same machine where we are running the models. To start, we will load in a few modules that will be needed for the task.\n\nfrom os import listdir\nfrom os.path import splitext, join\nfrom transformers import pipeline\nfrom PIL import Image\n\nimport torch\nimport polars as pl\nimport numpy as np\n\nNext, we load the model that we are interested in using.\n\npipe = pipeline(\n  task=\"depth-estimation\",\n  model=\"LiheYoung/depth-anything-small-hf\"\n)\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nDevice set to use mps:0\n\n\nWith the models loaded, the next step is to build a set of paths to the images that we are interested in using. Here, we will create a list of all of the image files in the directory containing the FSA-OWI images. Then, to save time, we take just the first five images to work with. We could run over all of the images or use a different collection by changing the variables below.\n\ncollection = 'fsaowi'\nnum_images = 5\n\npaths = sorted(listdir(join('img', collection)))\npaths = [x for x in paths if splitext(x)[1] == '.jpg']\npaths = paths[:num_images]\n\nNow, we will load each of the images and run the model over it. The output of the depth prediction algorithm is another image with the same height and width of the input. The values in the image range from 0 (farther point from the camera) to 255 (closest point to the camera). We can save the full output to visualize while also producing structured data about the detected depth. As an example, below we store the percentage of the image that is in the foreground (using the cutoff value of 192) and the percentage in the background (using the cutoff value of 64). We also store the brightness of the foreground and background. The cutoff values are just heuristics we use here. Test and explore what works for your applications.\n\noutput_image = {}\noutput = {\n  'path': [],\n  'foreground_percent': [],\n  'background_percent': [],\n  'foreground_value': [],\n  'background_rgb': []\n}\nfor path in paths:\n    image = Image.open(join('img', collection, path))\n    image = image.convert('RGB')\n    mask = pipe(image)[\"depth\"]\n    output_image[path] = mask\n    arr = np.asarray(mask)\n    img = np.asarray(image)\n    foreground_rgb = img[arr &gt; 192]\n    background_rgb = img[arr &lt; 64]\n    output['path'] += [path]\n    output['foreground_percent'] += [np.mean(arr &gt; 192)]\n    output['background_percent'] += [np.mean(arr &lt; 64)]\n    output['foreground_value'] += [np.mean(foreground_rgb)]\n    output['background_rgb'] += [np.mean(background_rgb)]\n\n/Users/admin/gh/dvscripts/env/lib/python3.12/site-packages/torch/nn/functional.py:4594: UserWarning: The operator 'aten::upsample_bicubic2d.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n  return torch._C._nn.upsample_bicubic2d(\n\n\nWe can convert the structured outputs that we have produced above into a data frame to save and explore further.\n\ndt = pl.from_dict(output)\ndt\n\n\nshape: (5, 5)\n\n\n\npath\nforeground_percent\nbackground_percent\nforeground_value\nbackground_rgb\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"LC-USE6-D-009289.jpg\"\n0.193682\n0.585502\n98.683269\n68.277978\n\n\n\"LC-USF33-003470.jpg\"\n0.145922\n0.368624\n141.380721\n132.634665\n\n\n\"LC-USF33-006507.jpg\"\n0.179074\n0.213099\n120.996769\n83.816742\n\n\n\"LC-USF33-011361.jpg\"\n0.112612\n0.506222\n110.983864\n156.559968\n\n\n\"LC-USF33-011555.jpg\"\n0.120001\n0.358493\n102.96259\n105.168447\n\n\n\n\n\n\nAnd, to visualize the output, print the full image outputs themselves. They will show the depth of each pixel as a range from black (farthest back) to white (closest to the camera).\n\nfor path in paths:\n    display(output_image[path])",
    "crumbs": [
      "Images",
      "2.5 Depth Estimation"
    ]
  },
  {
    "objectID": "segment.html",
    "href": "segment.html",
    "title": "2.6 Image Segmentation",
    "section": "",
    "text": "This is a minimal example script showing how to do image segmentation using a set of images that are stored on the same machine where we are running the models. To start, we will load in a few modules that will be needed for the task.\n\nfrom os import listdir\nfrom os.path import splitext, join\nfrom transformers import pipeline\nfrom PIL import Image\n\nimport torch\nimport polars as pl\nimport numpy as np\n\nNext, we load the model that we are interested in using. There are a large number of image segmentation algorithms on HuggingFace; most can be used exactly the same way by simply changing the name of the model in the function calls below.\n\npipeline = pipeline(\n  \"image-segmentation\",\n  \"nvidia/segformer-b0-finetuned-ade-512-512\"\n)\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nDevice set to use mps:0\n\n\nWith the models loaded, the next step is to build a set of paths to the images that we are interested in using. Here, we will create a list of all of the image files in the directory containing the FSA-OWI images. Then, to save time, we take just the first five images to work with. We could run over all of the images or use a different collection by changing the variables below.\n\ncollection = 'fsaowi'\nnum_images = 5\n\npaths = sorted(listdir(join('img', collection)))\npaths = [x for x in paths if splitext(x)[1] == '.jpg']\npaths = paths[:num_images]\n\nNow, we will load each of the images and run the model over it. For each image, output given by the pipeline is an array of dictionary objects, where each dictionary has a single score, a label, and an image mask. The mask is a black and white image where white represents those pixels associated with the category. We will save both the full output and a summary statistic for each of the images. The full output will be used for visualization below. There are many different kinds of summarization that can be done, including combining with other masks such as the depth estimation. Play around with different options for your task!\n\noutput_image = {}\noutput = {'path': [], 'score': [], 'label': [], 'percent': []}\nfor path in paths:\n    image = Image.open(join('img', collection, path))\n    image = image.convert('RGB')\n    outputs = pipeline(image)\n    output_image[path] = []\n    for oput in outputs:\n        output_image[path] += [oput]\n        output['path'] += [path]\n        output['score'] += [oput['score']]\n        output['label'] += [oput['label']]\n        mask = np.asarray(oput['mask'])\n        output['percent'] += [np.mean(mask == 255)]\n\nWe can convert the structured outputs that we have produced above into a data frame to save and explore further. Note that this particular model does not have score values, but we include it in the code because other segmentation models do. Unlike many other models, the ordering of the categories are fixed rather than sorted by proportion or probability. So, below we sort by the proportion of the image that is covered by a category.\n\ndt = pl.from_dict(output)\ndt = dt.sort(by = ['path', 'percent'], descending = [False, True])\ndt\n\n\nshape: (57, 4)\n\n\n\npath\nscore\nlabel\npercent\n\n\nstr\nnull\nstr\nf64\n\n\n\n\n\"LC-USE6-D-009289.jpg\"\nnull\n\"wall\"\n0.518868\n\n\n\"LC-USE6-D-009289.jpg\"\nnull\n\"person\"\n0.293332\n\n\n\"LC-USE6-D-009289.jpg\"\nnull\n\"door\"\n0.113768\n\n\n\"LC-USE6-D-009289.jpg\"\nnull\n\"floor\"\n0.053646\n\n\n\"LC-USE6-D-009289.jpg\"\nnull\n\"painting\"\n0.01292\n\n\n…\n…\n…\n…\n\n\n\"LC-USF33-011555.jpg\"\nnull\n\"earth\"\n0.153641\n\n\n\"LC-USF33-011555.jpg\"\nnull\n\"building\"\n0.027349\n\n\n\"LC-USF33-011555.jpg\"\nnull\n\"trade name\"\n0.006667\n\n\n\"LC-USF33-011555.jpg\"\nnull\n\"floor\"\n0.005737\n\n\n\"LC-USF33-011555.jpg\"\nnull\n\"painting\"\n0.000328\n\n\n\n\n\n\nTo visualize the output, we can display any of the masks as an image. For example, below we display each of the masks of the images for the label ‘person’. You can replace this with any of the categories that you are interested in looking at. Note that we are only showing those images with at least some pixels equal to the label ‘person’.\n\nfor path in paths:\n    masks = output_image[path]\n    mask_person = [x['mask'] for x in masks if x['label'] == 'person']\n    if mask_person:\n        display(mask_person[0])",
    "crumbs": [
      "Images",
      "2.6 Image Segmentation"
    ]
  },
  {
    "objectID": "embed.html",
    "href": "embed.html",
    "title": "2.7 Embedding",
    "section": "",
    "text": "This is a minimal example script showing how to do an image embedding using a set of images that are stored on the same machine where we are running the models. To start, we will load in a few modules that will be needed for the task.\n\nfrom os import listdir\nfrom os.path import splitext, join\nfrom transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\n\nimport torch\nimport polars as pl\nimport numpy as np\n\nNext, we load the model that we are interested in using. There are a large number of image embedding algorithms on HuggingFace; most can be used exactly the same way by simply changing the name of the model in the function calls below.\n\nimage_processor = ViTImageProcessor.from_pretrained(\n  'google/vit-base-patch16-224-in21k'\n)\nmodel = ViTModel.from_pretrained(\n  'google/vit-base-patch16-224-in21k'\n)\n\nWith the models loaded, the next step is to build a set of paths to the images that we are interested in using. Here, we will create a list of all of the image files in the directory containing the FSA-OWI images. For this model, it will be instructive to work with the entire set of images in the directory.\n\ncollection = 'fsaowi'\n\npaths = sorted(listdir(join('img', collection)))\npaths = [x for x in paths if splitext(x)[1] == '.jpg']\n\nNow, we will load each of the images and run the model over it.\n\noutput = {}\nfor path in paths:\n    image = Image.open(join('img', collection, path))\n    image = image.convert('RGB')\n    input = image_processor(image, return_tensors=\"pt\")\n    outputs = model(**input)\n    pooler_output = outputs.pooler_output\n    output[path] = pooler_output[0].detach().numpy() \n    output[path] = output[path] / np.linalg.norm(output[path], 2)\n\nThen, for any reference image, we can compute the similarity scores relative to all other images. Here we have an example looking at the score relative to the first image.\n\nref_path = paths[0]\nsimilarity = []\nfor path in paths:\n    similarity += [np.dot(output[ref_path], output[path])]\n\nThen, we can see which images are a closest match (the first image will be the one that we picked out, since any image will always be closest to itself).\n\nsimilarity = np.array(similarity)\nsimilarity_rank = np.argsort(similarity * -1)\nfor idx in similarity_rank[:5]:\n    path = paths[idx]\n    image = Image.open(join('img', collection, path))\n    display(image)",
    "crumbs": [
      "Images",
      "2.7 Embedding"
    ]
  },
  {
    "objectID": "video.html",
    "href": "video.html",
    "title": "Video + Audio",
    "section": "",
    "text": "The scripts in this section show how to generate annotations from a video file. As an example, we use a one minute sample from the 2010 State of the Union Address. If you are running the code on your own, you can use any of the following:\n\nSteamboat Willie (steamboat): A clip from the short animated film Steamboat Willie, the first film to feature Mickey and Minnie Mouse. [more info]\nMountain Community Television (mctv): Produced by Mountain Community Television, this is a clip from the ‘First citizen workshop … aimed at letting citizens know what they can do to help implement the Strip Mining Control and Reclamation Act of 1977.’ [more info]\nBlack Journal: Interview with Angela Davis (blackjournal): A clip from episode 67 the television program Black Journal. Produced by NET Division, Educational Broadcasting Corporation. [more info]\nWomen: What to Tell Children (women): A clip from the 12th episode of the television program Women. Produced by WNED, a PBS member television station in Buffalo, New York, United States. [more info]\nTaskmaster: Series 7, Episode 4 (taskmaster): A short clip from the British panel game show Taskmaster. [more info]\nState of the Union Address (sotu): A clip from Barack Obama’s State of the Union Address in 2010. [more info]\nNew Years Message, Emmanuel Macron [fr] (macron): A clip from Emmanuel Macron’s 2025 New Years message. Audio in French. [more info]\n\nTo get results in your browser for all of these collections and your own data, see the Distant Viewing Explorer.",
    "crumbs": [
      "Video + Audio"
    ]
  },
  {
    "objectID": "shot.html",
    "href": "shot.html",
    "title": "3.1 Shot Boundary",
    "section": "",
    "text": "This is a minimal example script showing how to detect shot boundaries in a video file. To start, we will load in a few modules that will be needed for the task.\n\nimport dvt\nimport polars as pl\nfrom PIL import Image\n\nWe will use the video ‘sotu.mp4’ in this script. You can use the other example files or your own files as well! To start, we want to get metadata about the video file, which will be helpful in a few moments.\n\nmeta = dvt.video_info('video/sotu.mp4')\nmeta\n\n{'frame_count': 1979.0,\n 'height': 720.0,\n 'width': 1280.0,\n 'fps': 29.97002997002997}\n\n\nNext, we run the shot break detection algorithm over the video file and store the results.\n\nanno = dvt.AnnoShotBreaks()\noutput = anno.run('video/sotu.mp4')\n\nOnce the data are generated, we produce a data frame and do bit of cleaning of the output to generate timestamps in addition to the frame numbers. And now we have the predicted shot breaks in a structured format!\n\ndt = pl.from_dict(output['scenes'])\ndt = dt.with_columns((pl.col(\"start\") / meta['fps']).alias(\"start_time\"))\ndt = dt.with_columns((pl.col(\"end\") / meta['fps']).alias(\"end_time\"))\ndt\n\n\nshape: (8, 4)\n\n\n\nstart\nend\nstart_time\nend_time\n\n\ni32\ni32\nf64\nf64\n\n\n\n\n0\n199\n0.0\n6.639967\n\n\n214\n842\n7.140467\n28.094733\n\n\n854\n1149\n28.495133\n38.3383\n\n\n1163\n1376\n38.805433\n45.912533\n\n\n1377\n1655\n45.9459\n55.221833\n\n\n1656\n1781\n55.2552\n59.426033\n\n\n1782\n1877\n59.4594\n62.629233\n\n\n1878\n1978\n62.6626\n65.999267\n\n\n\n\n\n\nWe can use the yield_video function to cycle through all the frames in the video. Using the shot boundary detection algorithm, we can save the first shot from each shot as well as print out the frames below. The images can be used as inputs to any of the image-based algorithms shown in the previous section.\n\nfirst_frames = []\nfor image, frame, timestamp in dvt.yield_video('video/sotu.mp4'):\n    if frame in dt['start'].to_list():\n        first_frames += [frame]\n        pimg = Image.fromarray(image, 'RGB')\n        display(pimg)",
    "crumbs": [
      "Video + Audio",
      "3.1 Shot Boundary"
    ]
  },
  {
    "objectID": "transcription.html",
    "href": "transcription.html",
    "title": "3.2 Transcription",
    "section": "",
    "text": "This is a minimal example script showing how to detect shot boundaries in a video file. To start, we will load in a few modules that will be needed for the task.\n\nfrom transformers import pipeline, WhisperProcessor\nfrom pydub import AudioSegment\n\nimport polars as pl\nimport librosa\nimport tempfile\n\nNext, we need to load the model. Here we will use a smaller version of the Whisper algorithm called ‘whisper-base’. The openai site has many larger (and smaller) models that can produce better results in trading off download size and speed (or worse results with smaller models, if you use the tiny version). Note that we can do multilingual transcriptions but need to set the language that we want to transcribe in the input.\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=\"openai/whisper-small\",\n    chunk_length_s=30,\n    stride_length_s=5\n)\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n\npipe.model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n  language=\"english\", task=\"transcribe\"\n)\n\nDevice set to use mps:0\n\n\nThe algorthm only takes audio inputs, so we need to convert our video into a temporary wave file. If you have a wave file, this can be loaded and passed directly into the model in the next step.\n\nwith tempfile.NamedTemporaryFile(suffix='.wav') as temp_file:\n    audio = AudioSegment.from_file('video/sotu.mp4', format=\"mp4\")\n    audio.export(temp_file.name, format=\"wav\")\n    audio, sr = librosa.load(temp_file.name, sr=16000)\n    sample = {'array': audio, 'sampling_rate': sr}\n\nNow, we generate the transcription itself and store the results in a dictionary.\n\nprediction = pipe(sample.copy(), return_timestamps='word')[\"chunks\"]\noutput = {\n    'start': [x['timestamp'][0] for x in prediction],\n    'stop': [x['timestamp'][1] for x in prediction],\n    'text': [x['text'] for x in prediction],\n}\n\nThe output is constructed such that we can call the from_dict method from polars to construct a data frame. If needed, this can be saved as a CSV file with the write_csv method of the resulting data frame.\n\ndt = pl.from_dict(output)\ndt\n\n\nshape: (87, 3)\n\n\n\nstart\nstop\ntext\n\n\nf64\nf64\nstr\n\n\n\n\n0.0\n0.08\n\" And\"\n\n\n0.08\n0.08\n\" what\"\n\n\n0.08\n0.28\n\" else\"\n\n\n0.28\n0.44\n\" they\"\n\n\n0.44\n2.02\n\" share?\"\n\n\n…\n…\n…\n\n\n41.94\n42.2\n\" future\"\n\n\n42.2\n42.38\n\" than\"\n\n\n42.38\n42.52\n\" I\"\n\n\n42.52\n42.58\n\" am\"\n\n\n42.58\nnull\n\" tonight.\"",
    "crumbs": [
      "Video + Audio",
      "3.2 Transcription"
    ]
  },
  {
    "objectID": "diarization.html",
    "href": "diarization.html",
    "title": "3.3 Diarization",
    "section": "",
    "text": "This is a minimal example script showing how to detect speakers in the audio track of a video file. To start, we will load in a few modules that will be needed for the task.\n\nfrom pyannote.audio import Pipeline\nfrom pydub import AudioSegment\n\nimport polars as pl\nimport tempfile\n\nNext, we need to load the model.\n\npipeline = Pipeline.from_pretrained(\"statsmaths/diarize\")\n\nINFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\nINFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n\n\nThe algorthm only takes audio inputs, so we need to convert our video into a temporary wave file. If you have a wave file, this can be loaded and passed directly into the model. We will pass the audio file directly to the diarization model here as well.\n\nwith tempfile.NamedTemporaryFile(suffix='.wav') as temp_file:\n    audio = AudioSegment.from_file('video/sotu.mp4', format=\"mp4\")\n    audio.export(temp_file.name, format=\"wav\")\n    diarization = pipeline(temp_file.name)\n\nThe output requires a little bit of parsing before it is ready to use. Here we produce an output dictionary.\n\ndata = diarization.to_lab().split('\\n')\ndata = [x.split(' ') for x in data]\ndata = [x for x in data if len(x) == 3]\noutput = {\n  'start_time': [float(x[0]) for x in data],\n  'end_time': [float(x[1]) for x in data],\n  'speaker': [x[2] for x in data]\n}\n\nThe output is constructed such that we can call the from_dict method from polars to construct a data frame. If needed, this can be saved as a CSV file with the write_csv method of the resulting data frame.\n\ndt = pl.from_dict(output)\ndt\n\n\nshape: (14, 3)\n\n\n\nstart_time\nend_time\nspeaker\n\n\nf64\nf64\nstr\n\n\n\n\n0.031\n0.875\n\"SPEAKER_00\"\n\n\n2.309\n5.954\n\"SPEAKER_00\"\n\n\n8.148\n10.78\n\"SPEAKER_00\"\n\n\n11.928\n17.007\n\"SPEAKER_00\"\n\n\n18.104\n20.348\n\"SPEAKER_00\"\n\n\n…\n…\n…\n\n\n29.495\n30.355\n\"SPEAKER_00\"\n\n\n32.903\n34.085\n\"SPEAKER_00\"\n\n\n35.536\n37.915\n\"SPEAKER_00\"\n\n\n39.265\n42.961\n\"SPEAKER_00\"\n\n\n65.641\n66.012\n\"SPEAKER_00\"",
    "crumbs": [
      "Video + Audio",
      "3.3 Diarization"
    ]
  },
  {
    "objectID": "text.html",
    "href": "text.html",
    "title": "Text",
    "section": "",
    "text": "The scripts in this section show how to generate annotations from a file of text. We include it here because text is often include with visual materials (i.e., captions) or produced from them (i.e., OCR or transcriptions). As an example, we use a set of 100 movie quotes in the scripts. If you are running the code on your own, you can use any of the following:\n\nAFI’s 100 Years… 100 Movie Quotes (afi): The 100 most iconic movie quotations, as voted by a panel 1,500 film artists, critics, and historians. [more info]\nState of the Union Address [transcript] (sotu-text): Transcript from Barack Obama’s State of the Union Address in 2010. [more info]\nAmazon Movie Reviews (amazon): A small sample of movie review titles from Amazon. [more info]\nNew Years Message, Emmanuel Macron [fr, transcript] (macron-text): Transcript from Emmanuel Macron’s New Years message. [more info]\n\nTo get results in your browser for all of these collections and your own data, see the Distant Viewing Explorer.",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "sentiment.html",
    "href": "sentiment.html",
    "title": "4.1 Sentiment Analysis",
    "section": "",
    "text": "This is a minimal example script showing how to do sentiment analysis classification using a set of short texts stored on the same machine where we are running the models. To start, we will load in a few modules that will be needed for the task.\n\nfrom os import listdir\nfrom os.path import splitext, join\nfrom transformers import (\n    DistilBertTokenizer,\n    DistilBertForSequenceClassification\n)\n\nimport torch\nimport polars as pl\n\nNext, we load the model that we are interested in using. There are a large number of sentiment analysis algorithms on HuggingFace; most can be used exactly the same way by simply changing the name of the model in the function calls below.\n\ntokenizer = DistilBertTokenizer.from_pretrained(\n  \"distilbert-base-uncased-finetuned-sst-2-english\"\n)\nmodel = DistilBertForSequenceClassification.from_pretrained(\n  \"distilbert-base-uncased-finetuned-sst-2-english\"\n)\n\nWith the models loaded, the next step is to load in the dataset. Here, we have a series of short texts stored with one text per line in a file.\n\nwith open('text/afi.txt', 'r') as f:\n    input_text = f.read().splitlines() \n\n\ninput_text = [x for x in input_text if x != \"\"]\n\nAnd now we run the model over each of the lines, saving the results.\n\noutput = {'text': [], 'label': [], 'score': []}\nfor iput in input_text:\n    iput_tokenized = tokenizer(iput, return_tensors=\"pt\")\n    with torch.no_grad():\n        logits = model(**iput_tokenized).logits\n    predicted_class_id = logits.argmax().item()\n    prob = 1 / (1 + torch.exp(-1 * logits.max()))\n    output['text'] += [iput]\n    output['label'] += [model.config.id2label[predicted_class_id]]\n    output['score'] += [prob.detach().numpy().tolist()]\n\nThe output is constructed such that we can call the from_dict method from polars to construct a data frame. If needed, this can be saved as a CSV file with the write_csv method of the resulting data frame.\n\ndt = pl.from_dict(output)\ndt\n\n\nshape: (100, 3)\n\n\n\ntext\nlabel\nscore\n\n\nstr\nstr\nf64\n\n\n\n\n\"Frankly, my dear, I don't give…\n\"NEGATIVE\"\n0.985764\n\n\n\"I'm gonna make him an offer he…\n\"POSITIVE\"\n0.937932\n\n\n\"You don't understand! I coulda…\n\"POSITIVE\"\n0.755843\n\n\n\"Toto, I've a feeling we're not…\n\"NEGATIVE\"\n0.943447\n\n\n\"Here's looking at you, kid.\"\n\"POSITIVE\"\n0.980705\n\n\n…\n…\n…\n\n\n\"Snap out of it!\"\n\"NEGATIVE\"\n0.64849\n\n\n\"My mother thanks you. My fathe…\n\"POSITIVE\"\n0.989329\n\n\n\"Nobody puts Baby in a corner.\"\n\"NEGATIVE\"\n0.922234\n\n\n\"I'll get you, my pretty, and y…\n\"POSITIVE\"\n0.987145\n\n\n\"I'm the king of the world!\"\n\"POSITIVE\"\n0.989484",
    "crumbs": [
      "Text",
      "4.1 Sentiment Analysis"
    ]
  },
  {
    "objectID": "review.html",
    "href": "review.html",
    "title": "4.2 Review Prediction",
    "section": "",
    "text": "This is a minimal example script showing how to do review prediction classification using a set of short texts stored on the same machine where we are running the models. To start, we will load in a few modules that will be needed for the task.\n\nfrom os import listdir\nfrom os.path import splitext, join\nfrom transformers import pipeline\n\nimport torch\nimport polars as pl\n\nNext, we load the model that we are interested in using.\n\nmodel = pipeline(\n  task='sentiment-analysis',\n  model='nlptown/bert-base-multilingual-uncased-sentiment'\n)\n\nDevice set to use mps:0\n\n\nWith the models loaded, the next step is to load in the dataset. Here, we have a series of short texts stored with one text per line in a file.\n\nwith open('text/afi.txt', 'r') as f:\n    input_text = f.read().splitlines() \n\n\ninput_text = [x for x in input_text if x != \"\"]\n\nAnd now we run the model over each of the lines, saving the results.\n\noutput = {'text': [], 'label': [], 'score': []}\nfor iput in input_text:\n    outputs = model(iput)[0]\n    output['text'] += [iput]\n    output['label'] += [outputs['label']]\n    output['score'] += [outputs['score']]\n\nThe output is constructed such that we can call the from_dict method from polars to construct a data frame. If needed, this can be saved as a CSV file with the write_csv method of the resulting data frame.\n\ndt = pl.from_dict(output)\ndt\n\n\nshape: (100, 3)\n\n\n\ntext\nlabel\nscore\n\n\nstr\nstr\nf64\n\n\n\n\n\"Frankly, my dear, I don't give…\n\"1 star\"\n0.331306\n\n\n\"I'm gonna make him an offer he…\n\"1 star\"\n0.389364\n\n\n\"You don't understand! I coulda…\n\"1 star\"\n0.292674\n\n\n\"Toto, I've a feeling we're not…\n\"3 stars\"\n0.233159\n\n\n\"Here's looking at you, kid.\"\n\"5 stars\"\n0.408434\n\n\n…\n…\n…\n\n\n\"Snap out of it!\"\n\"1 star\"\n0.663948\n\n\n\"My mother thanks you. My fathe…\n\"5 stars\"\n0.738161\n\n\n\"Nobody puts Baby in a corner.\"\n\"1 star\"\n0.647303\n\n\n\"I'll get you, my pretty, and y…\n\"5 stars\"\n0.738294\n\n\n\"I'm the king of the world!\"\n\"5 stars\"\n0.89788",
    "crumbs": [
      "Text",
      "4.2 Review Prediction"
    ]
  },
  {
    "objectID": "comment.html",
    "href": "comment.html",
    "title": "4.3 Comment Prediction",
    "section": "",
    "text": "This is a minimal example script showing how to do toxicity detection using a set of short texts stored on the same machine where we are running the models. To start, we will load in a few modules that will be needed for the task.\n\nfrom os import listdir\nfrom os.path import splitext, join\nfrom transformers import pipeline\n\nimport torch\nimport polars as pl\n\nNext, we load the model that we are interested in using.\n\nmodel = pipeline(\n  task='sentiment-analysis',\n  model='unitary/toxic-bert'\n)\n\nDevice set to use mps:0\n\n\nWith the models loaded, the next step is to load in the dataset. Here, we have a series of short texts stored with one text per line in a file.\n\nwith open('text/afi.txt', 'r') as f:\n    input_text = f.read().splitlines() \n\n\ninput_text = [x for x in input_text if x != \"\"]\n\nAnd now we run the model over each of the lines, saving the results.\n\noutput = {'text': [], 'label': [], 'score': []}\nfor iput in input_text:\n    outputs = model(iput, top_k=5)\n    for out in outputs:\n        output['text'] += [iput]\n        output['label'] += [out['label']]\n        output['score'] += [out['score']]\n\nThe output is constructed such that we can call the from_dict method from polars to construct a data frame. If needed, this can be saved as a CSV file with the write_csv method of the resulting data frame.\n\ndt = pl.from_dict(output)\ndt\n\n\nshape: (500, 3)\n\n\n\ntext\nlabel\nscore\n\n\nstr\nstr\nf64\n\n\n\n\n\"Frankly, my dear, I don't give…\n\"toxic\"\n0.891361\n\n\n\"Frankly, my dear, I don't give…\n\"obscene\"\n0.763789\n\n\n\"Frankly, my dear, I don't give…\n\"insult\"\n0.032939\n\n\n\"Frankly, my dear, I don't give…\n\"severe_toxic\"\n0.013396\n\n\n\"Frankly, my dear, I don't give…\n\"threat\"\n0.001882\n\n\n…\n…\n…\n\n\n\"I'm the king of the world!\"\n\"toxic\"\n0.006945\n\n\n\"I'm the king of the world!\"\n\"obscene\"\n0.000709\n\n\n\"I'm the king of the world!\"\n\"insult\"\n0.000302\n\n\n\"I'm the king of the world!\"\n\"identity_hate\"\n0.000203\n\n\n\"I'm the king of the world!\"\n\"threat\"\n0.000197",
    "crumbs": [
      "Text",
      "4.3 Comment Prediction"
    ]
  },
  {
    "objectID": "mask.html",
    "href": "mask.html",
    "title": "4.4 Text Mask",
    "section": "",
    "text": "This is a minimal example script showing how to do mask prediction detection using a set of short texts stored on the same machine where we are running the models. To start, we will load in a few modules that will be needed for the task.\n\nfrom os import listdir\nfrom os.path import splitext, join\nfrom transformers import pipeline\n\nimport torch\nimport polars as pl\n\nNext, we load the model that we are interested in using.\n\nmodel = pipeline(\n  task='fill-mask',\n  model='google-bert/bert-base-cased'\n)\n\nBertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\nSome weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use mps:0\n\n\nWith the models loaded, the next step is to load in the dataset. Here, we have a series of short texts stored with one text per line in a file.\n\nwith open('text/afimask.txt', 'r') as f:\n    input_text = f.read().splitlines() \n\n\ninput_text = [x for x in input_text if x != \"\"]\n\nAnd now we run the model over each of the lines, saving the results.\n\noutput = {'text': [], 'token_str': [], 'score': []}\nfor iput in input_text:\n    outputs = model(iput, top_k=5)\n    for out in outputs:\n        output['text'] += [iput]\n        output['token_str'] += [out['token_str']]\n        output['score'] += [out['score']]\n\nThe output is constructed such that we can call the from_dict method from polars to construct a data frame. If needed, this can be saved as a CSV file with the write_csv method of the resulting data frame.\n\ndt = pl.from_dict(output)\ndt\n\n\nshape: (500, 3)\n\n\n\ntext\ntoken_str\nscore\n\n\nstr\nstr\nf64\n\n\n\n\n\"Frankly, my dear, I don't give…\n\"shit\"\n0.460454\n\n\n\"Frankly, my dear, I don't give…\n\"damn\"\n0.388685\n\n\n\"Frankly, my dear, I don't give…\n\"fuck\"\n0.060158\n\n\n\"Frankly, my dear, I don't give…\n\"crap\"\n0.037962\n\n\n\"Frankly, my dear, I don't give…\n\"thing\"\n0.005191\n\n\n…\n…\n…\n\n\n\"I'm the [MASK] of the world!\"\n\"ruler\"\n0.147359\n\n\n\"I'm the [MASK] of the world!\"\n\"end\"\n0.125782\n\n\n\"I'm the [MASK] of the world!\"\n\"king\"\n0.067203\n\n\n\"I'm the [MASK] of the world!\"\n\"center\"\n0.028192\n\n\n\"I'm the [MASK] of the world!\"\n\"head\"\n0.02607",
    "crumbs": [
      "Text",
      "4.4 Text Mask"
    ]
  },
  {
    "objectID": "multimodal.html",
    "href": "multimodal.html",
    "title": "Multimodal",
    "section": "",
    "text": "The scripts in this section show how to use multimodal models to work with images. As an example, we use a small set of images from the FSA-OWI collection. If you are running the code on your own, you can use any of the following:\n\nThe Met Open Collections (met): Highlighted oil paintings from the many images of art works released under a public domain license by the the Metropolitan Museum of Art. [more info]\nUSDA Pomological Watercolors (usda): A selection of watercolors produced the USDA to document all known fruit varieties. [more info]\nFSA-OWI Archive (fsaowi): A selection of images from the photographic collection documenting life in the United States during the Great Depression and the Second World War. [more info]\nWomen: What to Tell Children (women-still): Still images from the shots detected in the 12th episode of the television show Women. Produced by WNED, a PBS member television station in Buffalo, New York, United States. [more info]\nDocumerica (documerica): Selection of photographs produced by the U.S. Environmental Protection Agency (EPA) in an effort to improve the health of the environment and American citizens. [more info]\nImageNet Large Scale Visual Recognition Challenge (ILSVRC) (imagenet): One example image from each of the 1000 categories used in the ILSVRC competition. [more info]\nMicrosoft COCO: Common Objects in Context (mscoco): One example image from each of the categories in the Microsoft COCO computer vision challenge. [more info]\n\nTo get results in your browser for all of these collections and your own data, see the Distant Viewing Explorer.",
    "crumbs": [
      "Multimodal"
    ]
  },
  {
    "objectID": "zeroshot.html",
    "href": "zeroshot.html",
    "title": "5.1 Zero-Shot Model",
    "section": "",
    "text": "This is a minimal example script showing how to apply a Zero-shot model using a set of images that are stored on the same machine where we are running the models. To start, we will load in a few modules that will be needed for the task.\n\nfrom os import listdir\nfrom os.path import splitext, join\nfrom transformers import (\n    AutoModel,\n    AutoTokenizer,\n    SiglipTextModel,\n    AutoProcessor,\n    SiglipVisionModel\n)\nfrom PIL import Image\n\nimport torch\nimport polars as pl\nimport numpy as np\n\nNext, we load the model that we are interested in using.\n\ntokenizer = AutoTokenizer.from_pretrained(\n  'google/siglip-base-patch16-256'\n)\ntext_model = SiglipTextModel.from_pretrained(\n  'google/siglip-base-patch16-256'\n)\nimage_processor = AutoProcessor.from_pretrained(\n  'google/siglip-base-patch16-256'\n).image_processor\nvision_model = SiglipVisionModel.from_pretrained(\n  'google/siglip-base-patch16-256'\n)\n\nWith the models loaded, the next step is to build a set of paths to the images that we are interested in using. Here, we will create a list of all of the image files in the directory containing the FSA-OWI images. For this model, it will be instructive to work with the entire set of images in the directory.\n\ncollection = 'fsaowi'\n\npaths = sorted(listdir(join('img', collection)))\npaths = [x for x in paths if splitext(x)[1] == '.jpg']\n\nNow, we will load each of the images and run the model over it.\n\noutput = {}\nfor path in paths:\n    image = Image.open(join('img', collection, path))\n    image = image.convert('RGB')\n    input = image_processor(image, return_tensors=\"pt\")\n    outputs = vision_model(**input)\n    pooler_output = outputs.pooler_output\n    output[path] = pooler_output[0].detach().numpy() \n    output[path] = output[path] / np.linalg.norm(output[path], 2)\n\nThen, for any reference caption, we can compute the embeding of the phrase as well. Here we have an example looking at a caption designed for the first image in our set, but you can replace it with anything of interest.\n\nref_phrase = \"Photograph of a nurse holding a beaker over a patient.\"\ntext_inputs = tokenizer(\n  [ref_phrase],\n  padding='max_length',\n  truncation=True,\n  return_tensors=\"pt\"\n)\noutput_text = text_model(**text_inputs).pooler_output\noutput_text = output_text[0].detach().numpy()\noutput_text = output_text / np.linalg.norm(output_text, 2)\n\nWith the text embedding, we can then compare it to each image embedding and determine the image that it is closest to. The SigLIP model has a way of converting this into a probability score, which we will compute as well.\n\nprobability = []\nlogit_scale = 117.330765\nlogit_bias = -12.932437\nfor path in paths:\n    dot_prod = np.dot(output_text, output[path])\n    value = (dot_prod * logit_scale + logit_bias)\n    probability += [1/(1 + np.exp(-1 * value))]\n\nThen, we can see which images are a closest match to the search query that we selected.\n\nprobability = np.array(probability)\nprobability_rank = np.argsort(probability * -1)\nfor idx in probability_rank[:5]:\n    path = paths[idx]\n    image = Image.open(join('img', collection, path))\n    print(\n        \"Predicted to have a probability of {0:.04f}:\".format(\n        probability[idx]\n    ))\n    display(image)\n\nPredicted to have a probability of 0.9985:\n\n\n\n\n\n\n\n\n\nPredicted to have a probability of 0.8788:\n\n\n\n\n\n\n\n\n\nPredicted to have a probability of 0.5130:\n\n\n\n\n\n\n\n\n\nPredicted to have a probability of 0.1805:\n\n\n\n\n\n\n\n\n\nPredicted to have a probability of 0.1739:",
    "crumbs": [
      "Multimodal",
      "5.1 Zero-Shot Model"
    ]
  },
  {
    "objectID": "caption.html",
    "href": "caption.html",
    "title": "5.2 Image Caption",
    "section": "",
    "text": "This is a minimal example script showing how to do image captioning using a set of images that are stored on the same machine where we are running the models. To start, we will load in a few modules that will be needed for the task.\n\nfrom os import listdir\nfrom os.path import splitext, join\nfrom transformers import pipeline\nfrom PIL import Image\n\nimport torch\nimport polars as pl\n\nNext, we load the model that we are interested in using. There are a large number of image classification algorithms on HuggingFace; most can be used exactly the same way by simply changing the name of the model in the function calls below.\n\nmodel = pipeline(\n  task=\"image-to-text\",\n  model=\"nlpconnect/vit-gpt2-image-captioning\"\n)\n\nWith the model loaded, the next step is to build a set of paths to the images that we are interested in using. Here, we will create a list of all of the image files in the directory containing the FSA-OWI images. Then, to save time, we take just the first five images to work with. We could run over all of the images or use a different collection by changing the variables below.\n\ncollection = 'fsaowi'\nnum_images = 5\n\npaths = sorted(listdir(join('img', collection)))\npaths = [x for x in paths if splitext(x)[1] == '.jpg']\npaths = paths[:num_images]\n\nNow, we will load each of the images and run the model over it. For each output, we save the path to the image along with the generated caption.\n\noutput = {'path': [], 'caption': []}\nfor path in paths:\n    image = Image.open(join('img', collection, path))\n    image = image.convert('RGB')\n    outputs = model(image)\n    output['path'] += [path]\n    output['caption'] += [outputs[0]['generated_text']]\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\nYou may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n\n\nThe output is constructed such that we can call the from_dict method from polars to construct a data frame. If needed, this can be saved as a CSV file with the write_csv method of the resulting data frame.\n\ndt = pl.from_dict(output)\ndt\n\n\nshape: (5, 2)\n\n\n\npath\ncaption\n\n\nstr\nstr\n\n\n\n\n\"LC-USE6-D-009289.jpg\"\n\"a woman in a white dress is ho…\n\n\n\"LC-USF33-003470.jpg\"\n\"a man is snowboarding down a s…\n\n\n\"LC-USF33-006507.jpg\"\n\"a vintage photo of a vintage t…\n\n\n\"LC-USF33-011361.jpg\"\n\"a man in a suit and tie walkin…\n\n\n\"LC-USF33-011555.jpg\"\n\"a large group of people posing…\n\n\n\n\n\n\nHere, we will display the images along with their captions to visualize the output of the model.\n\nfor path in paths:\n    image = Image.open(join('img', collection, path))\n    image = image.convert('RGB')\n    res = dt.filter(pl.col(\"path\") == path)\n    print(\"'{0:s}':\".format(res['caption'][0]))\n    display(image)\n\n'a woman in a white dress is holding a bottle ':\n\n\n\n\n\n\n\n\n\n'a man is snowboarding down a snowy street ':\n\n\n\n\n\n\n\n\n\n'a vintage photo of a vintage truck parked on the side of a road ':\n\n\n\n\n\n\n\n\n\n'a man in a suit and tie walking down a sidewalk ':\n\n\n\n\n\n\n\n\n\n'a large group of people posing for a picture ':",
    "crumbs": [
      "Multimodal",
      "5.2 Image Caption"
    ]
  },
  {
    "objectID": "citation.html",
    "href": "citation.html",
    "title": "Citation + Funding",
    "section": "",
    "text": "If you use the Distant Viewing Scripts for your work, please cite it as:\n\nTaylor Arnold and Lauren Tilon, Distant Viewing Scripts (2025). https://distantviewing.org/dvscripts\n\n\n\nFunding\n\n\nThe Distant Viewing GUI is funded with generous support from:",
    "crumbs": [
      "Citation + Funding"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Achiam, Josh, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,\nFlorencia Leoni Aleman, Diogo Almeida, et al. 2023.\n“GPT-4 Technical Report.” arXiv Preprint\narXiv:2303.08774.\n\n\nAdnan, Myasar Mundher, Mohd Shafry Mohd Rahim, Amjad Rehman, Zahid\nMehmood, Tanzila Saba, and Rizwan Ali Naqvi. 2021. “Automatic\nImage Annotation Based on Deep Learning Models: A Systematic Review and\nFuture Challenges.” IEEE Access 9: 50253–64.\n\n\nAfzal, Shehzad, Sohaib Ghani, Mohamad Mazen Hittawe, Sheikh Faisal\nRashid, Omar M Knio, Markus Hadwiger, and Ibrahim Hoteit. 2023.\n“Visualization and Visual Analytics Approaches for Image and Video\nDatasets: A Survey.” ACM Transactions on Interactive\nIntelligent Systems 13 (1): 1–41.\n\n\nAnitha Kumari, K, C Mouneeshwari, RB Udhaya, and R Jasmitha. 2020.\n“Automated Image Captioning for flickr8k Dataset.” In Proceedings of\nInternational Conference on Artificial Intelligence, Smart Grid and\nSmart City Applications: AISGSC 2019, 679–87. Springer.\n\n\nArchives, United States National. n.d. “DOCUMERICA:\nThe Environmental Protection Agency’s Program to Photographically\nDocument Subjects of Environmental Concern, 1972–1977.” https://catalog.archives.gov/id/542493.\n\n\nArnold, Taylor, and Lauren Tilton. 2019. “Distant Viewing:\nAnalyzing Large Visual Corpora.” Digital Scholarship in the\nHumanities 34 (Supplement_1): i3–16.\n\n\n———. 2020. “Distant Viewing Toolkit: A Python Package for the\nAnalysis of Visual Culture.” Journal of Open Source\nSoftware 5 (45): 1800.\n\n\n———. 2023. Distant Viewing: Computational Exploration of Digital\nImages. MIT Press.\n\n\nChen, Hailin, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut,\nRuochen Zhao, Caiming Xiong, and Shafiq Joty. 2023. “ChatGPT’s\nOne-Year Anniversary: Are Open-Source Large Language Models Catching\nUp?” arXiv Preprint arXiv:2311.16989.\n\n\nColeman, Catherine Nicole. 2020. “Managing Bias When Library\nCollections Become Data.” International Journal of\nLibrarianship 5 (1): 8–19.\n\n\nCuntz, Alexander, Paul J Heald, and Matthias Sahli. 2023.\n“Digitization and Availability of Artworks in Online Museum\nCollections.” World Intellectual Property Organization (WIPO)\nEconomic Research Working Paper Series, no. 75.\n\n\nDeal, Laura. 2015. “Visualizing Digital Collections.”\nTechnical Services Quarterly 32 (1): 14–34.\n\n\nDemiralp, Çagatay, Carlos E Scheidegger, Gordon L Kindlmann, David H\nLaidlaw, and Jeffrey Heer. 2014. “Visual Embedding: A Model for\nVisualization.” IEEE Computer Graphics and Applications\n34 (1): 10–15.\n\n\nDi Lenardo, Isabella, Benoı̂t Laurent Auguste Seguin, and Frédéric\nKaplan. 2016. “Visual Patterns Discovery in Large Databases of\nPaintings.” In Digital Humanities 2016.\n\n\nDı́az-Rodrı́guez, Natalia, and Galena Pisoni. 2020. “Accessible\nCultural Heritage Through Explainable Artificial Intelligence.”\nIn Adjunct Publication of the 28th ACM Conference on User Modeling,\nAdaptation and Personalization, 317–24.\n\n\nFlueckiger, Barbara, and Gaudenz Halter. 2020. “Methods and\nAdvanced Tools for the Analysis of Film Colors in Digital\nHumanities.” DHQ: Digital Humanities Quarterly 14 (4).\n\n\nFraser, Kathleen C, Svetlana Kiritchenko, and Isar Nejadgholi. 2023.\n“A Friendly Face: Do Text-to-Image Systems Rely on Stereotypes\nWhen the Input Is Under-Specified?” arXiv Preprint\narXiv:2302.07159.\n\n\nGefen, Alexandre, Léa Saint-Raymond, and Tommaso Venturini. 2021.\n“AI for Digital Humanities and Computational Social\nSciences.” Reflections on Artificial Intelligence for\nHumanity, 191–202.\n\n\nHiippala, Tuomo, and John A Bateman. 2022. “Semiotically-Grounded\nDistant Viewing of Diagrams: Insights from Two Multimodal\nCorpora.” Digital Scholarship in the Humanities 37 (2):\n405–25.\n\n\nKing, Ryan C, Vishnu Bharani, Kunal Shah, Yee Hui Yeo, and Jamil S\nSamaan. 2024. “GPT-4V Passes the\nBLS and ACLS Examinations: An Analysis of\nGPT-4V’s Image Recognition Capabilities.”\nResuscitation 195.\n\n\nKlinkert, Ivo, Liam A McDonnell, Stefan L Luxembourg, AF Maarten\nAltelaar, Erika R Amstalden, Sander R Piersma, and Ron Heeren. 2007.\n“Tools and Strategies for Visualization of Large Image Data Sets\nin High-Resolution Imaging Mass Spectrometry.” Review of\nScientific Instruments 78 (5).\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLee, Benjamin Charles Germain. 2023. “The ‘Collections as\nML Data’ Checklist for Machine Learning and Cultural\nHeritage.” Journal of the Association for Information Science\nand Technology.\n\n\nLei, Yiming, Zilong Li, Yangyang Li, Junping Zhang, and Hongming Shan.\n2024. “LICO: Explainable Models with Language-Image\nConsistency.” Advances in Neural Information Processing\nSystems 36.\n\n\nLiu, Fang, Mohan Zhang, Baoying Zheng, Shenglan Cui, Wentao Ma, and\nZhixiong Liu. 2023. “Feature Fusion via Multi-Target Learning for\nAncient Artwork Captioning.” Information Fusion 97:\n101811.\n\n\nMcInnes, Leland, John Healy, and James Melville. 2018.\n“UMAP: Uniform Manifold Approximation and Projection\nfor Dimension Reduction.” arXiv Preprint\narXiv:1802.03426.\n\n\nMeinecke, Christofer, Chris Hall, and Stefan Jänicke. 2022.\n“Towards Enhancing Virtual Museums by Contextualizing Art Through\nInteractive Visualizations.” ACM Journal on\nComputing and Cultural Heritage 15 (4): 1–26.\n\n\nMoreux, Jean-Philippe. 2023. “Intelligence Artificielle Et\nIndexation Des Images.” In Journées Du\nPatrimoine écrit:“l’image Aura-t-Elle\nLe Dernier Mot? Regards Croisés Sur Les Collections\nIconographiques En Bibliothèques”.\n\n\nMorse, Christopher, Blandine Landau, Carine Lallemand, Lars Wieneke, and\nVincent Koenig. 2022. “From #Museumathome to #Athomeatthemuseum:\nDigital Museums and Dialogical Engagement Beyond the\nCOVID-19 Pandemic.” ACM Journal on Computing and\nCultural Heritage (JOCCH) 15 (2): 1–29.\n\n\nMurtagh, Fionn, and Pierre Legendre. 2014. “Ward’s Hierarchical\nAgglomerative Clustering Method: Which Algorithms Implement Ward’s\nCriterion?” Journal of Classification 31: 274–95.\n\n\nPaiss, Roni, Hila Chefer, and Lior Wolf. 2022. “No Token Left\nBehind: Explainability-Aided Image Classification and\nGeneration.” In European Conference on Computer Vision,\n334–50. Springer.\n\n\nPetukhova, Alina, Joao P Matos-Carvalho, and Nuno Fachada. 2024.\n“Text Clustering with LLM Embeddings.”\narXiv Preprint arXiv:2403.15112.\n\n\nPuscasiu, Adela, Alexandra Fanca, Dan-Ioan Gota, and Honoriu Valean.\n2020. “Automated Image Captioning.” In 2020 IEEE\nInternational Conference on Automation, Quality and Testing, Robotics\n(AQTR), 1–6. IEEE.\n\n\nQi, Zhongang, Saeed Khorram, and Li Fuxin. 2021. “Embedding Deep\nNetworks into Visual Explanations.” Artificial\nIntelligence 292: 103435.\n\n\nQi, Zhongang, and Fuxin Li. 2017. “Learning Explainable Embeddings\nfor Deep Networks.” In NIPS Workshop on Interpretable Machine\nLearning. Vol. 31.\n\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, et al. 2021. “Learning\nTransferable Visual Models from Natural Language Supervision.” In\nInternational Conference on Machine Learning, 8748–63. PMLR.\n\n\nRinaldi, Antonio M, Cristiano Russo, and Cristian Tommasino. 2023.\n“Automatic Image Captioning Combining Natural Language Processing\nand Deep Neural Networks.” Results in Engineering 18:\n101107.\n\n\nSheng, Shurong, and Marie-Francine Moens. 2019. “Generating\nCaptions for Images of Ancient Artworks.” In Proceedings of\nthe 27th ACM International Conference on Multimedia, 2478–86.\n\n\nSiddiqui, Nabeel. 2024. “Cutting the Frame: An in-Depth Look at\nthe Hitchcock Computer Vision Dataset.” Journal of Open\nHumanities Data 10 (1).\n\n\nSmits, Thomas, and Melvin Wevers. 2023. “A Multimodal Turn in\nDigital Humanities. Using Contrastive Machine Learning Models to\nExplore, Enrich, and Analyze Digital Visual Historical\nCollections.” Digital Scholarship in the Humanities 38\n(3): 1267–80.\n\n\nStefanowitsch, Anatol. 2020. Corpus Linguistics: A Guide to the\nMethodology. Language Science Press.\n\n\nStraka, Milan, Jan Hajic, and Jana Straková. 2016.\n“UDPipe: Trainable Pipeline for Processing CoNLL-u\nFiles Performing Tokenization, Morphological Analysis, Pos Tagging and\nParsing.” In Proceedings of the Tenth International\nConference on Language Resources and Evaluation (LREC’16), 4290–97.\n\n\nTan, Mingxing, and Quoc Le. 2019. “EfficientNet:\nRethinking Model Scaling for Convolutional Neural Networks.” In\nInternational Conference on Machine Learning, 6105–14. PMLR.\n\n\nVerma, Akash, Arun Kumar Yadav, Mohit Kumar, and Divakar Yadav. 2024.\n“Automatic Image Caption Generation Using Deep Learning.”\nMultimedia Tools and Applications 83 (2): 5309–25.\n\n\nWevers, Melvin, and Thomas Smits. 2020. “The Visual Digital Turn:\nUsing Neural Networks to Study Historical Images.” Digital\nScholarship in the Humanities 35 (1): 194–207.\n\n\nWhitelaw, Mitchell. 2015. “Generous Interfaces for Digital\nCultural Collections.” Digital Humanities Quarterly 9\n(1): 1–16.\n\n\nWindhager, Florian, Paolo Federico, Günther Schreder, Katrin Glinka,\nMarian Dörk, Silvia Miksch, and Eva Mayr. 2018. “Visualization of\nCultural Heritage Collection Data: State of the Art and Future\nChallenges.” IEEE Transactions on Visualization and Computer\nGraphics 25 (6): 2311–30.\n\n\nWu, Wenhao, Huanjin Yao, Mengxi Zhang, Yuxin Song, Wanli Ouyang, and\nJingdong Wang. 2023. “GPT4Vis: What Can\nGPT-4 Do for Zero-Shot Visual Recognition?”\narXiv Preprint arXiv:2311.15732.\n\n\nYe, Yilin, Rong Huang, and Wei Zeng. 2022. “VISAtlas:\nAn Image-Based Exploration and Query System for Large Visualization\nCollections via Neural Image Embedding.” IEEE Transactions on\nVisualization and Computer Graphics.\n\n\nYin, Shukang, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and\nEnhong Chen. 2023. “A Survey on Multimodal Large Language\nModels.” arXiv Preprint arXiv:2306.13549.\n\n\nYou, Haoxuan, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui\nWang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. 2023.\n“Ferret: Refer and Ground Anything Anywhere at Any\nGranularity.” arXiv Preprint arXiv:2310.07704.",
    "crumbs": [
      "References"
    ]
  }
]